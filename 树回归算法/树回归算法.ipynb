{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 树回归 概述\n",
    "\n",
    "我们本章介绍 CART(Classification And Regression Trees， 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。\n",
    "\n",
    "## 树回归 场景\n",
    "\n",
    "我们在第 8 章中介绍了线性回归的一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙。而且，实际生活中很多问题都是非线性的，不可能使用全局线性模型来拟合任何数据。\n",
    "\n",
    "一种可行的方法是将数据集切分成很多份易建模的数据，然后利用我们的线性回归技术来建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树回归和回归法就相当有用。\n",
    "\n",
    "除了我们在 第3章 中介绍的 决策树算法，我们介绍一个新的叫做 CART(Classification And Regression Trees, 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、树回归 原理\n",
    "\n",
    "### 1.1、树回归 原理概述\n",
    "\n",
    "为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。第3章使用树进行分类，会在给定节点时计算数据的混乱度。那么如何计算连续型数值的混乱度呢？\n",
    "\n",
    "在这里，计算连续型数值的混乱度是非常简单的。首先计算所有数据的均值，然后计算每条数据的值到均值的差值。为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。\n",
    "\n",
    "上述做法有点类似于前面介绍过的统计学中常用的方差计算。唯一不同就是，方差是平方误差的均值(均方差)，而这里需要的是平方误差的总值(总方差)。总方差可以通过均方差乘以数据集中样本点的个数来得到。\n",
    "\n",
    "### 1.2、树构建算法 比较\n",
    "\n",
    "我们在 第3章 中使用的树构建算法是 ID3 。ID3 的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就是说，如果一个特征有 4 种取值，那么数据将被切分成 4 份。一旦按照某特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另外一种方法是二元切分法，即每次把数据集切分成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。\n",
    "\n",
    "除了切分过于迅速外， ID3 算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在 ID3 算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构造过程进行调整以处理连续型特征。具体的处理方法是: 如果特征值大于给定值就走左子树，否则就走右子树。另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。\n",
    "\n",
    "CART 是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对 CART 稍作修改就可以处理回归问题。第 3 章中使用香农熵来度量集合的无组织程度。如果选用其他方法来代替香农熵，就可以使用树构建算法来完成回归。\n",
    "\n",
    "回归树与分类树的思路类似，但是叶节点的数据类型不是离散型，而是连续型。\n",
    "\n",
    "**1.2.1、附加 各常见树构造算法的划分分支方式**   \n",
    "\n",
    "还有一点要说明，构建决策树算法，常用到的是三个方法: ID3, C4.5, CART.  \n",
    "三种方法区别是划分树的分支的方式:\n",
    "\n",
    "1. ID3 是信息增益分支\n",
    "2. C4.5 是信息增益率分支\n",
    "3. CART 做分类工作时，采用 GINI 值作为节点分裂的依据；回归时，采用样本的最小方差作为节点的分裂依据。\n",
    "\n",
    "工程上总的来说:\n",
    "\n",
    "CART 和 C4.5 之间主要差异在于分类结果上，CART 可以回归分析也可以分类，C4.5 只能做分类；C4.5 子节点是可以多分的，而 CART 是无数个二叉子节点；\n",
    "\n",
    "以此拓展出以 CART 为基础的 “树群” Random forest ， 以 回归树 为基础的 “树群” GBDT 。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3、树回归 工作原理\n",
    "\n",
    "1、找到数据集切分的最佳位置，函数 chooseBestSplit() 伪代码大致如下:\n",
    "\n",
    "```\n",
    "对每个特征:\n",
    "    对每个特征值: \n",
    "        将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树）\n",
    "        计算切分的误差\n",
    "        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差\n",
    "返回最佳切分的特征和阈值\n",
    "```\n",
    "\n",
    "2、树构建算法，函数 createTree() 伪代码大致如下:\n",
    "\n",
    "```\n",
    "找到最佳的待切分特征:\n",
    "    如果该节点不能再分，将该节点存为叶节点\n",
    "    执行二元切分\n",
    "    在右子树调用 createTree() 方法\n",
    "    在左子树调用 createTree() 方法\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4、树回归 开发流程\n",
    "\n",
    "```\n",
    "(1) 收集数据：采用任意方法收集数据。\n",
    "(2) 准备数据：需要数值型数据，标称型数据应该映射成二值型数据。\n",
    "(3) 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树。\n",
    "(4) 训练算法：大部分时间都花费在叶节点树模型的构建上。\n",
    "(5) 测试算法：使用测试数据上的R^2值来分析模型的效果。\n",
    "(6) 使用算法：使用训练处的树做预测，预测结果还可以用来做很多事情。\n",
    "```\n",
    "\n",
    "### 1.5、树回归 算法特点\n",
    "\n",
    "```\n",
    "优点：可以对复杂和非线性的数据建模。\n",
    "缺点：结果不易理解。\n",
    "适用数据类型：数值型和标称型数据。\n",
    "```\n",
    "\n",
    "### 1.6、回归树 项目案例\n",
    "\n",
    "**1.6.1、项目概述**\n",
    "\n",
    "在简单数据集上生成一棵回归树。\n",
    "\n",
    "**1.6.2、开发流程**\n",
    "\n",
    "```\n",
    "收集数据：采用任意方法收集数据\n",
    "准备数据：需要数值型数据，标称型数据应该映射成二值型数据\n",
    "分析数据：绘出数据的二维可视化显示结果，以字典方式生成树\n",
    "训练算法：大部分时间都花费在叶节点树模型的构建上\n",
    "测试算法：使用测试数据上的R^2值来分析模型的效果\n",
    "使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情\n",
    "```\n",
    "\n",
    "> 收集数据：采用任意方法收集数据\n",
    "\n",
    "data1.txt 文件中存储的数据格式如下:\n",
    "\n",
    "```\n",
    "0.036098\t0.155096\n",
    "0.993349\t1.077553\n",
    "0.530897\t0.893462\n",
    "0.712386\t0.564858\n",
    "0.343554\t-0.371700\n",
    "0.098016\t-0.332760\n",
    "```\n",
    "\n",
    "> 准备数据：需要数值型数据，标称型数据应该映射成二值型数据\n",
    "\n",
    "> 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树\n",
    "\n",
    "基于 CART 算法构建回归树的简单数据集\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1540868412253_MGmEqvtcPB.jpg)\n",
    "\n",
    "用于测试回归树的分段常数数据集\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1540868447145_muzqroDbNs.jpg)\n",
    "\n",
    "> 训练算法: 构造树的数据结构\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binSplitDataSet(dataSet, feature, value):\n",
    "    \"\"\"binSplitDataSet(将数据集，按照feature列的value进行 二元切分)\n",
    "        Description：在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。\n",
    "    Args:\n",
    "        dataMat 数据集\n",
    "        feature 待切分的特征列\n",
    "        value 特征列要比较的值\n",
    "    Returns:\n",
    "        mat0 小于等于 value 的数据集在左边\n",
    "        mat1 大于 value 的数据集在右边\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "    # # 测试案例\n",
    "    # print 'dataSet[:, feature]=', dataSet[:, feature]\n",
    "    # print 'nonzero(dataSet[:, feature] > value)[0]=', nonzero(dataSet[:, feature] > value)[0]\n",
    "    # print 'nonzero(dataSet[:, feature] <= value)[0]=', nonzero(dataSet[:, feature] <= value)[0]\n",
    "\n",
    "    # dataSet[:, feature] 取去每一行中，第1列的值(从0开始算)\n",
    "    # nonzero(dataSet[:, feature] > value)  返回结果为true行的index下标\n",
    "    mat0 = dataSet[nonzero(dataSet[:, feature] <= value)[0], :]\n",
    "    mat1 = dataSet[nonzero(dataSet[:, feature] > value)[0], :]\n",
    "    return mat0, mat1\n",
    "\n",
    "\n",
    "# 返回每一个叶子结点的均值\n",
    "# returns the value used for each leaf\n",
    "# 我的理解是：regLeaf 是产生叶节点的函数，就是求均值，即用聚类中心点来代表这类数据\n",
    "def regLeaf(dataSet):\n",
    "    return mean(dataSet[:, -1])\n",
    "\n",
    "\n",
    "# 计算总方差=方差*样本数\n",
    "# 我的理解是：求这组数据的方差，即通过决策树划分，可以让靠近的数据分到同一类中去\n",
    "def regErr(dataSet):\n",
    "    # shape(dataSet)[0] 表示行数\n",
    "    return var(dataSet[:, -1]) * shape(dataSet)[0]\n",
    "\n",
    "\n",
    "# 1.用最佳方式切分数据集\n",
    "# 2.生成相应的叶节点\n",
    "def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n",
    "    \"\"\"chooseBestSplit(用最佳方式切分数据集 和 生成相应的叶节点)\n",
    "\n",
    "    Args:\n",
    "        dataSet   加载的原始数据集\n",
    "        leafType  建立叶子点的函数\n",
    "        errType   误差计算函数(求总方差)\n",
    "        ops       [容许误差下降值，切分的最少样本数]。\n",
    "    Returns:\n",
    "        bestIndex feature的index坐标\n",
    "        bestValue 切分的最优值\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "\n",
    "    # ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。\n",
    "    # 之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。\n",
    "    # 最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分\n",
    "    tolS = ops[0]\n",
    "    # 划分最小 size 小于，就不继续划分了\n",
    "    tolN = ops[1]\n",
    "    #如果数据集的最后一列所有值相等就退出\n",
    "    #dataSet[:, -1].T.tolist()[0] 取数据集的最后一列，转置为行向量，然后转换为list,取该list中的第一个元素。\n",
    "    if len(set(dataSet[:, -1].T.tolist()[0])) == 1: # 如果集合size为1，也就是说全部的数据都是同一个类别，不用继续划分。\n",
    "        #  exit cond 1\n",
    "        return None, leafType(dataSet)\n",
    "    # 计算行列值\n",
    "    m, n = shape(dataSet)\n",
    "    # 无分类误差的总方差和\n",
    "    # the choice of the best feature is driven by Reduction in RSS error from mean\n",
    "    S = errType(dataSet)\n",
    "    # inf 正无穷大\n",
    "    bestS, bestIndex, bestValue = inf, 0, 0\n",
    "    # 循环处理每一列对应的feature值\n",
    "    for featIndex in range(n-1): # 对于每个特征\n",
    "        # 下面的一行表示的是将某一列全部的数据转换为行，然后设置为list形式\n",
    "        for splitVal in set(dataSet[:, featIndex].T.tolist()[0]):\n",
    "            # 对该列进行分组，然后组内的成员的val值进行 二元切分\n",
    "            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\n",
    "            # 判断二元切分的方式的元素数量是否符合预期\n",
    "            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):\n",
    "                continue\n",
    "            newS = errType(mat0) + errType(mat1)\n",
    "            # 如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差\n",
    "            # 如果划分后误差小于 bestS，则说明找到了新的bestS\n",
    "            if newS < bestS:\n",
    "                bestIndex = featIndex\n",
    "                bestValue = splitVal\n",
    "                bestS = newS\n",
    "    # 判断二元切分的方式的元素误差是否符合预期\n",
    "    # if the decrease (S-bestS) is less than a threshold don't do the split\n",
    "    if (S - bestS) < tolS:\n",
    "        return None, leafType(dataSet)\n",
    "    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n",
    "    # 对整体的成员进行判断，是否符合预期\n",
    "    # 如果集合的 size 小于 tolN \n",
    "    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): # 当最佳划分后，集合过小，也不划分，产生叶节点\n",
    "        return None, leafType(dataSet)\n",
    "    return bestIndex, bestValue\n",
    "\n",
    "\n",
    "# assume dataSet is NumPy Mat so we can array filtering\n",
    "# 假设 dataSet 是 NumPy Mat 类型的，那么我们可以进行 array 过滤\n",
    "def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n",
    "    \"\"\"createTree(获取回归树)\n",
    "        Description：递归函数：如果构建的是回归树，该模型是一个常数，如果是模型树，其模型师一个线性方程。\n",
    "    Args:\n",
    "        dataSet      加载的原始数据集\n",
    "        leafType     建立叶子点的函数\n",
    "        errType      误差计算函数\n",
    "        ops=(1, 4)   [容许误差下降值，切分的最少样本数]\n",
    "    Returns:\n",
    "        retTree    决策树最后的结果\n",
    "    \"\"\"\n",
    "    # 选择最好的切分方式： feature索引值，最优切分值\n",
    "    # choose the best split\n",
    "    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n",
    "    # if the splitting hit a stop condition return val\n",
    "    # 如果 splitting 达到一个停止条件，那么返回 val\n",
    "    if feat is None:\n",
    "        return val\n",
    "    retTree = {}\n",
    "    retTree['spInd'] = feat\n",
    "    retTree['spVal'] = val\n",
    "    # 大于在右边，小于在左边，分为2个数据集\n",
    "    lSet, rSet = binSplitDataSet(dataSet, feat, val)\n",
    "    # 递归的进行调用，在左右子树中继续递归生成树\n",
    "    retTree['left'] = createTree(lSet, leafType, errType, ops)\n",
    "    retTree['right'] = createTree(rSet, leafType, errType, ops)\n",
    "    return retTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法：使用测试数据上的R^2值来分析模型的效果\n",
    "\n",
    "> 使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、树剪枝\n",
    "\n",
    "一棵树如果节点过多，表明该模型可能对数据进行了 “过拟合”。\n",
    "\n",
    "通过降低决策树的复杂度来避免过拟合的过程称为 剪枝（pruning）。在函数 chooseBestSplit() 中提前终止条件，实际上是在进行一种所谓的 预剪枝（prepruning）操作。另一个形式的剪枝需要使用测试集和训练集，称作 后剪枝（postpruning）。\n",
    "\n",
    "### 2.1、预剪枝(prepruning)\n",
    "\n",
    "顾名思义，预剪枝就是及早的停止树增长，在构造决策树的同时进行剪枝。\n",
    "\n",
    "所有决策树的构建方法，都是在无法进一步降低熵的情况下才会停止创建分支的过程，为了避免过拟合，可以设定一个阈值，熵减小的数量小于这个阈值，即使还可以继续降低熵，也停止继续创建分支。但是这种方法实际中的效果并不好。\n",
    "\n",
    "### 2.2、后剪枝(postpruning)\n",
    "\n",
    "决策树构造完成后进行剪枝。剪枝的过程是对拥有同样父节点的一组节点进行检查，判断如果将其合并，熵的增加量是否小于某一阈值。如果确实小，则这一组节点可以合并一个节点，其中包含了所有可能的结果。合并也被称作 塌陷处理 ，在回归树中一般采用取需要合并的所有子树的平均值。后剪枝是目前最普遍的做法。\n",
    "\n",
    "后剪枝 prune() 的伪代码如下:\n",
    "\n",
    "```\n",
    "基于已有的树切分测试数据:\n",
    "    如果存在任一子集是一棵树，则在该子集递归剪枝过程\n",
    "    计算将当前两个叶节点合并后的误差\n",
    "    计算不合并的误差\n",
    "    如果合并会降低误差的话，就将叶节点合并\n",
    "```\n",
    "\n",
    "### 2.3、剪枝 代码\n",
    "\n",
    "回归树剪枝函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断节点是否是一个字典\n",
    "def isTree(obj):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        测试输入变量是否是一棵树,即是否是一个字典\n",
    "    Args:\n",
    "        obj -- 输入变量\n",
    "    Returns:\n",
    "        返回布尔类型的结果。如果 obj 是一个字典，返回true，否则返回 false\n",
    "    \"\"\"\n",
    "    return (type(obj).__name__ == 'dict')\n",
    "\n",
    "\n",
    "# 计算左右枝丫的均值\n",
    "def getMean(tree):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        从上往下遍历树直到叶节点为止，如果找到两个叶节点则计算它们的平均值。\n",
    "        对 tree 进行塌陷处理，即返回树平均值。\n",
    "    Args:\n",
    "        tree -- 输入的树\n",
    "    Returns:\n",
    "        返回 tree 节点的平均值\n",
    "    \"\"\"\n",
    "    if isTree(tree['right']):\n",
    "        tree['right'] = getMean(tree['right'])\n",
    "    if isTree(tree['left']):\n",
    "        tree['left'] = getMean(tree['left'])\n",
    "    return (tree['left']+tree['right'])/2.0\n",
    "\n",
    "\n",
    "# 检查是否适合合并分枝\n",
    "def prune(tree, testData):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        从上而下找到叶节点，用测试数据集来判断将这些叶节点合并是否能降低测试误差\n",
    "    Args:\n",
    "        tree -- 待剪枝的树\n",
    "        testData -- 剪枝所需要的测试数据 testData \n",
    "    Returns:\n",
    "        tree -- 剪枝完成的树\n",
    "    \"\"\"\n",
    "    # 判断是否测试数据集没有数据，如果没有，就直接返回tree本身的均值\n",
    "    if shape(testData)[0] == 0:\n",
    "        return getMean(tree)\n",
    "\n",
    "    # 判断分枝是否是dict字典，如果是就将测试数据集进行切分\n",
    "    if (isTree(tree['right']) or isTree(tree['left'])):\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "    # 如果是左边分枝是字典，就传入左边的数据集和左边的分枝，进行递归\n",
    "    if isTree(tree['left']):\n",
    "        tree['left'] = prune(tree['left'], lSet)\n",
    "    # 如果是右边分枝是字典，就传入左边的数据集和左边的分枝，进行递归\n",
    "    if isTree(tree['right']):\n",
    "        tree['right'] = prune(tree['right'], rSet)\n",
    "\n",
    "    # 上面的一系列操作本质上就是将测试数据集按照训练完成的树拆分好，对应的值放到对应的节点\n",
    "\n",
    "    # 如果左右两边同时都不是dict字典，也就是左右两边都是叶节点，而不是子树了，那么分割测试数据集。\n",
    "    # 1. 如果正确 \n",
    "    #   * 那么计算一下总方差 和 该结果集的本身不分枝的总方差比较\n",
    "    #   * 如果 合并的总方差 < 不合并的总方差，那么就进行合并\n",
    "    # 注意返回的结果： 如果可以合并，原来的dict就变为了 数值\n",
    "    if not isTree(tree['left']) and not isTree(tree['right']):\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "        # power(x, y)表示x的y次方\n",
    "        errorNoMerge = sum(power(lSet[:, -1] - tree['left'], 2)) + sum(power(rSet[:, -1] - tree['right'], 2))\n",
    "        treeMean = (tree['left'] + tree['right'])/2.0\n",
    "        errorMerge = sum(power(testData[:, -1] - treeMean, 2))\n",
    "        # 如果 合并的总方差 < 不合并的总方差，那么就进行合并\n",
    "        if errorMerge < errorNoMerge:\n",
    "            print(\"merging\")\n",
    "            return treeMean\n",
    "        else:\n",
    "            return tree\n",
    "    else:\n",
    "        return tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、模型树\n",
    "\n",
    "### 3.1、模型树 简介\n",
    "\n",
    "用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的 分段线性（piecewise linear） 是指模型由多个线性片段组成。\n",
    "\n",
    "我们看一下图 9-4 中的数据，如果使用两条直线拟合是否比使用一组常数来建模好呢？答案显而易见。可以设计两条分别从 0.00.3、从 0.31.0 的直线，于是就可以得到两个线性模型。因为数据集里的一部分数据（0.00.3）以某个线性模型建模，而另一部分数据（0.31.0）则以另一个线性模型建模，因此我们说采用了所谓的分段线性模型。\n",
    "\n",
    "决策树相比于其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1540869302009_QILwR6N6wU.jpg)\n",
    "\n",
    "将之前的回归树的代码稍作修改，就可以在叶节点生成线性模型而不是常数值。下面将利用树生成算法对数据进行划分，且每份切分数据都能很容易被线性模型所表示。这个算法的关键在于误差的计算。\n",
    "\n",
    "那么为了找到最佳切分，应该怎样计算误差呢？前面用于回归树的误差计算方法这里不能再用。稍加变化，对于给定的数据集，应该先用模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值。最后将这些差值的平方求和就得到了所需的误差。\n",
    "\n",
    "### 3.2、模型树 代码\n",
    "\n",
    "模型树的叶节点生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到模型的ws系数：f(x) = x0 + x1*featrue1+ x3*featrue2 ...\n",
    "# create linear model and return coeficients\n",
    "def modelLeaf(dataSet):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        当数据不再需要切分的时候，生成叶节点的模型。\n",
    "    Args:\n",
    "        dataSet -- 输入数据集\n",
    "    Returns:\n",
    "        调用 linearSolve 函数，返回得到的 回归系数ws\n",
    "    \"\"\"\n",
    "    ws, X, Y = linearSolve(dataSet)\n",
    "    return ws\n",
    "\n",
    "\n",
    "# 计算线性模型的误差值\n",
    "def modelErr(dataSet):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        在给定数据集上计算误差。\n",
    "    Args:\n",
    "        dataSet -- 输入数据集\n",
    "    Returns:\n",
    "        调用 linearSolve 函数，返回 yHat 和 Y 之间的平方误差。\n",
    "    \"\"\"\n",
    "    ws, X, Y = linearSolve(dataSet)\n",
    "    yHat = X * ws\n",
    "    # print corrcoef(yHat, Y, rowvar=0)\n",
    "    return sum(power(Y - yHat, 2))\n",
    "\n",
    "\n",
    " # helper function used in two places\n",
    "def linearSolve(dataSet):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        将数据集格式化成目标变量Y和自变量X，执行简单的线性回归，得到ws\n",
    "    Args:\n",
    "        dataSet -- 输入数据\n",
    "    Returns:\n",
    "        ws -- 执行线性回归的回归系数 \n",
    "        X -- 格式化自变量X\n",
    "        Y -- 格式化目标变量Y\n",
    "    \"\"\"\n",
    "    m, n = shape(dataSet)\n",
    "    # 产生一个关于1的矩阵\n",
    "    X = mat(ones((m, n)))\n",
    "    Y = mat(ones((m, 1)))\n",
    "    # X的0列为1，常数项，用于计算平衡误差\n",
    "    X[:, 1: n] = dataSet[:, 0: n-1]\n",
    "    Y = dataSet[:, -1]\n",
    "\n",
    "    # 转置矩阵*矩阵\n",
    "    xTx = X.T * X\n",
    "    # 如果矩阵的逆不存在，会造成程序异常\n",
    "    if linalg.det(xTx) == 0.0:\n",
    "        raise NameError('This matrix is singular, cannot do inverse,\\ntry increasing the second value of ops')\n",
    "    # 最小二乘法求最优解:  w0*1+w1*x1=y\n",
    "    ws = xTx.I * (X.T * Y)\n",
    "    return ws, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3、模型树 运行结果\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1540869413156_he4LliKJDr.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、树回归 项目案例\n",
    "\n",
    "### 4.1、项目案例1: 树回归与标准回归的比较\n",
    "\n",
    "**4.1.1、项目概述**\n",
    "\n",
    "前面介绍了模型树、回归树和一般的回归方法，下面测试一下哪个模型最好。\n",
    "\n",
    "这些模型将在某个数据上进行测试，该数据涉及人的智力水平和自行车的速度的关系。当然，数据是假的。\n",
    "\n",
    "**4.1.2、开发流程**\n",
    "```\n",
    "收集数据：采用任意方法收集数据\n",
    "准备数据：需要数值型数据，标称型数据应该映射成二值型数据\n",
    "分析数据：绘出数据的二维可视化显示结果，以字典方式生成树\n",
    "训练算法：模型树的构建\n",
    "测试算法：使用测试数据上的R^2值来分析模型的效果\n",
    "使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情\n",
    "```\n",
    "\n",
    "> 收集数据: 采用任意方法收集数据\n",
    "\n",
    "> 准备数据：需要数值型数据，标称型数据应该映射成二值型数据\n",
    "\n",
    "数据存储格式:\n",
    "\n",
    "```\n",
    "3.000000\t46.852122\n",
    "23.000000\t178.676107\n",
    "0.000000\t86.154024\n",
    "6.000000\t68.707614\n",
    "15.000000\t139.737693\n",
    "```\n",
    "\n",
    "> 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1540869604144_y2835aaXKX.jpg)\n",
    "\n",
    "> 训练算法：模型树的构建\n",
    "\n",
    "用树回归进行预测的代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归树测试案例\n",
    "# 为了和 modelTreeEval() 保持一致，保留两个输入参数\n",
    "def regTreeEval(model, inDat):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        对 回归树 进行预测\n",
    "    Args:\n",
    "        model -- 指定模型，可选值为 回归树模型 或者 模型树模型，这里为回归树\n",
    "        inDat -- 输入的测试数据\n",
    "    Returns:\n",
    "        float(model) -- 将输入的模型数据转换为 浮点数 返回\n",
    "    \"\"\"\n",
    "    return float(model)\n",
    "\n",
    "\n",
    "# 模型树测试案例\n",
    "# 对输入数据进行格式化处理，在原数据矩阵上增加第0列，元素的值都是1，\n",
    "# 也就是增加偏移值，和我们之前的简单线性回归是一个套路，增加一个偏移量\n",
    "def modelTreeEval(model, inDat):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        对 模型树 进行预测\n",
    "    Args:\n",
    "        model -- 输入模型，可选值为 回归树模型 或者 模型树模型，这里为模型树模型\n",
    "        inDat -- 输入的测试数据\n",
    "    Returns:\n",
    "        float(X * model) -- 将测试数据乘以 回归系数 得到一个预测值 ，转化为 浮点数 返回\n",
    "    \"\"\"\n",
    "    n = shape(inDat)[1]\n",
    "    X = mat(ones((1, n+1)))\n",
    "    X[:, 1: n+1] = inDat\n",
    "    # print X, model\n",
    "    return float(X * model)\n",
    "\n",
    "\n",
    "# 计算预测的结果\n",
    "# 在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。\n",
    "# modelEval是对叶节点进行预测的函数引用，指定树的类型，以便在叶节点上调用合适的模型。\n",
    "# 此函数自顶向下遍历整棵树，直到命中叶节点为止，一旦到达叶节点，它就会在输入数据上\n",
    "# 调用modelEval()函数，该函数的默认值为regTreeEval()\n",
    "def treeForeCast(tree, inData, modelEval=regTreeEval):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        对特定模型的树进行预测，可以是 回归树 也可以是 模型树\n",
    "    Args:\n",
    "        tree -- 已经训练好的树的模型\n",
    "        inData -- 输入的测试数据\n",
    "        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树\n",
    "    Returns:\n",
    "        返回预测值\n",
    "    \"\"\"\n",
    "    if not isTree(tree):\n",
    "        return modelEval(tree, inData)\n",
    "    if inData[tree['spInd']] <= tree['spVal']:\n",
    "        if isTree(tree['left']):\n",
    "            return treeForeCast(tree['left'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['left'], inData)\n",
    "    else:\n",
    "        if isTree(tree['right']):\n",
    "            return treeForeCast(tree['right'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['right'], inData)\n",
    "\n",
    "\n",
    "# 预测结果\n",
    "def createForeCast(tree, testData, modelEval=regTreeEval):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        调用 treeForeCast ，对特定模型的树进行预测，可以是 回归树 也可以是 模型树\n",
    "    Args:\n",
    "        tree -- 已经训练好的树的模型\n",
    "        inData -- 输入的测试数据\n",
    "        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树\n",
    "    Returns:\n",
    "        返回预测值矩阵\n",
    "    \"\"\"\n",
    "    m = len(testData)\n",
    "    yHat = mat(zeros((m, 1)))\n",
    "    # print yHat\n",
    "    for i in range(m):\n",
    "        yHat[i, 0] = treeForeCast(tree, mat(testData[i]), modelEval)\n",
    "        # print \"yHat==>\", yHat[i, 0]\n",
    "    return yHat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法：使用测试数据上的R^2值来分析模型的效果\n",
    "\n",
    "R^2 判定系数就是拟合优度判定系数，它体现了回归模型中自变量的变异在因变量的变异中所占的比例。如 R^2=0.99999 表示在因变量 y 的变异中有 99.999% 是由于变量 x 引起。当 R^2=1 时表示，所有观测点都落在拟合的直线或曲线上；当 R^2=0 时，表示自变量与因变量不存在直线或曲线关系。\n",
    "\n",
    "所以我们看出， R^2 的值越接近 1.0 越好。\n",
    "\n",
    "> 使用算法：使用训练出的树做预测，预测结果还可以用来做很多事情"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5、附加 Python 中 GUI 的使用\n",
    "\n",
    "### 5.1、使用 Python 的 Tkinter 库创建 GUI\n",
    "\n",
    "如果能让用户不需要任何指令就可以按照他们自己的方式来分析数据，就不需要对数据做出过多解释。其中一个能同时支持数据呈现和用户交互的方式就是构建一个图形用户界面(GUI，Graphical User Interface)，如图9-7所示。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1540870501594_DCcdLyzB8W.jpg)\n",
    "\n",
    "### 5.2、用 Tkinter 创建 GUI\n",
    "Python 有很多 GUI 框架，其中一个易于使用的 Tkinter，是随 Python 的标准版编译版本发布的。Tkinter 可以在 Windows、Mac OS和大多数的 Linux 平台上使用。\n",
    "\n",
    "### 5.3、集成 Matplotlib 和 Tkinter\n",
    "MatPlotlib 的构建程序包含一个前端，也就是面向用户的一些代码，如 plot() 和 scatter() 方法等。事实上，它同时创建了一个后端，用于实现绘图和不同应用之间接口。\n",
    "\n",
    "通过改变后端可以将图像绘制在PNG、PDF、SVG等格式的文件上。下面将设置后端为 TkAgg (Agg 是一个 C++ 的库，可以从图像创建光栅图)。TkAgg可以在所选GUI框架上调用Agg，把 Agg 呈现在画布上。我们可以在Tk的GUI上放置一个画布，并用 .grid()来调整布局。\n",
    "\n",
    "### 5.4、用treeExplore 的GUI构建的模型树示例图\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1540870536258_Ba8UbWWJAy.jpg)\n",
    "\n",
    "## 6、树回归 小结\n",
    "\n",
    "数据集中经常包含一些复杂的相关关系，使得输入数据和目标变量之间呈现非线性关系。对这些复杂的关系建模，一种可行的方式是使用树来对预测值分段，包括分段常数或分段直线。一般采用树结构来对这种数据建模。相应地，若叶节点使用的模型是分段常数则称为回归树，若叶节点使用的模型师线性回归方程则称为模型树。\n",
    "\n",
    "CART 算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART 算法构建模型树和回归树。该算法构建出的树会倾向于对数据过拟合。一棵过拟合的树常常十分复杂，剪枝技术的出现就是为了解决这个问题。两种剪枝方法分别是预剪枝（在树的构建过程中就进行剪枝）和后剪枝（当树构建完毕再进行剪枝），预剪枝更有效但需要用户定义一些参数。\n",
    "\n",
    "Tkinter 是 Python 的一个 GUI 工具包。虽然并不是唯一的包，但它最常用。利用 Tkinter ，我们可以轻轻松松绘制各种部件并安排它们的位置。另外，可以为 Tkinter 构造一个特殊的部件来显示 Matplotlib 绘出的图。所以，Matplotlib 和 Tkinter 的集成可以构建出更强大的 GUI ，用户可以以更自然的方式来探索机器学习算法的奥妙。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spInd': 0, 'spVal': 10.0, 'left': {'spInd': 0, 'spVal': 7.0, 'left': {'spInd': 0, 'spVal': 5.0, 'left': 50.94683665, 'right': 69.02117757692308}, 'right': 94.7066578125}, 'right': {'spInd': 0, 'spVal': 17.0, 'left': {'spInd': 0, 'spVal': 14.0, 'left': 122.90893026923078, 'right': 141.06067981481482}, 'right': {'spInd': 0, 'spVal': 20.0, 'left': 157.0484078846154, 'right': 168.34161286956524}}}\n",
      "--------------\n",
      "\n",
      "回归树: 0.9640852318222141\n",
      "{'spInd': 0, 'spVal': 4.0, 'left': matrix([[ 68.87014372],\n",
      "        [-11.78556471]]), 'right': {'spInd': 0, 'spVal': 12.0, 'left': {'spInd': 0, 'spVal': 9.0, 'left': {'spInd': 0, 'spVal': 6.0, 'left': matrix([[-17.21714265],\n",
      "        [ 13.72153115]]), 'right': matrix([[-11.84548851],\n",
      "        [ 12.12382261]])}, 'right': matrix([[-2.87684083],\n",
      "        [10.20804482]])}, 'right': {'spInd': 0, 'spVal': 16.0, 'left': matrix([[43.41251481],\n",
      "        [ 6.37966738]]), 'right': {'spInd': 0, 'spVal': 20.0, 'left': matrix([[37.54851927],\n",
      "        [ 6.23298637]]), 'right': matrix([[47.58621512],\n",
      "        [ 5.51066299]])}}}}\n",
      "模型树: 0.9760412191380593\n",
      "[[37.58916794]\n",
      " [ 6.18978355]]\n",
      "线性回归: 0.9434684235674763\n"
     ]
    }
   ],
   "source": [
    "## 项目完整代码：\n",
    "\n",
    "#!/usr/bin/python\n",
    "# coding:utf8\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "# 默认解析的数据是用tab分隔，并且是数值类型\n",
    "# general function to parse tab -delimited floats\n",
    "def loadDataSet(fileName):\n",
    "    \"\"\"loadDataSet(解析每一行，并转化为float类型)\n",
    "        Desc：该函数读取一个以 tab 键为分隔符的文件，然后将每行的内容保存成一组浮点数\n",
    "    Args:\n",
    "        fileName 文件名\n",
    "    Returns:\n",
    "        dataMat 每一行的数据集array类型\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "    # 假定最后一列是结果值\n",
    "    # assume last column is target value\n",
    "    dataMat = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.strip().split('\\t')\n",
    "        #将每行转换成浮点数\n",
    "        fltLine = [float(x) for x in curLine]\n",
    "        dataMat.append(fltLine)\n",
    "    return dataMat\n",
    "\n",
    "\n",
    "def binSplitDataSet(dataSet, feature, value):\n",
    "    \"\"\"binSplitDataSet(将数据集，按照feature列的value进行 二元切分)\n",
    "        Description：在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。\n",
    "    Args:\n",
    "        dataMat 数据集\n",
    "        feature 待切分的特征列\n",
    "        value 特征列要比较的值\n",
    "    Returns:\n",
    "        mat0 小于等于 value 的数据集在左边\n",
    "        mat1 大于 value 的数据集在右边\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "    # # 测试案例\n",
    "    # print 'dataSet[:, feature]=', dataSet[:, feature]\n",
    "    # print 'nonzero(dataSet[:, feature] > value)[0]=', nonzero(dataSet[:, feature] > value)[0]\n",
    "    # print 'nonzero(dataSet[:, feature] <= value)[0]=', nonzero(dataSet[:, feature] <= value)[0]\n",
    "\n",
    "    # dataSet[:, feature] 取去每一行中，第1列的值(从0开始算)\n",
    "    # nonzero(dataSet[:, feature] > value)  返回结果为true行的index下标\n",
    "    mat0 = dataSet[nonzero(dataSet[:, feature] <= value)[0], :]\n",
    "    mat1 = dataSet[nonzero(dataSet[:, feature] > value)[0], :]\n",
    "    return mat0, mat1\n",
    "\n",
    "\n",
    "# 返回每一个叶子结点的均值\n",
    "# returns the value used for each leaf\n",
    "# 我的理解是：regLeaf 是产生叶节点的函数，就是求均值，即用聚类中心点来代表这类数据\n",
    "def regLeaf(dataSet):\n",
    "    return mean(dataSet[:, -1])\n",
    "\n",
    "\n",
    "# 计算总方差=方差*样本数\n",
    "# 我的理解是：求这组数据的方差，即通过决策树划分，可以让靠近的数据分到同一类中去\n",
    "def regErr(dataSet):\n",
    "    # shape(dataSet)[0] 表示行数\n",
    "    return var(dataSet[:, -1]) * shape(dataSet)[0]\n",
    "\n",
    "\n",
    "# 1.用最佳方式切分数据集\n",
    "# 2.生成相应的叶节点\n",
    "def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n",
    "    \"\"\"chooseBestSplit(用最佳方式切分数据集 和 生成相应的叶节点)\n",
    "\n",
    "    Args:\n",
    "        dataSet   加载的原始数据集\n",
    "        leafType  建立叶子点的函数\n",
    "        errType   误差计算函数(求总方差)\n",
    "        ops       [容许误差下降值，切分的最少样本数]。\n",
    "    Returns:\n",
    "        bestIndex feature的index坐标\n",
    "        bestValue 切分的最优值\n",
    "    Raises:\n",
    "    \"\"\"\n",
    "\n",
    "    # ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。\n",
    "    # 之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。\n",
    "    # 最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分\n",
    "    tolS = ops[0]\n",
    "    # 划分最小 size 小于，就不继续划分了\n",
    "    tolN = ops[1]\n",
    "    #如果数据集的最后一列所有值相等就退出\n",
    "    #dataSet[:, -1].T.tolist()[0] 取数据集的最后一列，转置为行向量，然后转换为list,取该list中的第一个元素。\n",
    "    if len(set(dataSet[:, -1].T.tolist()[0])) == 1: # 如果集合size为1，也就是说全部的数据都是同一个类别，不用继续划分。\n",
    "        #  exit cond 1\n",
    "        return None, leafType(dataSet)\n",
    "    # 计算行列值\n",
    "    m, n = shape(dataSet)\n",
    "    # 无分类误差的总方差和\n",
    "    # the choice of the best feature is driven by Reduction in RSS error from mean\n",
    "    S = errType(dataSet)\n",
    "    # inf 正无穷大\n",
    "    bestS, bestIndex, bestValue = inf, 0, 0\n",
    "    # 循环处理每一列对应的feature值\n",
    "    for featIndex in range(n-1): # 对于每个特征\n",
    "        # 下面的一行表示的是将某一列全部的数据转换为行，然后设置为list形式\n",
    "        for splitVal in set(dataSet[:, featIndex].T.tolist()[0]):\n",
    "            # 对该列进行分组，然后组内的成员的val值进行 二元切分\n",
    "            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\n",
    "            # 判断二元切分的方式的元素数量是否符合预期\n",
    "            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):\n",
    "                continue\n",
    "            newS = errType(mat0) + errType(mat1)\n",
    "            # 如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差\n",
    "            # 如果划分后误差小于 bestS，则说明找到了新的bestS\n",
    "            if newS < bestS:\n",
    "                bestIndex = featIndex\n",
    "                bestValue = splitVal\n",
    "                bestS = newS\n",
    "    # 判断二元切分的方式的元素误差是否符合预期\n",
    "    # if the decrease (S-bestS) is less than a threshold don't do the split\n",
    "    if (S - bestS) < tolS:\n",
    "        return None, leafType(dataSet)\n",
    "    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n",
    "    # 对整体的成员进行判断，是否符合预期\n",
    "    # 如果集合的 size 小于 tolN \n",
    "    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): # 当最佳划分后，集合过小，也不划分，产生叶节点\n",
    "        return None, leafType(dataSet)\n",
    "    return bestIndex, bestValue\n",
    "\n",
    "\n",
    "# assume dataSet is NumPy Mat so we can array filtering\n",
    "# 假设 dataSet 是 NumPy Mat 类型的，那么我们可以进行 array 过滤\n",
    "def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n",
    "    \"\"\"createTree(获取回归树)\n",
    "        Description：递归函数：如果构建的是回归树，该模型是一个常数，如果是模型树，其模型师一个线性方程。\n",
    "    Args:\n",
    "        dataSet      加载的原始数据集\n",
    "        leafType     建立叶子点的函数\n",
    "        errType      误差计算函数\n",
    "        ops=(1, 4)   [容许误差下降值，切分的最少样本数]\n",
    "    Returns:\n",
    "        retTree    决策树最后的结果\n",
    "    \"\"\"\n",
    "    # 选择最好的切分方式： feature索引值，最优切分值\n",
    "    # choose the best split\n",
    "    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n",
    "    # if the splitting hit a stop condition return val\n",
    "    # 如果 splitting 达到一个停止条件，那么返回 val\n",
    "    if feat is None:\n",
    "        return val\n",
    "    retTree = {}\n",
    "    retTree['spInd'] = feat\n",
    "    retTree['spVal'] = val\n",
    "    # 大于在右边，小于在左边，分为2个数据集\n",
    "    lSet, rSet = binSplitDataSet(dataSet, feat, val)\n",
    "    # 递归的进行调用，在左右子树中继续递归生成树\n",
    "    retTree['left'] = createTree(lSet, leafType, errType, ops)\n",
    "    retTree['right'] = createTree(rSet, leafType, errType, ops)\n",
    "    return retTree\n",
    "\n",
    "\n",
    "# 判断节点是否是一个字典\n",
    "def isTree(obj):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        测试输入变量是否是一棵树,即是否是一个字典\n",
    "    Args:\n",
    "        obj -- 输入变量\n",
    "    Returns:\n",
    "        返回布尔类型的结果。如果 obj 是一个字典，返回true，否则返回 false\n",
    "    \"\"\"\n",
    "    return (type(obj).__name__ == 'dict')\n",
    "\n",
    "\n",
    "# 计算左右枝丫的均值\n",
    "def getMean(tree):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        从上往下遍历树直到叶节点为止，如果找到两个叶节点则计算它们的平均值。\n",
    "        对 tree 进行塌陷处理，即返回树平均值。\n",
    "    Args:\n",
    "        tree -- 输入的树\n",
    "    Returns:\n",
    "        返回 tree 节点的平均值\n",
    "    \"\"\"\n",
    "    if isTree(tree['right']):\n",
    "        tree['right'] = getMean(tree['right'])\n",
    "    if isTree(tree['left']):\n",
    "        tree['left'] = getMean(tree['left'])\n",
    "    return (tree['left']+tree['right'])/2.0\n",
    "\n",
    "\n",
    "# 检查是否适合合并分枝\n",
    "def prune(tree, testData):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        从上而下找到叶节点，用测试数据集来判断将这些叶节点合并是否能降低测试误差\n",
    "    Args:\n",
    "        tree -- 待剪枝的树\n",
    "        testData -- 剪枝所需要的测试数据 testData \n",
    "    Returns:\n",
    "        tree -- 剪枝完成的树\n",
    "    \"\"\"\n",
    "    # 判断是否测试数据集没有数据，如果没有，就直接返回tree本身的均值\n",
    "    if shape(testData)[0] == 0:\n",
    "        return getMean(tree)\n",
    "\n",
    "    # 判断分枝是否是dict字典，如果是就将测试数据集进行切分\n",
    "    if (isTree(tree['right']) or isTree(tree['left'])):\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "    # 如果是左边分枝是字典，就传入左边的数据集和左边的分枝，进行递归\n",
    "    if isTree(tree['left']):\n",
    "        tree['left'] = prune(tree['left'], lSet)\n",
    "    # 如果是右边分枝是字典，就传入左边的数据集和左边的分枝，进行递归\n",
    "    if isTree(tree['right']):\n",
    "        tree['right'] = prune(tree['right'], rSet)\n",
    "\n",
    "    # 上面的一系列操作本质上就是将测试数据集按照训练完成的树拆分好，对应的值放到对应的节点\n",
    "\n",
    "    # 如果左右两边同时都不是dict字典，也就是左右两边都是叶节点，而不是子树了，那么分割测试数据集。\n",
    "    # 1. 如果正确 \n",
    "    #   * 那么计算一下总方差 和 该结果集的本身不分枝的总方差比较\n",
    "    #   * 如果 合并的总方差 < 不合并的总方差，那么就进行合并\n",
    "    # 注意返回的结果： 如果可以合并，原来的dict就变为了 数值\n",
    "    if not isTree(tree['left']) and not isTree(tree['right']):\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "        # power(x, y)表示x的y次方\n",
    "        errorNoMerge = sum(power(lSet[:, -1] - tree['left'], 2)) + sum(power(rSet[:, -1] - tree['right'], 2))\n",
    "        treeMean = (tree['left'] + tree['right'])/2.0\n",
    "        errorMerge = sum(power(testData[:, -1] - treeMean, 2))\n",
    "        # 如果 合并的总方差 < 不合并的总方差，那么就进行合并\n",
    "        if errorMerge < errorNoMerge:\n",
    "            print(\"merging\")\n",
    "            return treeMean\n",
    "        else:\n",
    "            return tree\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "\n",
    "# 得到模型的ws系数：f(x) = x0 + x1*featrue1+ x3*featrue2 ...\n",
    "# create linear model and return coeficients\n",
    "def modelLeaf(dataSet):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        当数据不再需要切分的时候，生成叶节点的模型。\n",
    "    Args:\n",
    "        dataSet -- 输入数据集\n",
    "    Returns:\n",
    "        调用 linearSolve 函数，返回得到的 回归系数ws\n",
    "    \"\"\"\n",
    "    ws, X, Y = linearSolve(dataSet)\n",
    "    return ws\n",
    "\n",
    "\n",
    "# 计算线性模型的误差值\n",
    "def modelErr(dataSet):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        在给定数据集上计算误差。\n",
    "    Args:\n",
    "        dataSet -- 输入数据集\n",
    "    Returns:\n",
    "        调用 linearSolve 函数，返回 yHat 和 Y 之间的平方误差。\n",
    "    \"\"\"\n",
    "    ws, X, Y = linearSolve(dataSet)\n",
    "    yHat = X * ws\n",
    "    # print corrcoef(yHat, Y, rowvar=0)\n",
    "    return sum(power(Y - yHat, 2))\n",
    "\n",
    "\n",
    " # helper function used in two places\n",
    "def linearSolve(dataSet):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        将数据集格式化成目标变量Y和自变量X，执行简单的线性回归，得到ws\n",
    "    Args:\n",
    "        dataSet -- 输入数据\n",
    "    Returns:\n",
    "        ws -- 执行线性回归的回归系数 \n",
    "        X -- 格式化自变量X\n",
    "        Y -- 格式化目标变量Y\n",
    "    \"\"\"\n",
    "    m, n = shape(dataSet)\n",
    "    # 产生一个关于1的矩阵\n",
    "    X = mat(ones((m, n)))\n",
    "    Y = mat(ones((m, 1)))\n",
    "    # X的0列为1，常数项，用于计算平衡误差\n",
    "    X[:, 1: n] = dataSet[:, 0: n-1]\n",
    "    Y = dataSet[:, -1]\n",
    "\n",
    "    # 转置矩阵*矩阵\n",
    "    xTx = X.T * X\n",
    "    # 如果矩阵的逆不存在，会造成程序异常\n",
    "    if linalg.det(xTx) == 0.0:\n",
    "        raise NameError('This matrix is singular, cannot do inverse,\\ntry increasing the second value of ops')\n",
    "    # 最小二乘法求最优解:  w0*1+w1*x1=y\n",
    "    ws = xTx.I * (X.T * Y)\n",
    "    return ws, X, Y\n",
    "\n",
    "\n",
    "# 回归树测试案例\n",
    "# 为了和 modelTreeEval() 保持一致，保留两个输入参数\n",
    "def regTreeEval(model, inDat):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        对 回归树 进行预测\n",
    "    Args:\n",
    "        model -- 指定模型，可选值为 回归树模型 或者 模型树模型，这里为回归树\n",
    "        inDat -- 输入的测试数据\n",
    "    Returns:\n",
    "        float(model) -- 将输入的模型数据转换为 浮点数 返回\n",
    "    \"\"\"\n",
    "    return float(model)\n",
    "\n",
    "\n",
    "# 模型树测试案例\n",
    "# 对输入数据进行格式化处理，在原数据矩阵上增加第0列，元素的值都是1，\n",
    "# 也就是增加偏移值，和我们之前的简单线性回归是一个套路，增加一个偏移量\n",
    "def modelTreeEval(model, inDat):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        对 模型树 进行预测\n",
    "    Args:\n",
    "        model -- 输入模型，可选值为 回归树模型 或者 模型树模型，这里为模型树模型\n",
    "        inDat -- 输入的测试数据\n",
    "    Returns:\n",
    "        float(X * model) -- 将测试数据乘以 回归系数 得到一个预测值 ，转化为 浮点数 返回\n",
    "    \"\"\"\n",
    "    n = shape(inDat)[1]\n",
    "    X = mat(ones((1, n+1)))\n",
    "    X[:, 1: n+1] = inDat\n",
    "    # print X, model\n",
    "    return float(X * model)\n",
    "\n",
    "\n",
    "# 计算预测的结果\n",
    "# 在给定树结构的情况下，对于单个数据点，该函数会给出一个预测值。\n",
    "# modelEval是对叶节点进行预测的函数引用，指定树的类型，以便在叶节点上调用合适的模型。\n",
    "# 此函数自顶向下遍历整棵树，直到命中叶节点为止，一旦到达叶节点，它就会在输入数据上\n",
    "# 调用modelEval()函数，该函数的默认值为regTreeEval()\n",
    "def treeForeCast(tree, inData, modelEval=regTreeEval):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        对特定模型的树进行预测，可以是 回归树 也可以是 模型树\n",
    "    Args:\n",
    "        tree -- 已经训练好的树的模型\n",
    "        inData -- 输入的测试数据\n",
    "        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树\n",
    "    Returns:\n",
    "        返回预测值\n",
    "    \"\"\"\n",
    "    if not isTree(tree):\n",
    "        return modelEval(tree, inData)\n",
    "    if inData[tree['spInd']] <= tree['spVal']:\n",
    "        if isTree(tree['left']):\n",
    "            return treeForeCast(tree['left'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['left'], inData)\n",
    "    else:\n",
    "        if isTree(tree['right']):\n",
    "            return treeForeCast(tree['right'], inData, modelEval)\n",
    "        else:\n",
    "            return modelEval(tree['right'], inData)\n",
    "\n",
    "\n",
    "# 预测结果\n",
    "def createForeCast(tree, testData, modelEval=regTreeEval):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        调用 treeForeCast ，对特定模型的树进行预测，可以是 回归树 也可以是 模型树\n",
    "    Args:\n",
    "        tree -- 已经训练好的树的模型\n",
    "        inData -- 输入的测试数据\n",
    "        modelEval -- 预测的树的模型类型，可选值为 regTreeEval（回归树） 或 modelTreeEval（模型树），默认为回归树\n",
    "    Returns:\n",
    "        返回预测值矩阵\n",
    "    \"\"\"\n",
    "    m = len(testData)\n",
    "    yHat = mat(zeros((m, 1)))\n",
    "    # print yHat\n",
    "    for i in range(m):\n",
    "        yHat[i, 0] = treeForeCast(tree, mat(testData[i]), modelEval)\n",
    "        # print \"yHat==>\", yHat[i, 0]\n",
    "    return yHat\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # # 测试数据集\n",
    "    # testMat = mat(eye(4))\n",
    "    # print testMat\n",
    "    # print type(testMat)\n",
    "    # mat0, mat1 = binSplitDataSet(testMat, 1, 0.5)\n",
    "    # print mat0, '\\n-----------\\n', mat1\n",
    "\n",
    "    # # 回归树\n",
    "    # myDat = loadDataSet('input/9.RegTrees/data1.txt')\n",
    "    # # myDat = loadDataSet('input/9.RegTrees/data2.txt')\n",
    "    # # print 'myDat=', myDat\n",
    "    # myMat = mat(myDat)\n",
    "    # # print 'myMat=',  myMat\n",
    "    # myTree = createTree(myMat)\n",
    "    # print myTree\n",
    "\n",
    "    # # 1. 预剪枝就是：提起设置最大误差数和最少元素数\n",
    "    # myDat = loadDataSet('input/9.RegTrees/data3.txt')\n",
    "    # myMat = mat(myDat)\n",
    "    # myTree = createTree(myMat, ops=(0, 1))\n",
    "    # print myTree\n",
    "\n",
    "    # # 2. 后剪枝就是：通过测试数据，对预测模型进行合并判断\n",
    "    # myDatTest = loadDataSet('input/9.RegTrees/data3test.txt')\n",
    "    # myMat2Test = mat(myDatTest)\n",
    "    # myFinalTree = prune(myTree, myMat2Test)\n",
    "    # print '\\n\\n\\n-------------------'\n",
    "    # print myFinalTree\n",
    "\n",
    "    # # --------\n",
    "    # # 模型树求解\n",
    "    # myDat = loadDataSet('input/9.RegTrees/data4.txt')\n",
    "    # myMat = mat(myDat)\n",
    "    # myTree = createTree(myMat, modelLeaf, modelErr)\n",
    "    # print myTree\n",
    "\n",
    "    # # 回归树 VS 模型树 VS 线性回归\n",
    "    trainMat = mat(loadDataSet('./dataset/bikeSpeedVsIq_train.txt'))\n",
    "    testMat = mat(loadDataSet('./dataset/bikeSpeedVsIq_test.txt'))\n",
    "    # # 回归树\n",
    "    myTree1 = createTree(trainMat, ops=(1, 20))\n",
    "    print(myTree1)\n",
    "    yHat1 = createForeCast(myTree1, testMat[:, 0])\n",
    "    print(\"--------------\\n\")\n",
    "    # print yHat1\n",
    "    # print \"ssss==>\", testMat[:, 1]\n",
    "    print(\"回归树:\", corrcoef(yHat1, testMat[:, 1],rowvar=0)[0, 1])\n",
    "\n",
    "    # 模型树\n",
    "    myTree2 = createTree(trainMat, modelLeaf, modelErr, ops=(1, 20))\n",
    "    yHat2 = createForeCast(myTree2, testMat[:, 0], modelTreeEval)\n",
    "    print(myTree2)\n",
    "    print(\"模型树:\", corrcoef(yHat2, testMat[:, 1],rowvar=0)[0, 1])\n",
    "\n",
    "    # 线性回归\n",
    "    ws, X, Y = linearSolve(trainMat)\n",
    "    print(ws)\n",
    "    m = len(testMat[:, 0])\n",
    "    yHat3 = mat(zeros((m, 1)))\n",
    "    for i in range(shape(testMat)[0]):\n",
    "        yHat3[i] = testMat[i, 0]*ws[1, 0] + ws[0, 0]\n",
    "    print(\"线性回归:\", corrcoef(yHat3, testMat[:, 1],rowvar=0)[0, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
