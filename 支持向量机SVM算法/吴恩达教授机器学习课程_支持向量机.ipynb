{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这一章我们主要学习    \n",
    "\n",
    "\n",
    "* 1.1 优化问题\n",
    "* 1.2 直观理解大间距分类器\n",
    "* 1.3 大间距分类器背后的数学原理 (选学)\n",
    "* 1.4 核技巧 I  \n",
    "* 1.5 核技巧 II\n",
    "* 1.6 使用 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 优化问题\n",
    "\n",
    "[吴恩达教授机器学习课程:  11.1 优化问题     视频观看地址](https://www.bilibili.com/video/av9912938/?p=66 )  \n",
    "\n",
    "**这一节我们主要学习以下内容**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 支持向量机（SVM）\n",
    "+ 由逻辑回归代价函数引入SVM代价函数\n",
    "+ SVM的假设  \n",
    "\n",
    "**支持向量机(Support Vector Machine)**是一个广泛应用于工业界和学术界的更加强大的算法。与逻辑回归和神经网络相比，支持向量机，或者简称 SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。  \n",
    "\n",
    "这是逻辑回归的优化目标：  \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559120772379_gHpZ2JK2Hz.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，用z来表示$\\theta^Tx$。\n",
    "\n",
    "在逻辑回归中：如果有一个$y=1$的样本，想要正确地将此样本分类，则$h_\\theta(x)$要趋近1，这就意味着当$\\theta^Tx$应当远大于$0$，这里的>>意思是远远大于$0$。由于$z$表示$\\theta^Tx$，当$z$远大于$0$时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于$1$。相反地，如果有另一个样本$y=0$，那么我们希假设函数的输出值应该趋近于$0$，这对应于 $\\theta^Tx$或者就是$z$会远小于 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559120866836_ry6MtyRm83.jpg)  \n",
    "\n",
    "进一步观察逻辑回归的代价函数，会发现每个样本 (x, y)都会为总代价函数增加了一项，因此，对于总代价函数通常会有对所有的训练样本求和，并且还有一个 1/m 项，\n",
    "\n",
    "现在，先忽略 1/m 这一项，但是这一项是影响整个总代价函数中的这一项的。一起来考虑两种情况：一种是 y 等于 1 的情况；另一种是 y 等于 0 的情况。\n",
    "\n",
    "在第一种情况中，假设 y=1，此时在目标函数中只需有第一项起作用，因为 y 等于 1 时，(1-y) 项将等于0。因此，当在 y=1 的样本中时，即在 (x, y) 中 y 等于 1，我们得到这样一项$-log(1-\\frac{1}{1 + e^{-z}})$：用z来表示$\\theta^Tx$。当然，在代价函数中，y 前面有负号。如果画出关于 z 的函数，你会看到这条曲线，当 z 增大时，也就是$\\theta^Tx$增大时，z 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本 y=1 时，试图将$\\theta^Tx$设置得非常大。因为，在代价函数中的这\n",
    "一项会变的非常小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559121399661_8PLKVGBefx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用z来表示$\\theta^Tx$。当然，在代价函数中，y 前面有负号。如果画出关于 z 的函数，你会看到这条曲线，当 z 增大时，也就是$\\theta^Tx$增大时，z 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本 y=1 时，试图将$\\theta^Tx$设置得非常大。因为，在代价函数中的这\n",
    "一项会变的非常小。  \n",
    "\n",
    "逻辑回归的代价函数，也就是$-log(1-\\frac{1}{1 + e^{-z}})$。取z=1 点，先画出将要用的代价函数。 然后我再画一条同逻辑回归非常相似的直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559121516790_vNWdypBJTz.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给这两个方程（紫红色曲线）命名，左边的函数为$\\cos t^{(z)}_1$，右边函数为$\\cos t^{(z)}_0$ 。下标是指在代价函数中对应的 y=1 和 y=0 的情况.\n",
    "\n",
    "\n",
    "这是我们在逻辑回归中使用代价函数$J(\\theta)$,将负号移到了表达式的里面:  \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559122069953_mhOLrWLA71.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于支持向量机而言，实质上我们要将这替换为$\\cos t^{(z)}_1$ ，也就是 $\\cos t^{(\\theta^Tx)}_1$ ，同样地，我也将这一项替换为$\\cos t^{(\\theta^Tx)}_0$。这里的代价函数$\\cos t^{(\\theta^Tx)}_1$，就是之前所提到的那条线。然后，再加上正则化参数。对于支持向量机，我们得到了这里的最小化问题，即:\n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559122252627_7Jgo5u5Dlf.jpg)\n",
    " \n",
    "这个公式有一些不同：\n",
    "1. 1/m 这一项被除去。仅仅是由于人们使用支持向量机时的习惯所致，1/m 仅是个常量不影响最终得到的最优值$\\theta$。\n",
    "2. 概念上的变化。训练样本的代价表示为A，正则化项为B。逻辑回归的目标函数为$A+\\lambda B$，通过设置不同正则参数$\\lambda$达到优化目的,即最小化A。但对于支持向量机，目标函数为$C×A+B$，这里的参数 C 可以考虑成$\\frac{1}{\\lambda}$，同 1/λ 所扮演的角色相同。，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心代价函数的优化，还是更关心正则项的优化。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559126246523_tOOtcZYURF.jpg) \n",
    "\n",
    "最后有别于逻辑回归输出的概率。在这里，我们的代价函数，获得参数 𝜃 时，支持向量机所做的是它来直接预测 y 的值等于 1，还是等于 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 直观理解大间距分类器\n",
    "\n",
    "[吴恩达教授机器学习课程:  11.2 直观理解大间距分类器     视频观看地址](https://www.bilibili.com/video/av9912938/?p=67 )  \n",
    "\n",
    "**这一节我们主要学习以下内容**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 大间距分类器\n",
    "+ 直观理解SVM模型\n",
    "+ 对于参数C的选择  \n",
    "\n",
    "这是支持向量机模型的代价函数  \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559126601749_4r7EYjwFOR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个正样本$y=1$，则只有在$z\\geq1$时，代价函数$\\cos t^{(z)}_1$才等于 0。换句话说，如果你有一个正样本，我们会希望 $\\theta^Tx\\ge1$，反之，如果$y=0$的，函数$\\cos t^{(z)}_0$只有在$ z\\leq1$的区间里函数值为0。事实上，如果你有一个正样本$y=1$，则仅仅要求 $\\theta^Tx\\ge0$，就能将该样本恰当分出，类似地，如果你有一个负样本，则仅需要  $\\theta^Tx\\leq0$就会将负例正确分离。\n",
    "\n",
    "但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求 $\\theta^Tx\\ge0$，我们需要的是比 0 值大很多，比如大于等于 1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。 \n",
    "\n",
    "考虑一个特例：我们将这个常数 C 设置成一个非常大的值，比如假设 C 的值为 100000，然后来观察支持向量机会给出什么结果。\n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559126706031_kI3FJoMqIe.jpg)\n",
    " \n",
    " 如果 C 非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为 0 的最优解。因此，让我们尝试在代价项的第一项为 0 的情形下理解该优化问题。\n",
    " \n",
    " 这将遵从以下的约束：如果$y^{(i)}=1$，则要求$\\theta^Tx^{(i)}\\geq1$;如果$y^{(i)}=0$，则要求$\\theta^Tx^{(i)}\\leq-1$;\n",
    " \n",
    "这样当你求解这个优化问题的时候，当你最小化这个关于变量$\\theta$的函数的时候，你会得到一个非常有趣的决策边界。 \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559126770320_mGgIzuVvCg.jpg)\n",
    "\n",
    "具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。我的意思是，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。支持向量机将会选择这个黑色的决策边界，相较于之前粉色或者绿色的决策边界。黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线有更大的距离，这个距离叫做**间距 (margin)**。 这个距离叫做支持向量机的间距，这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点 (outlier) 的影响。比如我们加入一个额外的正样本。  \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559126883705_onuCwNtPTb.jpg)  \n",
    "\n",
    "如果加了这个样本，为了将样本用最大间距分开，最终会得到一条类似粉色的线的决策界。仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数 C设置的非常大，支持向量机会将决策界从黑线变到了粉线，但是**如果将 C 设置的不要太大，则你最终会得到这条黑线**。\n",
    "\n",
    "实际上，应用支持向量机的时候，当 C 不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。\n",
    "\n",
    "回顾 C=1/λ，因此： \n",
    "+ C 较大时，相当于 λ 较小，可能会导致过拟合，高方差。 \n",
    "+ C 较小时，相当于 λ 较大，可能会导致低拟合，高偏差。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 大间距分类器背后的数学原理 (选学)\n",
    "\n",
    "[吴恩达教授机器学习课程:  1.3 大间距分类器背后的数学原理 (选学)     视频观看地址](https://www.bilibili.com/video/av9912938/?p=68 )  \n",
    "\n",
    "**这一节我们主要学习以下内容**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 大间距分类器的数学实现\n",
    "+ 向量的内积\n",
    "+ SVM的目标函数  \n",
    "\n",
    "假设有两个向量，u和v，都是二维向量。$u^Tv$叫做向量u 和v之间的**内积**。\n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559127065753_1aP0JKSWiK.jpg)\n",
    "\n",
    "将两个向量画在图上。\n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559127129471_wJXkQKAnnb.jpg)\n",
    "\n",
    "向量u在横轴上，取值为某个$u_1$，而在纵轴上，高度是$u_2$作为u的第二个分量。现在，很容易计算的一个量就是向量u的范数$||u||$,即向量$u$的欧几里得长度。根据毕达哥拉斯定理， $||u||=\\sqrt{u_1^2+u_2^2}$，这是向量u的长度，它是一个实数。\n",
    "\n",
    "计算u和v之间的内积的几何做法，我们将向量v做一个直角投影到向量u上，度量这条红线的长度p，因此 p 就是向量v投影到向量u上的量。因此可以将$u^Tv=p\\cdot||u||$ 。\n",
    "\n",
    "另一种线性代数方法，$u^Tv=u_1\\times v_1+u_2\\times v_2$,会给出同样的结果。\n",
    "\n",
    "顺便，有$u^Tv=v^Tu$。\n",
    "\n",
    "需要注意的就是p值是有符号的，即它可能是正值，也可能是负值。如下图，如果u 和v之间的夹角大于 90 度，则如果将v投影到u 上，会得到这样的一个投影，这是 p 的长度，在这个情形下我们仍然有$u^Tv=p\\cdot||u||$，唯一一点不同的是 p 在这里是负的。在内积计算中，如果u 和v之间的夹角小于 90 度，那么那条红线的长度 p 是正值。然而如果这个夹角大于 90 度，则 p 将会是负的。就是这个小线段的长度是负的。如果它们之间的夹角大于 90 度，两个向量之间的内积也是负的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559127197412_K4GwcQBAFM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559127283621_8eUIzImbOe.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这就是我们先前给出的支持向量机模型中的目标函数。接下来忽略掉截距，令$\\theta_0=0$，这样更容易画示意图。假设特征数 n = 2，则仅有两个特征$x_1$,$x_2$。\n",
    "此时，支持向量机的优化目标函数可以写作： \n",
    "$$\\min_\\theta\\frac{1}{2}\\sum^n_{j=1}\\theta^2_j = \\frac{1}{2}(\\theta^2_1+\\theta^2_2) = \\frac{1}{2}\\sqrt{\\theta_1^2+\\theta_2^2}=\\frac{1}{2}||theta||^2$$\n",
    "\n",
    "当然你可以将其写作$\\theta_0,\\theta_1,\\theta_2$，但是，数学上不管你是否包含，其实并没有差别，因此在我们接下来的推导中去掉$\\theta_0$不会有影响这意味着我们的目标函数是等于\n",
    "$\\frac{1}{2}||theta||^2$。因此**支持向量机做的全部事情，就是极小化参数向量$\\theta$范数的平方。** \n",
    "\n",
    "现在我们将要深入地理解$\\theta^Tx$。给定参数向量$\\theta$给定一个样本$x^{(i)}就类似于u和v。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559127564345_weha0Rx9Js.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用我们之前的方法，我们计算的方式就是我将训练样本投影到参数向量$\\theta$，然后我来看一看这个线段的长度，我将它画成红色。我将它称为$ p^{(i)}$用来表示这是第i个训练样本在参数向量$\\theta$上的投影,$\\theta^Tx$将会等于p乘以向量$\\theta$的长度或范数。\n",
    "\n",
    "这里表达的意思是：这个$\\theta^Tx^{(i)} \\geq1$或者$\\theta^Tx^{(i)} < -1$的,约束是可以被$p^{(i)} \\cdot x \\geq 1$这个约束所代替的。我们的优化目标就变成了$p^{(i)} \\cdot x$\n",
    "。 \n",
    " \n",
    "\n",
    "需要提醒一点，我们之前曾讲过这个优化目标函数可以被写成等于$\\frac{1}{2}||theta||^2$ 。 \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559127941786_4tCgdzTMzL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设支持向量机会选择这个绿色决策边界。这不是一个非常好的选择，因为它的间距很小。这个决策界离训练样本的距离很近。我们来看一下为什么支持向量机不会选择它。 \n",
    "\n",
    "对于这样选择的参数$\\theta$，可以看到参数向量$\\theta$。事实上是和决策界是 90 度正交的，因此这个绿色的决策界对应着一个参数向量$\\theta$指向这个方向,顺便提一句$\\theta_0=0$的简化仅仅意味着决策界必须通过原点 (0,0)。现在让我们看一下这对于优化目标函数意味着什么。 \n",
    " \n",
    "比如这个样本，我们假设它是我的第一个样本$x^{(1)}$，如果我考察这个样本到参数$\\theta$的投影，投影是这个短的红线段，就等于$p^{(1)}$。类似地，样本$x^{(2)}$，它到$\\theta$的投影在这里。投影是这个短的粉线段$p^{(2)}$，它事实上是一个负值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559128317757_NQHg0uh3Nb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果这是决策界，这就是相对应的参数 θ 的方向，因此，在这个决策界之下，垂直线是决策界。使用线性代数的知识，可以说明，这个绿色的决策界有一个垂直于它的向量$\\theta$ 。现在如果你考察你的数据在横轴 x 上的投影，比如这个我之前提到的样本，我的样本 $x^{(1)}$，当我将它投影到横轴 x 上，或说投影到 θ 上，就会得到这样的$p^{(1)}$。它的长度是 $p^{(1)}$，另一个样本，那个样本是 x(2)。我做同样的投影，我会发现，$p^{(2)}$的长度是负值。你会注意到现在$p^{(1)}$和$p^{(2)}$这些投影长度是长多了。如果我们仍然要满足这些约束，$p^{(i)} \\cdot x > 1$，这意味着通过选择右边的决策界，而不是左边的那个，支持向量机可以使参数$\\theta$的范数变小很多。因此，如果我们想令$\\theta$的范数变小，从而令$\\theta$范数的平方变小，就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。\n",
    "     \n",
    "看这条绿线，这个绿色的决策界。我们希望正样本和负样本投影到 θ 的值大。要做到这一点的唯一方式就是选择这条绿线做决策界。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是$p^{(1)}$,$p^{(2)}$,$p^{(3)}$等等的值。通过让间距变大，即通过这些$p^{(1)}$,$p^{(2)}$,$p^{(3)}$等等的值，支持向量机最终可以找到一个较小的$\\theta$范数。这正是支持向量机中最小化目标函数的目的。 \n",
    "\n",
    "以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图极大化这些$p^{(i)}$的范数，它们是训练样本到决策边界的距离。\n",
    "\n",
    "最后一点，我们的推导自始至终使用了这个简化假设，就是参数 $\\theta_0=0$。它能够让决策界通过原点。如果你令\n",
    "$\\theta_0$不是 0 的话，含义就是你希望决策界不通过原点。我将不会做全部的推导。实际上，支持向量机产生大间距分类器的结论，会被证明同样成立，证明方式是非常类似的，是我们刚刚做的证明的推广。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 核技巧 I\n",
    "\n",
    "[吴恩达教授机器学习课程:  1.4 核技巧 I     视频观看地址](https://www.bilibili.com/video/av9912938/?p=69 )  \n",
    "\n",
    "**这一节我们主要学习以下内容**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 回顾分类问题的多项式模型\n",
    "+ 高斯核函数  \n",
    "\n",
    "分类问题的多项式模型：  \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559128532935_Jg8VxJzZLv.jpg)  \n",
    "\n",
    "为了获得上图所示的判定边界，我们的模型可能是$\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1x_2+\\theta_4x^2_1+\\theta_5x^2_2+...$的形式。 \n",
    "\n",
    "我们可以用一系列的新的特征 f 来替换模型中的每一项。例如令：\n",
    "\n",
    "$f_1 =x_1, f_2=x_2,f_3=x_1x_2, f_4=x^2_1, f_5=x^2_2,...$\n",
    " 得到 $h_\\theta(x)=f1+f2+...+fn$。\n",
    " \n",
    " 然而，除了对原有的特征进行组合以外，有没有更好的方法来构造 f1,f2,f3？我们可以利用核函数来计算出新的特征。   \n",
    " \n",
    " 给定一个训练实例x，我们利用x的各个特征与我们预先选定的地标（landmarks） $l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征 f1,f2,f3。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559128636289_CqThO2hCAy.jpg)  \n",
    "\n",
    "例如： \n",
    " $$f_1=similarity(x,l^{(1)})= exp(-\\frac{||x-l^{(1)}||^2}{2\\sigma^2})$$\n",
    "其中：$||x-l^{(1)}||^2=\\sum^n_{j=1}(x_j-l^{(1)}_j)^2$为实例 x 中所有特征与地标$l^{(1)}$之间的距离的和。\n",
    "\n",
    "上例中的 $similarity（x,l^{(1)})$就是核函数，具体而言，这里是一个高斯核函数（Gaussian Kernel）。 **注：这个函数与正态分布没什么实际上的关系，只是看上去像而已.**\n",
    "\n",
    "这些地标的作用是什么？如果一个训练实例 x 与地标 L 之间的距离近似于 0，则新特征 f 近似于$ e^{-0}=1$，如果训练实例 x 与地标 L 之间距离较远，则 f 近似于 $e^{-(一个较大的数)}=0$。 \n",
    "\n",
    "假设我们的训练实例含有两个特征$[x_1 x_2]$，给定地标 $l^{(1)}$与不同的 σ 值，见下图：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559128844126_pL4RsZaz4X.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中水平面的坐标为$[x_1 x_2]$,而垂直坐标轴代表 f。可以看出，只有当 x 与 $l^{(1)}$重合时 f 才具有最大值。随着 x 的改变 f 值改变的速率受到 σ2的控制。 \n",
    "\n",
    "在下图中，当实例处于洋红色的点位置处，因为其离$l^{(1)}$更近，但是离$l^{(2)}$和$l^{(3)}$较远，因此 f1接近 1，而 f2,f3 接近 0。因此 hθ(x)=θ0+θ1f1+θ2f2+θ1f3>0，因此预测 y=1。同理可以求出，\n",
    "对于离 l(2)较近的绿色点，也预测 y=1，但是对于蓝绿色的点，因为其离三个地标都较远，预测 y=0。 \n",
    "\n",
    "![](http://jupter-oss.oss-cn-hangzhou.aliyuncs.com/public/files/image/1095279408848/1559128931427_85v0WEz6KA.jpg)\n",
    " \n",
    "这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选\n",
    "取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通 过核函数计算出的新特征 f1,f2,f3。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 核技巧 II\n",
    "\n",
    "[吴恩达教授机器学习课程:  1.5 核技巧 II     视频观看地址](https://www.bilibili.com/video/av9912938/?p=70 )  \n",
    "\n",
    "**这一节我们主要学习以下内容**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
