{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归原理小结\n",
    "\n",
    "线性回归可以说是机器学习中最基本的问题类型了，这里就对线性回归的原理和算法做一个小结。\n",
    "\n",
    "## 1. 线性回归的模型函数和损失函数\n",
    "\n",
    "线性回归遇到的问题一般是这样的。我们有m个样本，每个样本对应于n维特征和一个结果输出，如下：\n",
    "\n",
    "$$(x_1^{(0)},x_2^{(0)},...x_n^{(0)},y_0),(x_1^{(1)},x_2^{(1)},...x_n^{(1)},y_1),...(x_1^{(m)},x_2^{(m)},...x_n^{(m)},y_n)$$\n",
    "\n",
    "我们的问题是，对于一个新的$$(x_1^{(x)},x_2^{(x)},...x_n^{(x)})$$, 他所对应的$$y_x$$是多少呢？ 如果这个问题里面的$$y$$是连续的，则是一个回归问题，否则是一个分类问题。\n",
    "\n",
    "对于n维特征的样本数据，如果我们决定使用线性回归，那么对应的模型是这样的：\n",
    "\n",
    "$$h_θ(x_1,x_2,...x_n)=θ_0+θ_1x_1+...+θ_nx_n$$, \n",
    "\n",
    "其中$$θ_i (i = 0,1,2... n)$$为模型参数,\n",
    "\n",
    "$$x_i (i = 0,1,2... n)$$为每个样本的n个特征值。\n",
    "\n",
    "这个表示可以简化，我们增加一个特征$$x_0=1$$ ，这样$$h_θ(x_0,x_1,...x_n)=\\displaystyle\\sum_{i=0}^{n}θ_ix_i$$。\n",
    "\n",
    "进一步用矩阵形式表达更加简洁如下：\n",
    "\n",
    "$$h_θ(X)=Xθ$$\n",
    "\n",
    "其中， 假设函数$$h_θ(X)$$为$$m*1$$的向量,\n",
    "$$θ$$ 为$$n * 1$$的向量，里面有n个代数法的模型参数。\n",
    "\n",
    "$$X$$为$$m*n$$维的矩阵，m代表样本的个数，n代表样本的特征数。\n",
    "\n",
    "得到了模型，我们需要求出需要的损失函数，一般线性回归我们用均方误差作为损失函数。损失函数的代数法表示如下：\n",
    "\n",
    "$$J(θ_0,θ_1...,θ_n)=\\displaystyle\\sum_{i=0}^{m}(h_θ(x_0,x_1,...x_n)−y_i)^2$$\n",
    "\n",
    "进一步用矩阵形式表达损失函数：\n",
    "\n",
    "$$J(θ)=\\frac{1}{2}(Xθ−Y)^T(Xθ−Y)$$\n",
    "\n",
    "由于矩阵法表达比较的简洁，后面我们将统一采用矩阵方式表达模型函数和损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 线性回归的算法\n",
    "\n",
    "对于线性回归的损失函数$$J(θ)=\\frac{1}{2}(Xθ−Y)^T(Xθ−Y)$$，\n",
    "\n",
    "我们常用的有两种方法来求损失函数最小化时候的θ参数：\n",
    "\n",
    "一种是[**梯度下降法**](https://tianchi.aliyun.com/notebook/detail.html?spm=5176.11409357.0.0.23cb46d3wWJL37&id=26208)，一种是[**最小二乘法**](https://tianchi.aliyun.com/notebook/detail.html?spm=5176.11409386.0.0.4ab41d07LHLYZv&id=24774)。\n",
    "\n",
    "由于已经在其它篇中单独介绍了梯度下降法和最小二乘法，可以点链接到对应的文章链接去阅读。\n",
    "\n",
    "如果采用梯度下降法，则θ的迭代公式是这样的：\n",
    "\n",
    "$$θ=θ−αX^T(Xθ−Y)$$\n",
    "\n",
    "通过若干次迭代后，我们可以得到最终的θ的结果\n",
    "\n",
    "如果采用最小二乘法，则θ的结果公式如下：\n",
    "\n",
    "$$θ=(X^TX)^{−1}XTY$$\n",
    "\n",
    "当然线性回归，还有其他的常用算法，比如牛顿法和拟牛顿法，这里不详细描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 线性回归的推广：多项式回归\n",
    "\n",
    "回到我们开始的线性模型，\n",
    "\n",
    "$$h_θ(x_1,x_2,...x_n)=θ_0+θ_1x_1+...+θ_nx_n$$, \n",
    "\n",
    "如果这里不仅仅是x的一次方，比如增加二次方，那么模型就变成了多项式回归。\n",
    "\n",
    "这里写一个只有两个特征的p次方多项式回归的模型：\n",
    "\n",
    "$$h_θ(x1,x2)=θ_0+θ_1x_1+θ_2x_2+θ_3x_1^2+θ_4x_2^2+θ_5x_1x_2$$\n",
    "\n",
    "我们令\n",
    "\n",
    "$$x_0=1,x_1=x_1,x_2=x_2,x_3=x_1^2,x_4=x_2^2,x_5=x_1x_2$$ ,\n",
    "\n",
    "这样我们就得到了下式：\n",
    "\n",
    "$$h_θ(x_1,x_2)=θ_0+θ_1x_1+θ_2x_2+θ_3x_3+θ_4x_4+θ_5x_5$$\n",
    "\n",
    "可以发现，我们又重新回到了线性回归，这是一个五元线性回归，可以用线性回归的方法来完成算法。\n",
    "\n",
    "对于每个二元样本特征$$(x_1,x_2)$$,我们得到一个五元样本特征$$(1,x_1,x_2,x_1^2,x_2^2,x_1x_2)$$，通过这个改进的五元样本特征，我们重新把不是线性回归的函数变回线性回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 线性回归的推广：广义线性回归\n",
    "\n",
    "在上一节的线性回归的推广中，我们对样本特征端做了推广，这里我们对于特征y做推广。比如我们的输出$$Y$$不满足和$$X$$的线性关系，但是$$lnY$$和$$X$$满足线性关系，模型函数如下：\n",
    "\n",
    "$$lnY=Xθ$$\n",
    "\n",
    "这样对与每个样本的输入$$y$$，我们用$$lny$$去对应， 从而仍然可以用线性回归的算法去处理这个问题。我们把$$lny$$一般化，假设这个函数是单调可微函数$$g(.)$$,则一般化的广义线性回归形式是：\n",
    "\n",
    "$$g(Y)=Xθ  \\text{或者}   Y=g^{−1}(Xθ)$$ \n",
    "\n",
    "这个函数$$g(.)$$我们通常称为联系函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 线性回归的正则化\n",
    "\n",
    "为了防止模型的过拟合，我们在建立线性模型的时候经常需要加入正则化项。一般有L1正则化和L2正则化。\n",
    "\n",
    "线性回归的L1正则化通常称为Lasso回归，它和一般线性回归的区别是在损失函数上增加了一个L1正则化的项，L1正则化的项有一个常数系数α来调节损失函数的均方差项和正则化项的权重，具体Lasso回归的损失函数表达式如下：\n",
    "\n",
    "$$J(θ)=\\frac{1}{2n}(Xθ−Y)^T(Xθ−Y)+α||θ||_1$$\n",
    "\n",
    "其中$$n$$为样本个数，$$α$$为常数系数，需要进行调优。$$||θ||_1$$为L1范数。\n",
    "\n",
    "Lasso回归可以使得一些特征的系数变小，甚至还是一些绝对值较小的系数直接变为0。增强模型的泛化能力。\n",
    "\n",
    "Lasso回归的求解办法一般有坐标轴下降法（coordinate descent）和最小角回归法（ Least Angle Regression），由于它们比较复杂，在我的这篇文章单独讲述： [**线程回归的正则化-Lasso回归小结**]()\n",
    "\n",
    "线性回归的L2正则化通常称为Ridge回归，它和一般线性回归的区别是在损失函数上增加了一个L2正则化的项，和Lasso回归的区别是Ridge回归的正则化项是L2范数，而Lasso回归的正则化项是L1范数。具体Ridge回归的损失函数表达式如下：\n",
    "\n",
    "$$J(θ)=\\frac{1}{2}(Xθ−Y)^T(Xθ−Y)+\\frac{1}{2}α||θ||_2^2$$\n",
    "\n",
    "其中$$α$$为常数系数，需要进行调优。$$||θ||_2$$为L2范数。\n",
    "\n",
    "Ridge回归在不抛弃任何一个特征的情况下，缩小了回归系数，使得模型相对而言比较的稳定，但和Lasso回归比，这会使得模型的特征留的特别多，模型解释性差。\n",
    "\n",
    "Ridge回归的求解比较简单，一般用最小二乘法。这里给出用最小二乘法的矩阵推导形式，和普通线性回归类似。\n",
    "\n",
    "令$$J(θ)$$的导数为0，得到下式：\n",
    "\n",
    "$$X^T(Xθ−Y)+αθ=0$$\n",
    "\n",
    "整理即可得到最后的$$θ$$的结果：\n",
    "\n",
    "$$θ=(X^TX+αE)^{−1}X^TY$$\n",
    "\n",
    "其中E为单位矩阵。\n",
    "\n",
    "除了上面这两种常见的线性回归正则化，还有一些其他的线性回归正则化算法，区别主要就在于正则化项的不同，和损失函数的优化方式不同，这里就不累述了。\n",
    "\n",
    "\n",
    "\n",
    "[文章转载自：博客园 刘建平Pinard 线性回归原理小结](https://www.cnblogs.com/pinard/p/6004041.html#!comments)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
