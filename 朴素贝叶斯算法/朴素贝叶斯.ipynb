{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章 基于概率论的分类方法：朴素贝叶斯\n",
    "\n",
    "\n",
    "## 朴素贝叶斯 概述  \n",
    "\n",
    "贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。\n",
    "\n",
    "## 贝叶斯理论 & 条件概率  \n",
    "\n",
    "### 贝叶斯理论  \n",
    "\n",
    "我们现在有一个数据集，它由两类数据组成，数据分布如下图所示：\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535426494203_07NLFJ8noB.jpg)\n",
    "\n",
    "我们现在用 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率，用 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率，那么对于一个新数据点 (x,y)，可以用下面的规则来判断它的类别： 如果 p1(x,y) > p2(x,y) ，那么类别为1 如果 p2(x,y) > p1(x,y) ，那么类别为2\n",
    "\n",
    "也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 条件概率\n",
    "\n",
    "如果你对 p(x,y|c1) 符号很熟悉，那么可以跳过本小节。\n",
    "\n",
    "有一个装了 7 块石头的罐子，其中 3 块是白色的，4 块是黑色的。如果从罐子中随机取出一块石头，那么是白色石头的可能性是多少？由于取石头有 7 种可能，其中 3 种为白色，所以取出白色石头的概率为 3/7 。那么取到黑色石头的概率又是多少呢？很显然，是 4/7 。我们使用 P(white) 来表示取到白色石头的概率，其概率值可以通过白色石头数目除以总的石头数目来得到。  \n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535426566254_Cko33se4oe.jpg)\n",
    "\n",
    "如果这 7 块石头如下图所示，放在两个桶中，那么上述概率应该如何计算？\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535426593617_zptRaJlrwn.jpg)\n",
    "\n",
    "计算 P(white) 或者 P(black) ，如果事先我们知道石头所在桶的信息是会改变结果的。这就是所谓的条件概率（conditional probablity）。假定计算的是从 B 桶取到白色石头的概率，这个概率可以记作 P(white|bucketB) ，我们称之为“在已知石头出自 B 桶的条件下，取出白色石头的概率”。很容易得到，P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 1/3 。\n",
    "\n",
    "条件概率的计算公式如下：\n",
    "\n",
    "P(white|bucketB) = P(white and bucketB) / P(bucketB)\n",
    "\n",
    "首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = 1/3 。\n",
    "\n",
    "另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法： \n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535426827670_PyVQZA7wcR.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用条件概率来分类\n",
    "\n",
    "\n",
    "上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y): 如果 p1(x, y) > p2(x, y), 那么属于类别 1; 如果 p2(x, y) > p1(X, y), 那么属于类别 2.\n",
    "\n",
    "这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到:\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535426960986_O8ENxSlbmZ.jpg)\n",
    "\n",
    "使用上面这些定义，可以定义贝叶斯分类准则为: 如果 P(c1|x, y) > P(c2|x, y), 那么属于类别 c1; 如果 P(c2|x, y) > P(c1|x, y), 那么属于类别 c2.\n",
    "\n",
    "在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。\n",
    "\n",
    "我们假设特征之间 相互独立 。所谓 独立(independence) 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，每个特征同等重要。\n",
    "\n",
    "Note: 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯 场景\n",
    "\n",
    "机器学习的一个重要应用就是文档的自动分类。\n",
    "\n",
    "在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。\n",
    "\n",
    "朴素贝叶斯是上面介绍的贝叶斯分类器的一个扩展，是用于文档分类的常用算法。下面我们会进行一些朴素贝叶斯分类的实践项目。\n",
    "\n",
    "## 朴素贝叶斯 原理\n",
    "\n",
    "### 朴素贝叶斯 工作原理\n",
    "\n",
    "```\n",
    "提取所有文档中的词条并进行去重\n",
    "获取文档的所有类别\n",
    "计算每个类别中的文档数目\n",
    "对每篇训练文档: \n",
    "    对每个类别: \n",
    "        如果词条出现在文档中-->增加该词条的计数值（for循环或者矩阵相加）\n",
    "        增加所有词条的计数值（此类别下词条总数）\n",
    "对每个类别: \n",
    "    对每个词条: \n",
    "        将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）\n",
    "返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)）\n",
    "```\n",
    "\n",
    "### 朴素贝叶斯 开发流程\n",
    "\n",
    "```\n",
    "收集数据: 可以使用任何方法。\n",
    "准备数据: 需要数值型或者布尔型数据。\n",
    "分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。\n",
    "训练算法: 计算不同的独立特征的条件概率。\n",
    "测试算法: 计算错误率。\n",
    "使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。\n",
    "```\n",
    "\n",
    "### 朴素贝叶斯 算法特点\n",
    "\n",
    "```\n",
    "优点: 在数据较少的情况下仍然有效，可以处理多类别问题。\n",
    "缺点: 对于输入数据的准备方式较为敏感。\n",
    "适用数据类型: 标称型数据。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯 项目案例\n",
    "\n",
    "### 项目案例1: 屏蔽社区留言板的侮辱性言论\n",
    "\n",
    "**项目概述**\n",
    "\n",
    "构建一个快速过滤器来屏蔽在线社区留言板上的侮辱性言论。如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。对此问题建立两个类别: 侮辱类和非侮辱类，使用 1 和 0 分别表示。\n",
    "\n",
    "**开发流程**\n",
    "```\n",
    "收集数据: 可以使用任何方法\n",
    "准备数据: 从文本中构建词向量\n",
    "分析数据: 检查词条确保解析的正确性\n",
    "训练算法: 从词向量计算概率\n",
    "测试算法: 根据现实情况修改分类器\n",
    "使用算法: 对社区留言板言论进行分类\n",
    "```\n",
    "\n",
    "> 收集数据: 可以使用任何方法\n",
    "\n",
    "本例是我们自己构造的词表:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_set():\n",
    "    \"\"\"\n",
    "    创建数据集,都是假的 fake data set \n",
    "    :return: 单词列表posting_list, 所属类别class_vec\n",
    "    \"\"\"\n",
    "    posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    class_vec = [0, 1, 0, 1, 0, 1]  # 1 is 侮辱性的文字, 0 is not\n",
    "    return posting_list, class_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 准备数据: 从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_list(data_set):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param data_set: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocab_set = set()  # create empty set\n",
    "    for item in data_set:\n",
    "        # | 求两个集合的并集\n",
    "        vocab_set = vocab_set | set(item)\n",
    "    return list(vocab_set)\n",
    "\n",
    "\n",
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocab_list: 所有单词集合列表\n",
    "    :param input_set: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    result = [0] * len(vocab_list)\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            # 这个后面应该注释掉，因为对你没什么用，这只是为了辅助调试的\n",
    "            # print('the word: {} is not in my vocabulary'.format(word))\n",
    "            pass\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分析数据: 检查词条确保解析的正确性\n",
    "\n",
    "检查函数执行情况，检查词表，不出现重复单词，需要的话，可以对其进行排序。\n",
    "\n",
    "```\n",
    ">>> listOPosts, listClasses = bayes.loadDataSet()\n",
    ">>> myVocabList = bayes.createVocabList(listOPosts)\n",
    ">>> myVocabList\n",
    "['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', \n",
    "'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', \n",
    "'stupid', 'so', 'take', 'mr', 'steak', 'my']\n",
    "```\n",
    "\n",
    "检查函数有效性。例如：myVocabList 中索引为 2 的元素是什么单词？应该是是 help 。该单词在第一篇文档中出现了，现在检查一下看看它是否出现在第四篇文档中。\n",
    "\n",
    "```\n",
    ">>> bayes.setOfWords2Vec(myVocabList, listOPosts[0])\n",
    "[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    ">>> bayes.setOfWords2Vec(myVocabList, listOPosts[3])\n",
    "[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "> 训练算法: 从词向量计算概率\n",
    "\n",
    "现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 w. 粗体的 w 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535427657086_N8czwhultf.jpg)\n",
    "\n",
    "我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。\n",
    "\n",
    "首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。接下来计算 p(w | ci) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2...wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。\n",
    "\n",
    "朴素贝叶斯分类器训练函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类原版\n",
    "    :param train_mat: 文件单词类型矩阵\n",
    "                    总的输入文本，大致是 [[0,1,0,1,1...], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                         列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    #文件数\n",
    "    train_doc_num = len(train_mat)\n",
    "    #单词数\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版\n",
    "    p0num = np.zeros(words_num)\n",
    "    p1num = np.zeros(words_num)\n",
    "\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 0\n",
    "    p1num_all = 0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            # 如果不是侮辱性文件，则计算非侮辱性文件中出现的侮辱性单词的个数\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表\n",
    "    # 即 在1类别下，每个单词出现次数的占比\n",
    "    p1vec = p1num / p1num_all\n",
    "    # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表\n",
    "    # 即 在0类别下，每个单词出现次数的占比\n",
    "    p0vec = p0num / p0num_all\n",
    "    return p0vec, p1vec, pos_abusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法: 根据现实情况修改分类器\n",
    "\n",
    "在利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1) * p(w1|1) * p(w2|1)。如果其中一个概率值为 0，那么最后的乘积也为 0。为降低这种影响，可以将所有词的出现数初始化为 1，并将分母初始化为 2 （取1 或 2 的目的主要是为了保证分子和分母不为0，大家可以根据业务需求进行更改）。\n",
    "\n",
    "另一个遇到的问题是下溢出，这是由于太多很小的数相乘造成的。当计算乘积 p(w0|ci) * p(w1|ci) * p(w2|ci)... p(wn|ci) 时，由于大部分因子都非常小，所以程序会下溢出或者得到不正确的答案。（用 Python 尝试相乘许多很小的数，最后四舍五入后会得到 0）。一种解决办法是对乘积取自然对数。在代数中有 ln(a * b) = ln(a) + ln(b), 于是通过求对数可以避免下溢出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。\n",
    "\n",
    "下图给出了函数 f(x) 与 ln(f(x)) 的曲线。可以看出，它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，但不影响最终结果。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535427731791_pGmFJ0xMSK.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "    # p0num = np.zeros(words_num)\n",
    "    # p1num = np.zeros(words_num)\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 改成取 log 函数\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    return p0vec, p1vec, pos_abusive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用算法: 对社区留言板言论进行分类\n",
    "\n",
    "朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param p_class1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def testing_naive_bayes():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    :return: no return \n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    list_post, list_classes = load_data_set()\n",
    "    # 2. 创建单词集合\n",
    "    vocab_list = create_vocab_list(list_post)\n",
    "\n",
    "    # 3. 计算单词是否出现并创建数据矩阵\n",
    "    train_mat = []\n",
    "    for post_in in list_post:\n",
    "        train_mat.append(\n",
    "            # 返回m*len(vocab_list)的矩阵， 记录的都是0，1信息\n",
    "            # 其实就是那个东西的句子向量（就是data_set里面每一行,也不算句子吧)\n",
    "            set_of_words2vec(vocab_list, post_in)\n",
    "        )\n",
    "    # 4. 训练数据\n",
    "    p0v, p1v, p_abusive = train_naive_bayes(np.array(train_mat), np.array(list_classes))\n",
    "    # 5. 测试数据\n",
    "    test_one = ['love', 'my', 'dalmation']\n",
    "    test_one_doc = np.array(set_of_words2vec(vocab_list, test_one))\n",
    "    print('the result is: {}'.format(classify_naive_bayes(test_one_doc, p0v, p1v, p_abusive)))\n",
    "    test_two = ['stupid', 'garbage']\n",
    "    test_two_doc = np.array(set_of_words2vec(vocab_list, test_two))\n",
    "    print('the result is: {}'.format(classify_naive_bayes(test_two_doc, p0v, p1v, p_abusive)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result is: 0\n",
      "the result is: 1\n"
     ]
    }
   ],
   "source": [
    "# 项目案例1: 屏蔽社区留言板的侮辱性言论完整代码\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Oct 19, 2010\n",
    "Update  on 2017-05-18\n",
    "@author: Peter Harrington/羊三/小瑶/BBruceyuan\n",
    "《机器学习实战》更新地址：https://github.com/apachecn/MachineLearning\n",
    "\"\"\"\n",
    "\n",
    "# 我个人非常不喜欢 from numpy import *\n",
    "# 因为这样会和一些系统函数冲突，比如log, sum之类的\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "贝叶斯公式\n",
    "p(xy)=p(x|y)p(y)=p(y|x)p(x)\n",
    "p(x|y)=p(y|x)p(x)/p(y)\n",
    "\"\"\"\n",
    "\n",
    "# ------项目案例1: 屏蔽社区留言板的侮辱性言论------\n",
    "\n",
    "\n",
    "def load_data_set():\n",
    "    \"\"\"\n",
    "    创建数据集,都是假的 fake data set \n",
    "    :return: 单词列表posting_list, 所属类别class_vec\n",
    "    \"\"\"\n",
    "    posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    class_vec = [0, 1, 0, 1, 0, 1]  # 1 is 侮辱性的文字, 0 is not\n",
    "    return posting_list, class_vec\n",
    "\n",
    "\n",
    "def create_vocab_list(data_set):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param data_set: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocab_set = set()  # create empty set\n",
    "    for item in data_set:\n",
    "        # | 求两个集合的并集\n",
    "        vocab_set = vocab_set | set(item)\n",
    "    return list(vocab_set)\n",
    "\n",
    "\n",
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocab_list: 所有单词集合列表\n",
    "    :param input_set: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    result = [0] * len(vocab_list)\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            # 这个后面应该注释掉，因为对你没什么用，这只是为了辅助调试的\n",
    "            # print('the word: {} is not in my vocabulary'.format(word))\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "\n",
    "def _train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类原版\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版\n",
    "    p0num = np.zeros(words_num)\n",
    "    p1num = np.zeros(words_num)\n",
    "\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 0\n",
    "    p1num_all = 0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面需要改成改成取 log 函数\n",
    "    p1vec = p1num / p1num_all\n",
    "    p0vec = p0num / p0num_all\n",
    "    return p0vec, p1vec, pos_abusive\n",
    "\n",
    "\n",
    "def train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "    # p0num = np.zeros(words_num)\n",
    "    # p1num = np.zeros(words_num)\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面改成取 log 函数\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    return p0vec, p1vec, pos_abusive\n",
    "\n",
    "\n",
    "def classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param p_class1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def bag_words2vec(vocab_list, input_set):\n",
    "    # 注意和原来的做对比\n",
    "    result = [0] * len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] += 1\n",
    "        else:\n",
    "            print('the word: {} is not in my vocabulary'.format(word))\n",
    "    return result\n",
    "\n",
    "\n",
    "def testing_naive_bayes():\n",
    "    \"\"\"\n",
    "    测试朴素贝叶斯算法\n",
    "    :return: no return \n",
    "    \"\"\"\n",
    "    # 1. 加载数据集\n",
    "    list_post, list_classes = load_data_set()\n",
    "    # 2. 创建单词集合\n",
    "    vocab_list = create_vocab_list(list_post)\n",
    "\n",
    "    # 3. 计算单词是否出现并创建数据矩阵\n",
    "    train_mat = []\n",
    "    for post_in in list_post:\n",
    "        train_mat.append(\n",
    "            # 返回m*len(vocab_list)的矩阵， 记录的都是0，1信息\n",
    "            # 其实就是那个东西的句子向量（就是data_set里面每一行,也不算句子吧)\n",
    "            set_of_words2vec(vocab_list, post_in)\n",
    "        )\n",
    "    # 4. 训练数据\n",
    "    p0v, p1v, p_abusive = train_naive_bayes(np.array(train_mat), np.array(list_classes))\n",
    "    # 5. 测试数据\n",
    "    test_one = ['love', 'my', 'dalmation']\n",
    "    test_one_doc = np.array(set_of_words2vec(vocab_list, test_one))\n",
    "    print('the result is: {}'.format(classify_naive_bayes(test_one_doc, p0v, p1v, p_abusive)))\n",
    "    test_two = ['stupid', 'garbage']\n",
    "    test_two_doc = np.array(set_of_words2vec(vocab_list, test_two))\n",
    "    print('the result is: {}'.format(classify_naive_bayes(test_two_doc, p0v, p1v, p_abusive)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    testing_naive_bayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目案例2: 使用朴素贝叶斯过滤垃圾邮件\n",
    "\n",
    "**项目概述**  \n",
    "完成朴素贝叶斯的一个最著名的应用: 电子邮件垃圾过滤。\n",
    "\n",
    "**开发流程**   \n",
    "使用朴素贝叶斯对电子邮件进行分类\n",
    "\n",
    "* 收集数据: 提供文本文件  \n",
    "* 准备数据: 将文本文件解析成词条向量  \n",
    "* 分析数据: 检查词条确保解析的正确性  \n",
    "* 训练算法: 使用我们之前建立的 train_naive_bayes() 函数  \n",
    "* 测试算法: 使用朴素贝叶斯进行交叉验证  \n",
    "* 使用算法: 构建一个完整的程序对一组文档进行分类，将错分的文档输出到屏幕上\n",
    "\n",
    "> 收集数据: 提供文本文件\n",
    "\n",
    "```\n",
    "Hi Peter,\n",
    "\n",
    "With Jose out of town, do you want to\n",
    "meet once in a while to keep things\n",
    "going and do some interesting stuff?\n",
    "\n",
    "Let me know\n",
    "Eugene\n",
    "```\n",
    "\n",
    "> 准备数据: 将文本文件解析成词条向量  \n",
    "\n",
    "使用正则表达式来切分文本\n",
    "\n",
    "```\n",
    ">>> mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon.'\n",
    ">>> import re\n",
    ">>> regEx = re.compile('\\\\W*')\n",
    ">>> listOfTokens = regEx.split(mySent)\n",
    ">>> listOfTokens\n",
    "['This', 'book', 'is', 'the', 'best', 'book', 'on', 'Python', 'or', 'M.L.', 'I', 'have', 'ever', 'laid', 'eyes', 'upon', '']\n",
    "```\n",
    "\n",
    "> 分析数据: 检查词条确保解析的正确性  \n",
    "> \n",
    "> 训练算法: 使用我们之前建立的 train_naive_bayes() 函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "    # p0num = np.zeros(words_num)\n",
    "    # p1num = np.zeros(words_num)\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面改成取 log 函数\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    return p0vec, p1vec, pos_abusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法: 使用朴素贝叶斯进行交叉验证\n",
    "\n",
    "文件解析及完整的垃圾邮件测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is 0.1\n"
     ]
    }
   ],
   "source": [
    "# --------项目案例2: 使用朴素贝叶斯过滤垃圾邮件--------------\n",
    "import numpy as np\n",
    "\n",
    "def create_vocab_list(data_set):\n",
    "    \"\"\"\n",
    "    获取所有单词的集合\n",
    "    :param data_set: 数据集\n",
    "    :return: 所有单词的集合(即不含重复元素的单词列表)\n",
    "    \"\"\"\n",
    "    vocab_set = set()  # create empty set\n",
    "    for item in data_set:\n",
    "        # | 求两个集合的并集\n",
    "        vocab_set = vocab_set | set(item)\n",
    "    return list(vocab_set)\n",
    "\n",
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    遍历查看该单词是否出现，出现该单词则将该单词置1\n",
    "    :param vocab_list: 所有单词集合列表\n",
    "    :param input_set: 输入数据集\n",
    "    :return: 匹配列表[0,1,0,1...]，其中 1与0 表示词汇表中的单词是否出现在输入的数据集中\n",
    "    \"\"\"\n",
    "    # 创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    result = [0] * len(vocab_list)\n",
    "    # 遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            result[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            # 这个后面应该注释掉，因为对你没什么用，这只是为了辅助调试的\n",
    "            # print('the word: {} is not in my vocabulary'.format(word))\n",
    "            pass\n",
    "    return result\n",
    "\n",
    "def train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "    # p0num = np.zeros(words_num)\n",
    "    # p1num = np.zeros(words_num)\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面改成取 log 函数\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    return p0vec, p1vec, pos_abusive\n",
    "\n",
    "def classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n",
    "    \"\"\"\n",
    "    使用算法：\n",
    "        # 将乘法转换为加法\n",
    "        乘法：P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n",
    "        加法：P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    :param vec2classify: 待测数据[0,1,1,1,1...]，即要分类的向量\n",
    "    :param p0vec: 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表\n",
    "    :param p1vec: 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表\n",
    "    :param p_class1: 类别1，侮辱性文件的出现概率\n",
    "    :return: 类别1 or 0\n",
    "    \"\"\"\n",
    "    # 计算公式  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n",
    "    # 使用 NumPy 数组来计算两个向量相乘的结果，这里的相乘是指对应元素相乘，即先将两个向量中的第一个元素相乘，然后将第2个元素相乘，以此类推。\n",
    "    # 我的理解是：这里的 vec2Classify * p1Vec 的意思就是将每个词与其对应的概率相关联起来\n",
    "    # 可以理解为 1.单词在词汇表中的条件下，文件是good 类别的概率 也可以理解为 2.在整个空间下，文件既在词汇表中又是good类别的概率\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "  \n",
    "def text_parse(big_str):\n",
    "    \"\"\"\n",
    "    这里就是做词划分\n",
    "    :param big_str: 某个被拼接后的字符串\n",
    "    :return: 全部是小写的word列表，去掉少于 2 个字符的字符串\n",
    "    \"\"\"\n",
    "    import re\n",
    "    # 其实这里比较推荐用　\\W+ 代替 \\W*，\n",
    "    # 因为 \\W*会match empty patten，在py3.5+之后就会出现什么问题，推荐自己修改尝试一下，可能就会re.split理解更深了\n",
    "    token_list = re.split(r'\\W+', big_str)\n",
    "    if len(token_list) == 0:\n",
    "        print(token_list)\n",
    "    return [tok.lower() for tok in token_list if len(tok) > 2]\n",
    "\n",
    "\n",
    "def spam_test():\n",
    "    \"\"\"\n",
    "    对贝叶斯垃圾邮件分类器进行自动化处理。\n",
    "    :return: nothing\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    for i in range(1, 26):\n",
    "        # 添加垃圾邮件信息\n",
    "        # 这里需要做一个说明，为什么我会使用try except 来做\n",
    "        # 因为我们其中有几个文件的编码格式是 windows 1252　（spam: 17.txt, ham: 6.txt...)\n",
    "        # 这里其实还可以 :\n",
    "        # import os\n",
    "        # 然后检查 os.system(' file {}.txt'.format(i))，看一下返回的是什么\n",
    "        # 如果正常能读返回的都是：　ASCII text\n",
    "        # 对于except需要处理的都是返回： Non-ISO extended-ASCII text, with very long lines\n",
    "        try:\n",
    "            words = text_parse(open('./dataset/ham/{}.txt'.format(i)).read())\n",
    "        except:\n",
    "            words = text_parse(open('./dataset/ham/{}.txt'.format(i), encoding='Windows 1252').read())\n",
    "        doc_list.append(words)\n",
    "        full_text.extend(words)\n",
    "        class_list.append(1)\n",
    "        try:\n",
    "            # 添加非垃圾邮件\n",
    "            words = text_parse(open('./dataset/spm/{}.txt'.format(i)).read())\n",
    "        except:\n",
    "            words = text_parse(open('./dataset/spm/{}.txt'.format(i), encoding='Windows 1252').read())\n",
    "        doc_list.append(words)\n",
    "        full_text.extend(words)\n",
    "        class_list.append(0)\n",
    "    # 创建词汇表\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    \n",
    "    import random\n",
    "    # 生成随机取10个数, 为了避免警告将每个数都转换为整型\n",
    "    test_set = [int(num) for num in random.sample(range(50), 10)]\n",
    "    # 并在原来的training_set中去掉这10个数\n",
    "    training_set = list(set(range(50)) - set(test_set))\n",
    "    \n",
    "    training_mat = []\n",
    "    training_class = []\n",
    "    for doc_index in training_set:\n",
    "        training_mat.append(set_of_words2vec(vocab_list, doc_list[doc_index]))\n",
    "        training_class.append(class_list[doc_index])\n",
    "    p0v, p1v, p_spam = train_naive_bayes(\n",
    "        np.array(training_mat),\n",
    "        np.array(training_class)\n",
    "    )\n",
    "\n",
    "    # 开始测试\n",
    "    error_count = 0\n",
    "    for doc_index in test_set:\n",
    "        word_vec = set_of_words2vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_naive_bayes(\n",
    "            np.array(word_vec),\n",
    "            p0v,\n",
    "            p1v,\n",
    "            p_spam\n",
    "        ) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print('the error rate is {}'.format(\n",
    "        error_count / len(test_set)\n",
    "    ))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     spam_test()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目案例3: 使用朴素贝叶斯分类器从个人广告中获取区域倾向\n",
    "\n",
    "**项目概述**  \n",
    "广告商往往想知道关于一个人的一些特定人口统计信息，以便能更好地定向推销广告。\n",
    "\n",
    "我们将分别从美国的两个城市中选取一些人，通过分析这些人发布的信息，来比较这两个城市的人们在广告用词上是否不同。如果结论确实不同，那么他们各自常用的词是哪些，从人们的用词当中，我们能否对不同城市的人所关心的内容有所了解。\n",
    "\n",
    "**开发流程**  \n",
    "\n",
    "* 收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口\n",
    "* 准备数据: 将文本文件解析成词条向量\n",
    "* 分析数据: 检查词条确保解析的正确性\n",
    "* 训练算法: 使用我们之前建立的 trainNB0() 函数\n",
    "* 测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果\n",
    "* 使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词\n",
    "* 收集数据: 从 RSS 源收集内容，这里需要对 RSS 源构建一个接口\n",
    "\n",
    "也就是导入 RSS 源，我们使用 python 下载文本，在http://code.google.com/p/feedparser/ 下浏览相关文档，安装 feedparse，首先解压下载的包，并将当前目录切换到解压文件所在的文件夹，然后在 python 提示符下输入：\n",
    "\n",
    "```\n",
    ">>> python setup.py install\n",
    "```\n",
    "\n",
    "> 准备数据: 将文本文件解析成词条向量\n",
    "\n",
    "文档词袋模型\n",
    "\n",
    "我们将每个词的出现与否作为一个特征，这可以被描述为**词集模型(set-of-words model)**。如果一个词在文档中出现不止一次，这可能意味着包含该词是否出现在文档中所不能表达的某种信息，这种方法被称为**词袋模型(bag-of-words model)**。在词袋中，每个单词可以出现多次，而在词集中，每个词只能出现一次。为适应词袋模型，需要对函数 setOfWords2Vec() 稍加修改，修改后的函数为 bagOfWords2Vec() 。\n",
    "\n",
    "如下给出了基于词袋模型的朴素贝叶斯代码。它与函数 setOfWords2Vec() 几乎完全相同，唯一不同的是每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocaList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocaList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个包含在所有文档中出现的不重复词的列表\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet=set([])    #创建一个空集\n",
    "    for document in dataSet:\n",
    "        vocabSet=vocabSet|set(document)   #创建两个集合的并集\n",
    "    return list(vocabSet)\n",
    "def setOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec=[0]*len(vocabList)  #创建一个其中所含元素都为0的向量\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "                returnVec[vocabList.index(word)]+=1\n",
    "    return returnVec\n",
    "\n",
    "#文件解析\n",
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens=re.split(r'\\W*',bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok)>2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分析数据: 检查词条确保解析的正确性\n",
    "\n",
    "> 训练算法: 使用我们之前建立的 train_naive_bayes() 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(train_mat, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类修正版，　注意和原来的对比，为什么这么做可以查看书\n",
    "    :param train_mat:  type is ndarray\n",
    "                    总的输入文本，大致是 [[0,1,0,1], [], []]\n",
    "    :param train_category: 文件对应的类别分类， [0, 1, 0],\n",
    "                            列表的长度应该等于上面那个输入文本的长度\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_doc_num = len(train_mat)\n",
    "    words_num = len(train_mat[0])\n",
    "    # 因为侮辱性的被标记为了1， 所以只要把他们相加就可以得到侮辱性的有多少\n",
    "    # 侮辱性文件的出现概率，即train_category中所有的1的个数，\n",
    "    # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率\n",
    "    pos_abusive = np.sum(train_category) / train_doc_num\n",
    "    # 单词出现的次数\n",
    "    # 原版，变成ones是修改版，这是为了防止数字过小溢出\n",
    "    # p0num = np.zeros(words_num)\n",
    "    # p1num = np.zeros(words_num)\n",
    "    p0num = np.ones(words_num)\n",
    "    p1num = np.ones(words_num)\n",
    "    # 整个数据集单词出现的次数（原来是0，后面改成2了）\n",
    "    p0num_all = 2.0\n",
    "    p1num_all = 2.0\n",
    "\n",
    "    for i in range(train_doc_num):\n",
    "        # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数\n",
    "        if train_category[i] == 1:\n",
    "            p1num += train_mat[i]\n",
    "            p1num_all += np.sum(train_mat[i])\n",
    "        else:\n",
    "            p0num += train_mat[i]\n",
    "            p0num_all += np.sum(train_mat[i])\n",
    "    # 后面改成取 log 函数\n",
    "    p1vec = np.log(p1num / p1num_all)\n",
    "    p0vec = np.log(p0num / p0num_all)\n",
    "    return p0vec, p1vec, pos_abusive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法: 观察错误率，确保分类器可用。可以修改切分程序，以降低错误率，提高分类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_most_freq(vocab_list, full_text):\n",
    "    # RSS源分类器及高频词去除函数\n",
    "    from operator import itemgetter\n",
    "    freq_dict = {}\n",
    "    for token in vocab_list:\n",
    "        freq_dict[token] = full_text.count(token)\n",
    "    sorted_freq = sorted(freq_dict.items(), key=itemgetter(1), reverse=True)\n",
    "    return sorted_freq[0:30]\n",
    "\n",
    "\n",
    "def local_words(feed1, feed0):\n",
    "    # import feedparser # 其实呢，这一行没用到，最好删了\n",
    "    # 下面操作和上面那个 spam_test函数基本一样，理解了一个，两个都ok\n",
    "    doc_list = []\n",
    "    class_list = []\n",
    "    full_text = []\n",
    "    # 找出两个中最小的一个\n",
    "    min_len = min(len(feed0), len(feed1))\n",
    "    for i in range(min_len):\n",
    "        # 类别　１\n",
    "        word_list = text_parse(feed1['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        # 类别　０\n",
    "        word_list = text_parse(feed0['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    # 去掉高频词\n",
    "    top30words = calc_most_freq(vocab_list, full_text)\n",
    "    for pair in top30words:\n",
    "        if pair[0] in vocab_list:\n",
    "            vocab_list.remove(pair[0])\n",
    "    # 获取训练数据和测试数据\n",
    "    \n",
    "    import random\n",
    "    # 生成随机取10个数, 为了避免警告将每个数都转换为整型\n",
    "    test_set = [int(num) for num in random.sample(range(2 * min_len), 20)]\n",
    "    # 并在原来的training_set中去掉这10个数\n",
    "    training_set = list(set(range(2 * min_len)) - set(test_set))\n",
    "    \n",
    "    # 把这些训练集和测试集变成向量的形式\n",
    "    training_mat = []\n",
    "    training_class = []\n",
    "    for doc_index in training_set:\n",
    "        training_mat.append(bag_words2vec(vocab_list, doc_list[doc_index]))\n",
    "        training_class.append(class_list[doc_index])\n",
    "    p0v, p1v, p_spam = train_naive_bayes(\n",
    "        np.array(training_mat),\n",
    "        np.array(training_class)\n",
    "    )\n",
    "    error_count = 0\n",
    "    for doc_index in test_set:\n",
    "        word_vec = bag_words2vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_naive_bayes(\n",
    "            np.array(word_vec),\n",
    "            p0v,\n",
    "            p1v,\n",
    "            p_spam\n",
    "        ) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print(\"the error rate is {}\".format(error_count / len(test_set)))\n",
    "    return vocab_list, p0v, p1v\n",
    "\n",
    "def classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n",
    "    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n",
    "    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用算法: 构建一个完整的程序，封装所有内容。给定两个 RSS 源，改程序会显示最常用的公共词\n",
    "\n",
    "函数 localWords() 使用了两个 RSS 源作为参数，RSS 源要在函数外导入，这样做的原因是 RSS 源会随时间而改变，重新加载 RSS 源就会得到新的数据\n",
    "\n",
    "```\n",
    ">>> reload(bayes)\n",
    "<module 'bayes' from 'bayes.pyc'>\n",
    ">>> import feedparser\n",
    ">>> ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    ">>> sy=feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    ">>> vocabList,pSF,pNY=bayes.localWords(ny,sf)\n",
    "the error rate is: 0.2\n",
    ">>> vocabList,pSF,pNY=bayes.localWords(ny,sf)\n",
    "the error rate is: 0.3\n",
    ">>> vocabList,pSF,pNY=bayes.localWords(ny,sf)\n",
    "the error rate is: 0.55\n",
    "```\n",
    "\n",
    "为了得到错误率的精确估计，应该多次进行上述实验，然后取平均值\n",
    "\n",
    "接下来，我们要分析一下数据，显示地域相关的用词\n",
    "\n",
    "可以先对向量pSF与pNY进行排序，然后按照顺序打印出来，将下面的代码添加到文件中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words():\n",
    "    import feedparser\n",
    "    ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "    sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n",
    "    vocab_list, p_sf, p_ny = local_words(ny, sf)\n",
    "    top_ny = []\n",
    "    top_sf = []\n",
    "    for i in range(len(p_sf)):\n",
    "        if p_sf[i] > -6.0:\n",
    "            top_sf.append((vocab_list[i], p_sf[i]))\n",
    "        if p_ny[i] > -6.0:\n",
    "            top_ny.append((vocab_list[i], p_ny[i]))\n",
    "    sorted_sf = sorted(top_sf, key=lambda pair: pair[1], reverse=True)\n",
    "    sorted_ny = sorted(top_ny, key=lambda pair: pair[1], reverse=True)\n",
    "    print('\\n----------- this is SF ---------------\\n')\n",
    "    for item in sorted_sf:\n",
    "        print(item[0])\n",
    "    print('\\n----------- this is NY ---------------\\n')\n",
    "    for item in sorted_ny:\n",
    "        print(item[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数 getTopWords() 使用两个 RSS 源作为输入，然后训练并测试朴素贝叶斯分类器，返回使用的概率值。然后创建两个列表用于元组的存储，与之前返回排名最高的 X 个单词不同，这里可以返回大于某个阈值的所有词，这些元组会按照它们的条件概率进行排序。\n",
    "\n",
    "保存 bayes.py 文件，在python提示符下输入：\n",
    "\n",
    "```\n",
    ">>> reload(bayes)\n",
    "<module 'bayes' from 'bayes.pyc'>\n",
    ">>> bayes.getTopWords(ny,sf)\n",
    "the error rate is: 0.55\n",
    "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
    "how\n",
    "last\n",
    "man\n",
    "...\n",
    "veteran\n",
    "still\n",
    "ends\n",
    "late\n",
    "off\n",
    "own\n",
    "know\n",
    "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
    "someone\n",
    "meet\n",
    "...\n",
    "apparel\n",
    "recalled\n",
    "starting\n",
    "strings\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当注释掉用于移除高频词的三行代码，然后比较注释前后的分类性能，去掉这几行代码之后，错误率为54%，，而保留这些代码得到的错误率为70%。这里观察到，这些留言中出现次数最多的前30个词涵盖了所有用词的30%，vocabList的大小约为3000个词，也就是说，词汇表中的一小部分单词却占据了所有文本用词的一大部分。产生这种现象的原因是因为语言中大部分都是冗余和结构辅助性内容。另一个常用的方法是不仅移除高频词，同时从某个预定高频词中移除结构上的辅助词，该词表称为停用词表。\n",
    "\n",
    "从最后输出的单词，可以看出程序输出了大量的停用词，可以移除固定的停用词看看结果如何，这样做的话，分类错误率也会降低。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
