{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成方法: ensemble method（元算法: meta algorithm） 概述\n",
    "\n",
    "概念：是对其他算法进行组合的一种形式。\n",
    "\n",
    "通俗来说： 当做重要决定时，大家可能都会考虑吸取多个专家而不只是一个人的意见。 机器学习处理问题时又何尝不是如此？ 这就是集成方法背后的思想。\n",
    "\n",
    "集成方法：\n",
    "\n",
    "* 投票选举(bagging: 自举汇聚法 bootstrap aggregating): 是基于数据随机重抽样分类器构造的方法\n",
    "* 再学习(boosting): 是基于所有分类器的加权求和的方法\n",
    "\n",
    "## 集成方法 场景\n",
    "\n",
    "目前 bagging 方法最流行的版本是: 随机森林(random forest)  \n",
    "\n",
    "**选男友**：美女选择择偶对象的时候，会问几个闺蜜的建议，最后选择一个综合得分最高的一个作为男朋友\n",
    "\n",
    "目前 boosting 方法最流行的版本是: AdaBoost\n",
    "\n",
    "**追女友**：3个帅哥追同一个美女，第1个帅哥失败->(传授经验：姓名、家庭情况) 第2个帅哥失败->(传授经验：兴趣爱好、性格特点) 第3个帅哥成功\n",
    "\n",
    "> bagging 和 boosting 区别是什么？\n",
    "\n",
    "bagging 是一种与 boosting 很类似的技术, 所使用的多个分类器的类型（数据量和特征量）都是一致的。\n",
    "\n",
    "bagging 是由不同的分类器（1.数据随机化 2.特征随机化）经过训练，综合得出的出现最多分类结果；boosting 是通过调整已有分类器错分的那些数据来获得新的分类器，得出目前最优的结果。\n",
    "\n",
    "bagging 中的分类器权重是相等的；而 boosting 中的分类器加权求和，所以权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。\n",
    "\n",
    "\n",
    "## 随机森林\n",
    "\n",
    "### 随机森林 概述\n",
    "\n",
    "随机森林指的是利用多棵树对样本进行训练并预测的一种分类器。\n",
    "\n",
    "决策树相当于一个大师，通过自己在数据集中学到的知识用于新数据的分类。但是俗话说得好，一个诸葛亮，玩不过三个臭皮匠。随机森林就是希望构建多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。\n",
    "\n",
    "### 随机森林 原理\n",
    "\n",
    "那随机森林具体如何构建呢？  \n",
    "有两个方面：\n",
    "\n",
    "1. 数据的随机性化\n",
    "2. 待选特征的随机化\n",
    "\n",
    "使得随机森林中的决策树都能够彼此不同，提升系统的多样性，从而提升分类性能。\n",
    "\n",
    "> 数据的随机化：使得随机森林中的决策树更普遍化一点，适合更多的场景。\n",
    "\n",
    "（有放回的准确率在：70% 以上， 无放回的准确率在：60% 以上）\n",
    "\n",
    "1. 采取有放回的抽样方式 构造子数据集，保证不同子集之间的数量级一样（不同子集／同一子集 之间的元素可以重复）\n",
    "\n",
    "2. 利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。\n",
    "\n",
    "3. 然后统计子决策树的投票结果，得到最终的分类 就是 随机森林的输出结果。\n",
    "\n",
    "4. 如下图，假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1537926819839_mtyQOlCjQx.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 待选特征的随机化\n",
    "\n",
    "1. 子树从所有的待选特征中随机选取一定的特征。\n",
    "2. 在选取的特征中选取最优的特征。\n",
    " \n",
    "下图中，蓝色的方块代表所有可以被选择的特征，也就是目前的待选特征；黄色的方块是分裂特征。\n",
    "\n",
    "左边是一棵决策树的特征选取过程，通过在待选特征中选取最优的分裂特征（别忘了前文提到的ID3算法，C4.5算法，CART算法等等），完成分裂。\n",
    "\n",
    "右边是一个随机森林中的子树的特征选取过程。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1537926883534_9Ay9BwF7V5.jpg)\n",
    "\n",
    "> 随机森林 开发流程\n",
    "\n",
    "* 收集数据：任何方法\n",
    "* 准备数据：转换样本集\n",
    "* 分析数据：任何方法\n",
    "* 训练算法：通过数据随机化和特征随机化，进行多实例的分类评估\n",
    "* 测试算法：计算错误率\n",
    "* 使用算法：输入样本数据，然后运行 随机森林 算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理\n",
    "\n",
    "> 随机森林 算法特点\n",
    "\n",
    "* 优点：几乎不需要输入准备、可实现隐式特征选择、训练速度非常快、其他模型很难超越、很难建立一个糟糕的随机森林模型、大量优秀、免费以及开源的实现。\n",
    "* 缺点：劣势在于模型大小、是个很难去解释的黑盒子。\n",
    "* 适用数据范围：数值型和标称型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目案例: 声纳信号分类\n",
    "\n",
    "**项目概述**\n",
    "\n",
    "这是 Gorman 和 Sejnowski 在研究使用神经网络的声纳信号分类中使用的数据集。任务是训练一个模型来区分声纳信号。\n",
    "\n",
    "**开发流程**\n",
    "\n",
    "* 收集数据：提供的文本文件\n",
    "* 准备数据：转换样本集\n",
    "* 分析数据：手工检查数据\n",
    "* 训练算法：在数据上，利用 random_forest() 函数进行优化评估，返回模型的综合分类结果\n",
    "* 测试算法：在采用自定义 n_folds 份随机重抽样 进行测试评估，得出综合的预测评分\n",
    "* 使用算法：若你感兴趣可以构建完整的应用程序，从案例进行封装，也可以参考我们的代码\n",
    "\n",
    "> 收集数据：提供的文本文件\n",
    "\n",
    "样本数据：sonar-all-data.txt\n",
    "\n",
    "```\n",
    "0.02,0.0371,0.0428,0.0207,0.0954,0.0986,0.1539,0.1601,0.3109,0.2111,0.1609,0.1582,0.2238,0.0645,0.066,0.2273,0.31,0.2999,0.5078,0.4797,0.5783,0.5071,0.4328,0.555,0.6711,0.6415,0.7104,0.808,0.6791,0.3857,0.1307,0.2604,0.5121,0.7547,0.8537,0.8507,0.6692,0.6097,0.4943,0.2744,0.051,0.2834,0.2825,0.4256,0.2641,0.1386,0.1051,0.1343,0.0383,0.0324,0.0232,0.0027,0.0065,0.0159,0.0072,0.0167,0.018,0.0084,0.009,0.0032,R\n",
    "0.0453,0.0523,0.0843,0.0689,0.1183,0.2583,0.2156,0.3481,0.3337,0.2872,0.4918,0.6552,0.6919,0.7797,0.7464,0.9444,1,0.8874,0.8024,0.7818,0.5212,0.4052,0.3957,0.3914,0.325,0.32,0.3271,0.2767,0.4423,0.2028,0.3788,0.2947,0.1984,0.2341,0.1306,0.4182,0.3835,0.1057,0.184,0.197,0.1674,0.0583,0.1401,0.1628,0.0621,0.0203,0.053,0.0742,0.0409,0.0061,0.0125,0.0084,0.0089,0.0048,0.0094,0.0191,0.014,0.0049,0.0052,0.0044,R\n",
    "0.0262,0.0582,0.1099,0.1083,0.0974,0.228,0.2431,0.3771,0.5598,0.6194,0.6333,0.706,0.5544,0.532,0.6479,0.6931,0.6759,0.7551,0.8929,0.8619,0.7974,0.6737,0.4293,0.3648,0.5331,0.2413,0.507,0.8533,0.6036,0.8514,0.8512,0.5045,0.1862,0.2709,0.4232,0.3043,0.6116,0.6756,0.5375,0.4719,0.4647,0.2587,0.2129,0.2222,0.2111,0.0176,0.1348,0.0744,0.013,0.0106,0.0033,0.0232,0.0166,0.0095,0.018,0.0244,0.0316,0.0164,0.0095,0.0078,R\n",
    "```\n",
    "\n",
    "> 准备数据：转换样本集\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入csv文件\n",
    "def loadDataSet(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as fr:\n",
    "        for line in fr.readlines():\n",
    "            if not line:\n",
    "                continue\n",
    "            lineArr = []\n",
    "            for featrue in line.split(','):\n",
    "                # strip()返回移除字符串头尾指定的字符生成的新字符串\n",
    "                str_f = featrue.strip()\n",
    "                if (str_f.isdigit()): # 判断是否是数字\n",
    "                    # 将数据集的第column列转换成float形式\n",
    "                    lineArr.append(float(str_f))\n",
    "                else:\n",
    "                    # 添加分类标签\n",
    "                    lineArr.append(str_f)\n",
    "            dataset.append(lineArr)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分析数据：手工检查数据\n",
    "\n",
    "> 训练算法：在数据上，利用 random_forest() 函数进行优化评估，返回模型的综合分类结果\n",
    "\n",
    "**样本数据随机无放回抽样-用于交叉验证** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, n_folds):\n",
    "    \"\"\"cross_validation_split(将数据集进行抽重抽样 n_folds 份，数据可以重复抽取)\n",
    "\n",
    "    Args:\n",
    "        dataset     原始数据集\n",
    "        n_folds     数据集dataset分成n_flods份\n",
    "    Returns:\n",
    "        dataset_split    list集合，存放的是：将数据集进行抽重抽样 n_folds 份，数据可以重复抽取\n",
    "    \"\"\"\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)       # 复制一份 dataset,防止 dataset 的内容改变\n",
    "    fold_size = len(dataset) / n_folds\n",
    "    for i in range(n_folds):\n",
    "        fold = list()                  # 每次循环 fold 清零，防止重复导入 dataset_split\n",
    "        while len(fold) < fold_size:   # 这里不能用 if，if 只是在第一次判断时起作用，while 执行循环，直到条件不成立\n",
    "            # 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此为自助采样法。从而保证每棵决策树训练集的差异性            \n",
    "            index = randrange(len(dataset_copy))\n",
    "            # 将对应索引 index 的内容从 dataset_copy 中导出，并将该内容从 dataset_copy 中删除。\n",
    "            # pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。\n",
    "            fold.append(dataset_copy.pop(index))  # 无放回的方式\n",
    "            # fold.append(dataset_copy[index])  # 有放回的方式\n",
    "        dataset_split.append(fold)\n",
    "    # 由dataset分割出的n_folds个数据构成的列表，为了用于交叉验证\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练数据集随机化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):   # 创建数据集的随机子样本\n",
    "    \"\"\"random_forest(评估算法性能，返回模型得分)\n",
    "\n",
    "    Args:\n",
    "        dataset         训练数据集\n",
    "        ratio           训练数据集的样本比例\n",
    "    Returns:\n",
    "        sample          随机抽样的训练样本\n",
    "    \"\"\"\n",
    "\n",
    "    sample = list()\n",
    "    # 训练样本的按比例抽样。\n",
    "    # round() 方法返回浮点数x的四舍五入值。\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        # 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此为自助采样法。从而保证每棵决策树训练集的差异性\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 特征随机化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))  # class_values =[0, 1]\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)  # 往 features 添加 n_features 个特征（ n_feature 等于特征数的个数），特征索引从 dataset 中随机取\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:                    # 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)  # groups=(left, right), row[index] 遍历每一行 index 索引下的特征值作为分类值 value, 找出最优的分类特征和特征值\n",
    "            gini = gini_index(groups, class_values)\n",
    "            # 左右两边的数量越一样，说明数据区分度不高，gini系数越大\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups  # 最后得到最优的分类特征 b_index,分类特征值 b_value,分类结果 b_groups。b_value 为分错的代价成本\n",
    "    # print b_score\n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    \"\"\"random_forest(评估算法性能，返回模型得分)\n",
    "\n",
    "    Args:\n",
    "        train           训练数据集\n",
    "        test            测试数据集\n",
    "        max_depth       决策树深度不能太深，不然容易导致过拟合\n",
    "        min_size        叶子节点的大小\n",
    "        sample_size     训练数据集的样本比例\n",
    "        n_trees         决策树的个数\n",
    "        n_features      选取的特征的个数\n",
    "    Returns:\n",
    "        predictions     每一行的预测结果，bagging 预测最后的分类结果\n",
    "    \"\"\"\n",
    "\n",
    "    trees = list()\n",
    "    # n_trees 表示决策树的数量\n",
    "    for i in range(n_trees):\n",
    "        # 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性\n",
    "        sample = subsample(train, sample_size)\n",
    "        # 创建一个决策树\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "\n",
    "    # 每一行的预测结果，bagging 预测最后的分类结果\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法：在采用自定义 n_folds 份随机重抽样 进行测试评估，得出综合的预测评分。\n",
    "\n",
    "* 计算随机森林的预测结果的正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估算法性能，返回模型得分\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    \"\"\"evaluate_algorithm(评估算法性能，返回模型得分)\n",
    "\n",
    "    Args:\n",
    "        dataset     原始数据集\n",
    "        algorithm   使用的算法\n",
    "        n_folds     数据的份数\n",
    "        *args       其他的参数\n",
    "    Returns:\n",
    "        scores      模型得分\n",
    "    \"\"\"\n",
    "\n",
    "    # 将数据集进行随机抽样，分成 n_folds 份，数据无重复的抽取\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    # 每次循环从 folds 从取出一个 fold 作为测试集，其余作为训练集，遍历整个 folds ，实现交叉验证\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        # 将多个 fold 列表组合成一个 train_set 列表, 类似 union all\n",
    "        \"\"\"\n",
    "        In [20]: l1=[[1, 2, 'a'], [11, 22, 'b']]\n",
    "        In [21]: l2=[[3, 4, 'c'], [33, 44, 'd']]\n",
    "        In [22]: l=[]\n",
    "        In [23]: l.append(l1)\n",
    "        In [24]: l.append(l2)\n",
    "        In [25]: l\n",
    "        Out[25]: [[[1, 2, 'a'], [11, 22, 'b']], [[3, 4, 'c'], [33, 44, 'd']]]\n",
    "        In [26]: sum(l, [])\n",
    "        Out[26]: [[1, 2, 'a'], [11, 22, 'b'], [3, 4, 'c'], [33, 44, 'd']]\n",
    "        \"\"\"\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        # fold 表示从原始数据集 dataset 提取出来的测试集\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            row_copy[-1] = None \n",
    "            test_set.append(row_copy)\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "\n",
    "        # 计算随机森林的预测结果的正确率\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random= 0.13436424411240122\n",
      "Trees: 1\n",
      "Scores: [71.42857142857143, 69.04761904761905, 78.57142857142857, 73.80952380952381, 69.04761904761905]\n",
      "Mean Accuracy: 72.381%\n",
      "random= 0.13436424411240122\n",
      "Trees: 10\n",
      "Scores: [78.57142857142857, 78.57142857142857, 78.57142857142857, 78.57142857142857, 73.80952380952381]\n",
      "Mean Accuracy: 77.619%\n",
      "random= 0.13436424411240122\n",
      "Trees: 20\n",
      "Scores: [78.57142857142857, 80.95238095238095, 78.57142857142857, 78.57142857142857, 73.80952380952381]\n",
      "Mean Accuracy: 78.095%\n"
     ]
    }
   ],
   "source": [
    "#项目1 完整代码\n",
    "#!/usr/bin/python\n",
    "# coding:utf8\n",
    "\n",
    "'''\n",
    "Created 2017-04-25\n",
    "Update  on 2017-05-18\n",
    "Random Forest Algorithm on Sonar Dataset\n",
    "@author: Flying_sfeng/片刻\n",
    "《机器学习实战》更新地址：https://github.com/apachecn/MachineLearning\n",
    "---\n",
    "源代码网址：http://www.tuicool.com/articles/iiUfeim\n",
    "Flying_sfeng博客地址：http://blog.csdn.net/flying_sfeng/article/details/64133822\n",
    "在此表示感谢你的代码和注解， 我重新也完善了个人注解\n",
    "'''\n",
    "from random import seed, randrange, random\n",
    "\n",
    "\n",
    "# 导入csv文件\n",
    "def loadDataSet(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as fr:\n",
    "        for line in fr.readlines():\n",
    "            if not line:\n",
    "                continue\n",
    "            lineArr = []\n",
    "            for featrue in line.split(','):\n",
    "                # strip()返回移除字符串头尾指定的字符生成的新字符串\n",
    "                str_f = featrue.strip()\n",
    "                # isdigit 如果是浮点型数值，就是 false，所以换成 isalpha() 函数\n",
    "                # if str_f.isdigit():   # 判断是否是数字\n",
    "                if str_f.isalpha():     # 如果是字母，说明是标签\n",
    "                    # 添加分类标签\n",
    "                    lineArr.append(str_f)\n",
    "                else:\n",
    "                    # 将数据集的第column列转换成float形式\n",
    "                    lineArr.append(float(str_f))\n",
    "            dataset.append(lineArr)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    \"\"\"cross_validation_split(将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取，每一次list的元素是无重复的)\n",
    "\n",
    "    Args:\n",
    "        dataset     原始数据集\n",
    "        n_folds     数据集dataset分成n_flods份\n",
    "    Returns:\n",
    "        dataset_split    list集合，存放的是：将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取，每一次list的元素是无重复的\n",
    "    \"\"\"\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(dataset)       # 复制一份 dataset,防止 dataset 的内容改变\n",
    "    fold_size = len(dataset) / n_folds\n",
    "    for i in range(n_folds):\n",
    "        fold = list()                  # 每次循环 fold 清零，防止重复导入 dataset_split\n",
    "        while len(fold) < fold_size:   # 这里不能用 if，if 只是在第一次判断时起作用，while 执行循环，直到条件不成立\n",
    "            # 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性            \n",
    "            index = randrange(len(dataset_copy))\n",
    "            # 将对应索引 index 的内容从 dataset_copy 中导出，并将该内容从 dataset_copy 中删除。\n",
    "            # pop() 函数用于移除列表中的一个元素（默认最后一个元素），并且返回该元素的值。\n",
    "            # fold.append(dataset_copy.pop(index))  # 无放回的方式\n",
    "            fold.append(dataset_copy[index])  # 有放回的方式\n",
    "        dataset_split.append(fold)\n",
    "    # 由dataset分割出的n_folds个数据构成的列表，为了用于交叉验证\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "# Split a dataset based on an attribute and an attribute value # 根据特征和特征值分割数据集\n",
    "def test_split(index, value, dataset):\n",
    "    left, right = list(), list()\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "\n",
    "# Calculate the Gini index for a split dataset\n",
    "def gini_index(groups, class_values):    # 个人理解：计算代价，分类越准确，则 gini 越小\n",
    "    gini = 0.0\n",
    "    for class_value in class_values:     # class_values = [0, 1] \n",
    "        for group in groups:             # groups = (left, right)\n",
    "            size = len(group)\n",
    "            if size == 0:\n",
    "                continue\n",
    "            proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
    "            gini += (proportion * (1.0 - proportion))    # 个人理解：计算代价，分类越准确，则 gini 越小\n",
    "    return gini\n",
    "\n",
    "\n",
    "# 找出分割数据集的最优特征，得到最优的特征 index，特征值 row[index]，以及分割完的数据 groups（left, right）\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))  # class_values =[0, 1]\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0])-1)  # 往 features 添加 n_features 个特征（ n_feature 等于特征数的根号），特征索引从 dataset 中随机取\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:                    # 在 n_features 个特征中选出最优的特征索引，并没有遍历所有特征，从而保证了每课决策树的差异性\n",
    "        for row in dataset:\n",
    "            groups = test_split(index, row[index], dataset)  # groups=(left, right), row[index] 遍历每一行 index 索引下的特征值作为分类值 value, 找出最优的分类特征和特征值\n",
    "            gini = gini_index(groups, class_values)\n",
    "            # 左右两边的数量越一样，说明数据区分度不高，gini系数越大\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups  # 最后得到最优的分类特征 b_index,分类特征值 b_value,分类结果 b_groups。b_value 为分错的代价成本\n",
    "    # print(b_score)\n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "\n",
    "# Create a terminal node value # 输出group中出现次数较多的标签\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]           # max() 函数中，当 key 参数不为空时，就以 key 的函数对象为判断的标准\n",
    "    return max(set(outcomes), key=outcomes.count)   # 输出 group 中出现次数较多的标签  \n",
    "\n",
    "\n",
    "# Create child splits for a node or make terminal  # 创建子分割器，递归分类，直到分类结束\n",
    "def split(node, max_depth, min_size, n_features, depth):  # max_depth = 10, min_size = 1, n_features = int(sqrt((dataset[0])-1))\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "# check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "# check for max depth\n",
    "    if depth >= max_depth:   # max_depth=10 表示递归十次，若分类还未结束，则选取数据中分类标签较多的作为结果，使分类提前结束，防止过拟合\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "# process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)  # node['left']是一个字典，形式为{'index':b_index, 'value':b_value, 'groups':b_groups}，所以node是一个多层字典\n",
    "        split(node['left'], max_depth, min_size, n_features, depth+1)  # 递归，depth+1计算递归层数\n",
    "# process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth+1)\n",
    "\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    \"\"\"build_tree(创建一个决策树)\n",
    "\n",
    "    Args:\n",
    "        train           训练数据集\n",
    "        max_depth       决策树深度不能太深，不然容易导致过拟合\n",
    "        min_size        叶子节点的大小\n",
    "        n_features      选取的特征的个数\n",
    "    Returns:\n",
    "        root            返回决策树\n",
    "    \"\"\"\n",
    "\n",
    "    # 返回最优列和相关的信息\n",
    "    root = get_split(train, n_features)\n",
    "\n",
    "    # 对左右2边的数据 进行递归的调用，由于最优特征使用过，所以在后面进行使用的时候，就没有意义了\n",
    "    # 例如： 性别-男女，对男使用这一特征就没任何意义了\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root\n",
    "\n",
    "\n",
    "# Make a prediction with a decision tree\n",
    "def predict(node, row):   # 预测模型分类结果\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):       # isinstance 是 Python 中的一个内建函数。是用来判断一个对象是否是一个已知的类型。\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "\n",
    "# Make a prediction with a list of bagged trees\n",
    "def bagging_predict(trees, row):\n",
    "    \"\"\"bagging_predict(bagging预测)\n",
    "\n",
    "    Args:\n",
    "        trees           决策树的集合\n",
    "        row             测试数据集的每一行数据\n",
    "    Returns:\n",
    "        返回随机森林中，决策树结果出现次数做大的\n",
    "    \"\"\"\n",
    "\n",
    "    # 使用多个决策树trees对测试集test的第row行进行预测，再使用简单投票法判断出该行所属分类\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)\n",
    "\n",
    "\n",
    "# Create a random subsample from the dataset with replacement\n",
    "def subsample(dataset, ratio):   # 创建数据集的随机子样本\n",
    "    \"\"\"random_forest(评估算法性能，返回模型得分)\n",
    "\n",
    "    Args:\n",
    "        dataset         训练数据集\n",
    "        ratio           训练数据集的样本比例\n",
    "    Returns:\n",
    "        sample          随机抽样的训练样本\n",
    "    \"\"\"\n",
    "\n",
    "    sample = list()\n",
    "    # 训练样本的按比例抽样。\n",
    "    # round() 方法返回浮点数x的四舍五入值。\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        # 有放回的随机采样，有一些样本被重复采样，从而在训练集中多次出现，有的则从未在训练集中出现，此则自助采样法。从而保证每棵决策树训练集的差异性\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Random Forest Algorithm\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    \"\"\"random_forest(评估算法性能，返回模型得分)\n",
    "\n",
    "    Args:\n",
    "        train           训练数据集\n",
    "        test            测试数据集\n",
    "        max_depth       决策树深度不能太深，不然容易导致过拟合\n",
    "        min_size        叶子节点的大小\n",
    "        sample_size     训练数据集的样本比例\n",
    "        n_trees         决策树的个数\n",
    "        n_features      选取的特征的个数\n",
    "    Returns:\n",
    "        predictions     每一行的预测结果，bagging 预测最后的分类结果\n",
    "    \"\"\"\n",
    "\n",
    "    trees = list()\n",
    "    # n_trees 表示决策树的数量\n",
    "    for i in range(n_trees):\n",
    "        # 随机抽样的训练样本， 随机采样保证了每棵决策树训练集的差异性\n",
    "        sample = subsample(train, sample_size)\n",
    "        # 创建一个决策树\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)\n",
    "        trees.append(tree)\n",
    "\n",
    "    # 每一行的预测结果，bagging 预测最后的分类结果\n",
    "    predictions = [bagging_predict(trees, row) for row in test]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):  # 导入实际值和预测值，计算精确度\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "\n",
    "# 评估算法性能，返回模型得分\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    \"\"\"evaluate_algorithm(评估算法性能，返回模型得分)\n",
    "\n",
    "    Args:\n",
    "        dataset     原始数据集\n",
    "        algorithm   使用的算法\n",
    "        n_folds     数据的份数\n",
    "        *args       其他的参数\n",
    "    Returns:\n",
    "        scores      模型得分\n",
    "    \"\"\"\n",
    "\n",
    "    # 将数据集进行抽重抽样 n_folds 份，数据可以重复重复抽取，每一次 list 的元素是无重复的\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    # 每次循环从 folds 从取出一个 fold 作为测试集，其余作为训练集，遍历整个 folds ，实现交叉验证\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        # 将多个 fold 列表组合成一个 train_set 列表, 类似 union all\n",
    "        \"\"\"\n",
    "        In [20]: l1=[[1, 2, 'a'], [11, 22, 'b']]\n",
    "        In [21]: l2=[[3, 4, 'c'], [33, 44, 'd']]\n",
    "        In [22]: l=[]\n",
    "        In [23]: l.append(l1)\n",
    "        In [24]: l.append(l2)\n",
    "        In [25]: l\n",
    "        Out[25]: [[[1, 2, 'a'], [11, 22, 'b']], [[3, 4, 'c'], [33, 44, 'd']]]\n",
    "        In [26]: sum(l, [])\n",
    "        Out[26]: [[1, 2, 'a'], [11, 22, 'b'], [3, 4, 'c'], [33, 44, 'd']]\n",
    "        \"\"\"\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        # fold 表示从原始数据集 dataset 提取出来的测试集\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            row_copy[-1] = None\n",
    "            test_set.append(row_copy)\n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "\n",
    "        # 计算随机森林的预测结果的正确率\n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy)\n",
    "    return scores\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # 加载数据\n",
    "    dataset = loadDataSet('./dataset/sonar-all-data.txt')\n",
    "    # print(dataset)\n",
    "\n",
    "    n_folds = 5        # 分成5份数据，进行交叉验证\n",
    "    max_depth = 20     # 调参（自己修改） #决策树深度不能太深，不然容易导致过拟合\n",
    "    min_size = 1       # 决策树的叶子节点最少的元素数量\n",
    "    sample_size = 1.0  # 做决策树时候的样本的比例\n",
    "    # n_features = int((len(dataset[0])-1))\n",
    "    n_features = 15     # 调参（自己修改） #准确性与多样性之间的权衡\n",
    "    for n_trees in [1, 10, 20]:  # 理论上树是越多越好\n",
    "        scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n",
    "        # 每一次执行本文件时都能产生同一个随机数\n",
    "        seed(1)\n",
    "        print('random=', random())\n",
    "        print('Trees: %d' % n_trees)\n",
    "        print('Scores: %s' % scores)\n",
    "        print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "### AdaBoost (adaptive boosting: 自适应 boosting) 概述\n",
    "\n",
    "能否使用弱分类器和多个实例来构建一个强分类器？ 这是一个非常有趣的理论问题。\n",
    "\n",
    "### AdaBoost 原理\n",
    "\n",
    "> AdaBoost 工作原理\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1537955607545_P9WpLY4wCk.jpg)\n",
    "\n",
    "> AdaBoost 开发流程\n",
    "\n",
    "* 收集数据：可以使用任意方法\n",
    "* 准备数据：依赖于所使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。  \n",
    "    当然也可以使用任意分类器作为弱分类器，第2章到第6章中的任一分类器都可以充当弱分类器。  \n",
    "    作为弱分类器，简单分类器的效果更好。\n",
    "* 分析数据：可以使用任意方法。\n",
    "* 训练算法：AdaBoost 的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器。\n",
    "* 测试算法：计算分类的错误率。\n",
    "* 使用算法：通SVM一样，AdaBoost 预测两个类别中的一个。如果想把它应用到多个类别的场景，那么就要像多类 SVM 中的做法一样对 AdaBoost 进行修改。\n",
    "\n",
    "> AdaBoost 算法特点\n",
    "\n",
    "* 优点：泛化（由具体的、个别的扩大为一般的）错误率低，易编码，可以应用在大部分分类器上，无参数调节。\n",
    "* 缺点：对离群点敏感。\n",
    "* 适用数据类型：数值型和标称型数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目案例: 马疝病的预测\n",
    "\n",
    "**项目流程图**\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1537955758645_xSl5VdESHD.jpg)\n",
    "\n",
    "**基于单层决策树构建弱分类器**\n",
    "\n",
    "单层决策树(decision stump, 也称决策树桩)是一种简单的决策树。\n",
    "\n",
    "**项目概述**\n",
    "\n",
    "预测患有疝气病的马的存活问题，这里的数据包括368个样本和28个特征，疝气病是描述马胃肠痛的术语，然而，这种病并不一定源自马的胃肠问题，其他问题也可能引发疝气病，该数据集中包含了医院检测马疝气病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。另外，除了部分指标主观和难以测量之外，该数据还存在一个问题，数据集中有30%的值是缺失的。\n",
    "\n",
    "**开发流程** \n",
    "\n",
    "* 收集数据：提供的文本文件\n",
    "* 准备数据：确保类别标签是+1和-1，而非1和0\n",
    "* 分析数据：统计分析\n",
    "* 训练算法：在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器\n",
    "* 测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较\n",
    "* 使用算法：观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去\n",
    "\n",
    "> 收集数据：提供的文本文件\n",
    "\n",
    "训练数据：horseColicTraining.txt\n",
    "测试数据：horseColicTest.txt\n",
    "\n",
    "```\n",
    "2.000000\t1.000000\t38.500000\t66.000000\t28.000000\t3.000000\t3.000000\t0.000000\t2.000000\t5.000000\t4.000000\t4.000000\t0.000000\t0.000000\t0.000000\t3.000000\t5.000000\t45.000000\t8.400000\t0.000000\t0.000000\t-1.000000\n",
    "1.000000\t1.000000\t39.200000\t88.000000\t20.000000\t0.000000\t0.000000\t4.000000\t1.000000\t3.000000\t4.000000\t2.000000\t0.000000\t0.000000\t0.000000\t4.000000\t2.000000\t50.000000\t85.000000\t2.000000\t2.000000\t-1.000000\n",
    "2.000000\t1.000000\t38.300000\t40.000000\t24.000000\t1.000000\t1.000000\t3.000000\t1.000000\t3.000000\t3.000000\t1.000000\t0.000000\t0.000000\t0.000000\t1.000000\t1.000000\t33.000000\t6.700000\t0.000000\t0.000000\t1.000000\n",
    "```\n",
    "\n",
    "> 准备数据：确保类别标签是+1和-1，而非1和0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fileName):\n",
    "    # 获取 feature 的数量, 便于获取\n",
    "    numFeat = len(open(fileName).readline().split('\\t'))\n",
    "    dataArr = []\n",
    "    labelArr = []\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        lineArr = []\n",
    "        curLine = line.strip().split('\\t')\n",
    "        for i in range(numFeat-1):\n",
    "            lineArr.append(float(curLine[i]))\n",
    "        dataArr.append(lineArr)\n",
    "        labelArr.append(float(curLine[-1]))\n",
    "    return dataArr, labelArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分析数据：统计分析\n",
    "\n",
    "过拟合(overfitting, 也称为过学习)\n",
    "\n",
    "发现测试错误率在达到一个最小值之后有开始上升，这种现象称为过拟合。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1537955963675_JH9KECF6Sr.jpg)\n",
    "\n",
    "通俗来说：就是把一些噪音数据也拟合进去的，如下图。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1537955978008_WSZktltK4p.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练算法：在数据上，利用 adaBoostTrainDS() 函数训练出一系列的分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaBoostTrainDS(dataArr, labelArr, numIt=40):\n",
    "    \"\"\"adaBoostTrainDS(adaBoost训练过程放大)\n",
    "    Args:\n",
    "        dataArr   特征标签集合\n",
    "        labelArr  分类标签集合\n",
    "        numIt     实例数\n",
    "    Returns:\n",
    "        weakClassArr  弱分类器的集合\n",
    "        aggClassEst   预测的分类结果值\n",
    "    \"\"\"\n",
    "    weakClassArr = []\n",
    "    m = shape(dataArr)[0]\n",
    "    # 初始化 D，设置每个样本的权重值，平均分为m份\n",
    "    D = mat(ones((m, 1))/m)\n",
    "    aggClassEst = mat(zeros((m, 1)))\n",
    "    for i in range(numIt):\n",
    "        # 得到决策树的模型\n",
    "        bestStump, error, classEst = buildStump(dataArr, labelArr, D)\n",
    "\n",
    "        # alpha目的主要是计算每一个分类器实例的权重(组合就是分类结果)\n",
    "        # 计算每个分类器的alpha权重值\n",
    "        alpha = float(0.5*log((1.0-error)/max(error, 1e-16)))\n",
    "        bestStump['alpha'] = alpha\n",
    "        # store Stump Params in Array\n",
    "        weakClassArr.append(bestStump)\n",
    "\n",
    "        print (\"alpha=%s, classEst=%s, bestStump=%s, error=%s \" % (alpha, classEst.T, bestStump, error))\n",
    "        # 分类正确：乘积为1，不会影响结果，-1主要是下面求e的-alpha次方\n",
    "        # 分类错误：乘积为 -1，结果会受影响，所以也乘以 -1\n",
    "        expon = multiply(-1*alpha*mat(labelArr).T, classEst)\n",
    "        print ('(-1取反)预测值expon=', expon.T)\n",
    "        # 计算e的expon次方，然后计算得到一个综合的概率的值\n",
    "        # 结果发现： 判断错误的样本，D中相对应的样本权重值会变大。\n",
    "        D = multiply(D, exp(expon))\n",
    "        D = D/D.sum()\n",
    "\n",
    "        # 预测的分类结果值，在上一轮结果的基础上，进行加和操作\n",
    "        print ('当前的分类结果：', alpha*classEst.T)\n",
    "        aggClassEst += alpha*classEst\n",
    "        print (\"叠加后的分类结果aggClassEst: \", aggClassEst.T)\n",
    "        # sign 判断正为1， 0为0， 负为-1，通过最终加和的权重值，判断符号。\n",
    "        # 结果为：错误的样本标签集合，因为是 !=,那么结果就是0 正, 1 负\n",
    "        aggErrors = multiply(sign(aggClassEst) != mat(labelArr).T, ones((m, 1)))\n",
    "        errorRate = aggErrors.sum()/m\n",
    "        # print \"total error=%s \" % (errorRate)\n",
    "        if errorRate == 0.0:\n",
    "            break\n",
    "    return weakClassArr, aggClassEst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现：\n",
    "alpha （模型权重）目的主要是计算每一个分类器实例的权重(加和就是分类结果)\n",
    "\n",
    "  分类的权重值：最大的值= alpha 的加和，最小值=-最大值\n",
    "D （样本权重）的目的是为了计算错误概率： weightedError = D.T*errArr，求最佳分类器\n",
    "\n",
    "样本的权重值：如果一个值误判的几率越小，那么 D 的样本权重越小\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1537956113153_NfumwYjHZQ.jpg)\n",
    "\n",
    "> 测试算法：我们拥有两个数据集。在不采用随机抽样的方法下，我们就会对 AdaBoost 和 Logistic 回归的结果进行完全对等的比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaClassify(datToClass, classifierArr):\n",
    "    \"\"\"adaClassify(ada分类测试)\n",
    "    Args:\n",
    "        datToClass     多个待分类的样例\n",
    "        classifierArr  弱分类器的集合\n",
    "    Returns:\n",
    "        sign(aggClassEst) 分类结果\n",
    "    \"\"\"\n",
    "    # do stuff similar to last aggClassEst in adaBoostTrainDS\n",
    "    dataMat = mat(datToClass)\n",
    "    m = shape(dataMat)[0]\n",
    "    aggClassEst = mat(zeros((m, 1)))\n",
    "\n",
    "    # 循环 多个分类器\n",
    "    for i in range(len(classifierArr)):\n",
    "        # 前提： 我们已经知道了最佳的分类器的实例\n",
    "        # 通过分类器来核算每一次的分类结果，然后通过alpha*每一次的结果 得到最后的权重加和的值。\n",
    "        classEst = stumpClassify(dataMat, classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq'])\n",
    "        aggClassEst += classifierArr[i]['alpha']*classEst\n",
    "    return sign(aggClassEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用算法：观察该例子上的错误率。不过，也可以构建一个 Web 网站，让驯马师输入马的症状然后预测马是否会死去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    # D = np.mat(np.ones((5, 1)) / 5)\n",
    "    # data_mat, class_labels = load_sim_data()\n",
    "    # print(data_mat.shape)\n",
    "    # result = build_stump(data_mat, class_labels, D)\n",
    "    # print(result)\n",
    "    # classifier_array, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 9)\n",
    "    # print(classifier_array, agg_class_est)\n",
    "    data_mat, class_labels = load_data_set('./dataset/horseColicTraining2.txt')\n",
    "    print(data_mat.shape, len(class_labels))\n",
    "    weak_class_arr, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 40)\n",
    "    print(weak_class_arr, '\\n-----\\n', agg_class_est.T)\n",
    "    plot_roc(agg_class_est, class_labels)\n",
    "    data_arr_test, label_arr_test = load_data_set(\"./dataset/horseColicTest2.txt\")\n",
    "    m = np.shape(data_arr_test)[0]\n",
    "    predicting10 = ada_classify(data_arr_test, weak_class_arr)\n",
    "    err_arr = np.mat(np.ones((m, 1)))\n",
    "    # 测试：计算总样本数，错误样本数，错误率\n",
    "    print(m,\n",
    "          err_arr[predicting10 != np.mat(label_arr_test).T].sum(),\n",
    "          err_arr[predicting10 != np.mat(label_arr_test).T].sum() / m\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 21) 299\n",
      "[{'dim': 9, 'thresh': 3.0, 'ineq': 'gt', 'alpha': 0.4616623792657674}, {'dim': 17, 'thresh': 52.5, 'ineq': 'gt', 'alpha': 0.31248245042467104}, {'dim': 3, 'thresh': 55.199999999999996, 'ineq': 'gt', 'alpha': 0.2868097320169577}, {'dim': 18, 'thresh': 62.300000000000004, 'ineq': 'lt', 'alpha': 0.23297004638939506}, {'dim': 10, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.19803846151213741}, {'dim': 5, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.18847887349020634}, {'dim': 12, 'thresh': 1.2, 'ineq': 'lt', 'alpha': 0.15227368997476778}, {'dim': 7, 'thresh': 1.2, 'ineq': 'gt', 'alpha': 0.15510870821690512}, {'dim': 5, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.13536197353359405}, {'dim': 4, 'thresh': 28.799999999999997, 'ineq': 'lt', 'alpha': 0.12521587326132078}, {'dim': 11, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.1334764812820767}, {'dim': 9, 'thresh': 4.0, 'ineq': 'lt', 'alpha': 0.1418224325377107}, {'dim': 14, 'thresh': 0.0, 'ineq': 'gt', 'alpha': 0.10264268449708028}, {'dim': 0, 'thresh': 1.0, 'ineq': 'lt', 'alpha': 0.11883732872109484}, {'dim': 4, 'thresh': 19.2, 'ineq': 'gt', 'alpha': 0.09879216527106625}, {'dim': 2, 'thresh': 36.72, 'ineq': 'lt', 'alpha': 0.12029960885056867}, {'dim': 3, 'thresh': 92.0, 'ineq': 'lt', 'alpha': 0.10846927663989175}, {'dim': 15, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.09652967982091411}, {'dim': 3, 'thresh': 73.6, 'ineq': 'gt', 'alpha': 0.08958515309272022}, {'dim': 18, 'thresh': 8.9, 'ineq': 'lt', 'alpha': 0.09210361961272426}, {'dim': 16, 'thresh': 4.0, 'ineq': 'gt', 'alpha': 0.10464142217079622}, {'dim': 11, 'thresh': 3.2, 'ineq': 'lt', 'alpha': 0.09575457291711606}, {'dim': 20, 'thresh': 0.0, 'ineq': 'gt', 'alpha': 0.09624217440331524}, {'dim': 17, 'thresh': 37.5, 'ineq': 'lt', 'alpha': 0.0785966288518967}, {'dim': 9, 'thresh': 2.0, 'ineq': 'lt', 'alpha': 0.0714286363455072}, {'dim': 5, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.07830753154662214}, {'dim': 4, 'thresh': 28.799999999999997, 'ineq': 'lt', 'alpha': 0.07606159074712784}, {'dim': 4, 'thresh': 19.2, 'ineq': 'gt', 'alpha': 0.08306752811081955}, {'dim': 7, 'thresh': 4.2, 'ineq': 'gt', 'alpha': 0.0830416741141175}, {'dim': 3, 'thresh': 92.0, 'ineq': 'lt', 'alpha': 0.08893356802801224}, {'dim': 14, 'thresh': 3.0, 'ineq': 'gt', 'alpha': 0.07000509315417908}, {'dim': 7, 'thresh': 5.3999999999999995, 'ineq': 'lt', 'alpha': 0.07697582358565964}, {'dim': 18, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.08507457442866707}, {'dim': 5, 'thresh': 3.2, 'ineq': 'lt', 'alpha': 0.0676590387302069}, {'dim': 7, 'thresh': 3.0, 'ineq': 'gt', 'alpha': 0.08045680822237049}, {'dim': 12, 'thresh': 1.2, 'ineq': 'lt', 'alpha': 0.05616862921969557}, {'dim': 11, 'thresh': 2.0, 'ineq': 'gt', 'alpha': 0.06454264376249863}, {'dim': 7, 'thresh': 5.3999999999999995, 'ineq': 'lt', 'alpha': 0.05308888435382875}, {'dim': 11, 'thresh': 0.0, 'ineq': 'lt', 'alpha': 0.0734605861478849}, {'dim': 13, 'thresh': 0.0, 'ineq': 'gt', 'alpha': 0.07872267320907414}] \n",
      "-----\n",
      " [[-3.46206894e-01  5.39820797e-01  1.34706655e+00 -1.96577914e-01\n",
      "  -2.92640472e-01  1.41661959e+00  1.02056225e+00  7.09312478e-01\n",
      "  -6.97844480e-01  8.52681888e-01 -1.51497974e-01  1.03289265e+00\n",
      "   1.43744689e+00  5.23008588e-01  1.00754226e+00 -5.04904838e-01\n",
      "  -1.55314011e+00  3.37496140e-01  1.83380220e-01 -2.73211412e-01\n",
      "   1.83753941e-01  1.94948307e+00  1.36536919e+00  1.51795899e+00\n",
      "   4.95458633e-01  1.66431604e-01  1.58899728e-01  7.56968915e-01\n",
      "   2.22941652e-01  2.33498196e+00 -9.09968780e-01 -2.25738386e-02\n",
      "   1.02102068e+00  6.63777579e-01 -2.08699355e-01 -1.78741084e+00\n",
      "  -3.78931896e-01  1.31751382e-01  1.67107366e+00  1.44392330e-01\n",
      "  -1.99093605e+00  5.16925045e-01  6.03341572e-01 -8.92743621e-01\n",
      "  -1.40693534e+00 -1.12567454e+00 -1.05681881e+00  1.72070073e+00\n",
      "  -8.66939191e-01 -1.60476417e+00  1.36634010e+00  3.79725055e-01\n",
      "   1.48452270e+00  6.64838547e-01 -3.26008621e-01  9.02376775e-02\n",
      "   3.93090801e-01 -1.14416817e-01 -6.16067208e-01 -1.27022815e+00\n",
      "   1.09311484e+00  1.72115916e+00 -4.29767329e-01  8.56337614e-02\n",
      "  -5.03473910e-01  4.64372963e-01  4.37330130e-01  1.93105924e+00\n",
      "   3.76440825e-02  1.53848847e+00 -1.70686489e-01  1.65834558e+00\n",
      "  -1.10078594e-01 -8.87042088e-01  3.93090801e-01 -7.93642352e-02\n",
      "   1.53618383e+00  2.00712868e+00 -1.29429189e-01  3.01572381e-01\n",
      "  -7.39738757e-01  1.05727042e+00  6.20389927e-01 -1.54961074e+00\n",
      "   5.38841873e-02  1.97319467e+00 -1.03933604e+00  5.95474367e-01\n",
      "   1.07315325e+00  2.02648722e+00  8.84069009e-01 -3.26008621e-01\n",
      "   1.39748643e+00  1.57824639e-03  1.20825738e+00  1.30012673e+00\n",
      "   7.59283818e-01 -1.32980287e+00  1.58143651e+00 -7.74682670e-01\n",
      "   1.61012477e+00 -1.58747355e+00  7.92723052e-01  1.62137341e+00\n",
      "  -1.44363067e-01  7.62472658e-01  5.55115414e-01  1.76395746e+00\n",
      "  -8.07172746e-01 -7.13443067e-01 -7.62149648e-01  1.06275540e+00\n",
      "   1.18148515e+00 -6.66282330e-01  2.36684357e-01  1.89994809e+00\n",
      "   2.24478748e+00 -9.17624883e-02  2.96253630e-01  1.08742569e+00\n",
      "   1.36883746e+00 -1.13525331e+00  1.80620011e+00 -5.59442382e-01\n",
      "   3.73123890e-01  1.84054205e+00  1.06799214e-01  5.89414867e-01\n",
      "  -6.54353819e-01  1.19724374e+00  4.63540870e-01  3.74879494e-01\n",
      "   1.47856059e+00 -6.57446519e-01 -6.93668829e-01 -1.29429189e-01\n",
      "   1.31594962e+00  1.64420444e+00  9.53502459e-01  7.40170420e-01\n",
      "  -4.35385025e-01 -1.30434214e+00  1.65834558e+00 -1.61422920e-01\n",
      "   7.50544036e-01  1.50390920e+00  8.12006783e-01 -1.11433828e-01\n",
      "   1.58948356e+00  1.80166124e+00 -1.24848181e+00 -3.16358922e-01\n",
      "   1.39748643e+00  2.36697540e+00  5.45445830e-01  1.30735427e+00\n",
      "   6.47538358e-01  1.16305757e+00  1.74396199e+00 -7.62381985e-01\n",
      "   4.27163890e-01  1.51465087e+00 -1.12436666e-01  8.09957092e-01\n",
      "   7.23764279e-03  1.94052195e+00  1.48306468e+00  2.64418198e-01\n",
      "  -1.21867130e+00  1.20420927e+00 -1.30937256e+00 -4.84404331e-01\n",
      "  -1.34263866e+00 -1.01739294e+00  2.22941652e-01  2.00510724e+00\n",
      "  -2.29750158e+00 -3.83556393e-01 -7.55614485e-01 -1.34943874e+00\n",
      "   8.51461579e-01 -1.54961074e+00  1.86860473e+00  2.09992462e+00\n",
      "  -4.31918141e-01  9.12376049e-01  1.32753580e-01  1.05799984e+00\n",
      "   3.94948721e-01  8.84028367e-01  5.37930835e-01  1.84470134e+00\n",
      "  -1.78544316e+00  6.35626744e-01  2.15191386e+00 -9.72238538e-02\n",
      "   1.90924894e-01  1.59095456e+00  8.40972426e-01  1.49370558e+00\n",
      "  -5.84176145e-01  4.54713517e-01  2.13758537e+00 -3.16164749e-01\n",
      "  -7.55614485e-01  5.31183150e-01  5.50284059e-01 -4.07776113e-01\n",
      "   2.05323133e+00 -1.22382729e-01 -9.51971888e-01  7.76212308e-01\n",
      "   1.02082939e+00  1.72495570e+00  2.05710397e+00  4.60616309e-01\n",
      "  -4.02664156e-01  8.58959865e-01  3.77072878e-02 -6.71174191e-01\n",
      "   7.47080450e-01 -3.81125550e-01  1.12389210e+00  1.75800100e+00\n",
      "   1.73857489e+00 -1.33101576e+00  5.50284059e-01 -1.30289376e+00\n",
      "   1.31228529e+00 -5.60455028e-01 -1.46354481e+00  7.90883276e-01\n",
      "   1.58899728e-01  1.20825738e+00 -8.87042088e-01  1.65728396e+00\n",
      "   1.22103595e+00  2.14912978e-01  3.64699577e-02 -1.32127479e+00\n",
      "   1.33000192e+00 -4.70065248e-02 -4.22251353e-02  7.70710258e-01\n",
      "  -1.27022815e+00 -8.61045169e-01 -9.56969900e-01 -7.95783983e-02\n",
      "   2.08738328e+00 -4.85855645e-01  3.91162341e-01  2.72751462e-01\n",
      "  -6.20974162e-01 -5.83646639e-01  1.44817145e-01  1.36706959e+00\n",
      "  -1.21222173e+00  1.92002542e+00  1.70722977e-01  9.55937244e-01\n",
      "   1.70564235e+00 -2.45144481e-01 -5.56039012e-01  1.94646186e-01\n",
      "   1.12867995e+00  5.63912150e-01  4.95748848e-01  1.20050741e+00\n",
      "   5.50857009e-01  6.29115697e-01 -1.11443306e-01 -2.08699355e-01\n",
      "  -1.63615949e+00 -4.00661971e-01  2.54103847e-01  8.38248031e-01\n",
      "   1.32784858e+00  1.87814608e+00  2.34862459e-01  1.67991427e+00\n",
      "  -6.70647873e-01 -7.62149648e-01  6.00705546e-02  2.24515155e+00\n",
      "   1.05333955e+00 -1.17411061e+00  1.57410447e-01  2.22941652e-01\n",
      "   1.51614920e+00  1.76240799e+00 -8.66939191e-01 -9.92873427e-01\n",
      "  -4.92052272e-01  2.52829341e+00 -1.55549790e-01 -1.18640330e+00\n",
      "  -1.35781920e+00 -1.94654268e-01  4.54752882e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYFOXV9/HvYRMFxI1EZXXBsLggjqBEBSNuqKCiAkYUUYkao3GLGM0jLjGvJhofExdcMTxx1ygYFOOCiMoqiIJBARVwRcUFlf28f9w1TtN09/QMU109Pb/PdfU1VV3V1aeqa/p03XfVKXN3REREsqmXdAAiIlLclChERCQnJQoREclJiUJERHJSohARkZyUKEREJCclihJmwb1mtszMpiYUwygzuyaJ964pZuZmtnPScdQEM5tgZqdHw780s2drYJlDzGzSxke30XHsb2bzko6jFJVcojCz983sBzNbbmafRF9UTdPm6WFmL5jZt2b2tZmNNbNOafNsbmY3mdmiaFnzo/FtCrtGG2U/4GCglbt3q6mFmlmTaJuMq6llRsudYGYromV/bWYTzWy3mnyPDO9Z6xNZdbn7P939kEK+Z01u7/QE7u4vu/vPamLZcTKzXma2JOk4qqLkEkXkKHdvCnQB9gQuLZ9gZvsCzwJPAtsDOwBvAK+Y2Y7RPI2A54HOwGHA5kAP4Augxr5w05lZgxpeZFvgfXf/roZjOQ5YCRxiZttVN7gszok+u62BCcDoGl5+YmL4fEUKw91L6gG8D/ROGb8e+HfK+MvArRle9zTwj2j4dOBToGkV3rcz8B/gy+i1v4+eHwVckzJfL2BJWryXALMJX76XA4+mLft/gZuj4ebA3cDHwIfANUD9DPGcBqwA1gLLgSuj588A5kdxjgG2T3mNA78G3gXey7GuLwB/BF4HLkqbtmf0/LfAQ8CD5esPbAk8BSwFlkXDrVJeOwE4PWW8E7AqZXwT4Cbgo+hxE7BJyvSM6wYY8FfgM+DraFvvCgwDVgOrom00Nsv6OnBmtF2WAbcAFk2rF31mH0TL/wfQPJrWLnrtacAiYCLQGPg/wo+Or4BpwE+r8tlG89YHfg8siLb1DKB1NK1HtNyvo789Mm1jYAgwqbJ9OMN7bx1t32+AqcDVacvpkLKcecAJ0fMZtzfhB9tj0X7xHnBuZesZbUsHvouWNYAN/7c6Ruv7FTAH6JsybVT0Of47Wu4UYKcs65vxMwOOB2akzXsh8EQ03AeYGy3/Q+AioAnwA7Auint5tP71gOHRen4BPAxslbYfnQosJuyDZwJ7E/blr4C/x/q9GufCk3iQkiiAVsCbwP9G45sRvjgPzPC6U4GPo+EHgfuq8J7NCP/cF0Y7VTOge8oOWVmimBXt/JsSjgK+BzZP+Uf5GNgnGn8CGBntcD8h/KP+KktcQ1j/H/gXwOdAV8KX7t+AiSnTnfAPvhWwaZZltol28k7R+s5OmdaI8IV5PtCQcOSxmopEsTXQP/ocmgGPlP9TRdMnUPEl1oiQjFLjuwqYHK13C+BV4OrK1g04lPAFswUhaXQEtsv0+WRZZycktS2i9V8KHBZNG0pITjsCTYHHgdHRtHbRa/8RfV6bAr8CxkbboD6wV8pnXZXP9mLCvv2zaJ32iLbvVoQvksFAA2BQNL51hm384/5Bjn04w3s/SPgia0JIuB+mLKcJ4cvs1Oj9u0afS+cs/w/1os/mf6LPfEdgIXBorvVM+Vx2zvS/Rdj/5hOSTKNo//gW+FlKHF8SWggaAP8EHsyyvhk/M8J+9iXQMWXemUD/aPhjYP9oeEuga6bvgOi53xL27VbRckcCD6TtR7dHn80hhB+BT0T7SUvCj5SesX2vxrXgpB6EL97l0U7hhCakLaJpraLnOmR43WHA6mj4P8D/q8J7DgJmZpmW/o+x3k4SxTs07TWTgJOj4YOBBdHwTwlHHZumvfeLWd57COsniruB61PGmxK+yNtF4w78opJ1vRyYFQ1vT0i8e0bjBxB+6VvK/K+S5YuY0DS4LGV8AiFJfkX41fk1cFDK9AVAn5TxQwlNaznXjfAl8Q6wD1Av1+eTJU4H9ksZfxgYHg0/D5ydMu1n0fs2oOIffMeU6UOjbbJ72ntU9bOdB/TL8PxgYGrac68BQ1K2caZEkXUfTltW/Wj9OqQ8d23KcgYAL6e9ZiRwRZb/h+7AorT5LwXuzbWeKZ9LtkSxP/BJ6ucNPACMSInjrpRpfYD/ZnmfjJ9ZNO024I/RcGdCUt4kGl9ESDKbp73mxzhTnnub9ff17TLsRy1Tpn8BDEgZfwz4bWWfX3UfpdpHcbS7NyN8IB2A8g7oZYRfw5na1bcj/PKB8CFUpe29NeFLrLoWp43fT/jHBTgxGodwtNEQ+NjMvjKzrwj/hD/J8322J/ziB8DdlxPWtWWOWNKdTPj1hbt/BLwEnJKy/A892nMjP76fmW1mZiPN7AMz+4bQfLCFmdVPmf9cd9+C8MvpSOBRM9s9U/zR8PaVrZu7vwD8ndDU8KmZ3WFmm1eynuk+SRn+npCIssXUgPDFXy51m44GxgMPmtlHZna9mTWk6p9ttn0uPZ7ymFpmmDef5aVrQVi/1HVKfb+2QPfydYjW45fAtlmW1xbYPm3+31Ox/ar7v7U9sNjd16XFmbodsn2m6bJ9ZgD3ASeamRGS9MPuvjKa1p+QgD4ws5ei/tFs2gL/StkGbxN+hKXuR5+mDP+QYTxb/ButVBMFAO7+EuGXw1+i8e8Iv66OzzD7CYRfhwDPAYeaWZM832oxsFOWad8RDlnLZfqH8bTxR4BeZtYKOIaKRLGY8KtzG3ffInps7u6d84zzI8IOCYSzlwjNFR/miOVHZtYDaA9cGp1R9gnhF+GgqKP2Y6Bl9E9Trk3K8IWEX9zd3X1zwhEIhCaF9bj7Ond/mdB8UH5mznrxR8v+KJ91c/eb3X0vwq++XQhNGjnXN0+ZYlrD+v/EP76Hu6929yvdvROhL+FIQvKt6mebbZ9Lj6c8pg8zzJvP8tItJaxf67Tlpy7npZR12MLdm7r7WdH09O29mNAfljp/M3fvU8W40n0EtDaz1O+4fLbDBnJ8Zrj7ZMLR7/6EH3WjU143zd37EZL9E4QjUci8zy0GDk/bDo3dvcrxxqGkE0XkJuBgM+sSjQ8HTjGzc82smZltGZ2uty9wZTTPaMIH95iZdTCzema2tZn93sz6bPgWPAVsa2a/NbNNouV2j6bNAvqY2VZmti2hLTInd19KaCK4l/BP9Hb0/MeEM7ZuiE7frWdmO5lZzzy3xf3AqWbWxcw2ITQZTHH39/N8/SmEZrlOhGajLoQ26s2AwwlJeA1wrpk1MLNjWf8ssWaEXz5fmdlWwBW53iz6BdaJ0BEJoengcjNrEZ2m/D+ETsac62Zme5tZ9+hX4HdUdPJD+ELfMc/1z+QB4Hwz2yE6Dfta4CF3X5NlnQ40s92io6hvCM0La6vx2d4FXG1m7S3Y3cy2BsYBu5jZidFnMICwDZ+qZD1y7cM/cve1hH6YEdERYicqjijLl7OLmQ02s4bRY28z6xhNT9/eU4FvzOwSM9vUzOqb2a5mtncl65lpWammED7r30Ux9AKOIvSvVEm2zyxlln8QjljXuPuk6DWNLFyn0tzdV0evS93ntjaz5inLuB34o5m1jV7fwsz6VTXW2MTVppXUg7SznryiHfGxlPH9CF/Eywkf4L+BXdNe05yQZBZH8y0AbiTqSMvwvrsSjkiWEQ5py9uwGxPO/vmGcIbC+WzYR9E7w/IGE355XJwhrtuAJYQ2/JnAwCwxDSGljyJ67sxoXb5kw7OO1mvzTXtd42jdjsow7VaiM7WAsiim8rOeHqKiM3v7lO3+DqH91oEG0fQJhC/x8rNB5gPnp8VwM+HI5eNouHFl6wYcFG375YTmxX8SndFGOEKaRegXeSLLuqe3hY9KWad6hIS1mPBr+/+ALaNp7VLXL3puEKHd/TvCF8bNKetflc+2PqG/6L1oW09LWd/9CB3EX0d/U/tXJpD9rKeM+3CG924Rbd9sZz39jPA/tZTQ/PcC0CXb9o72iwei91xG6NTtncd6nhntB18RWgR6sf7/VmdC0+jXhLOPjsn0GUbj6702bX2zfmbR9PITPK5Mea4R8Ey0Pt9Ecad+DvdQcRZV+VlPF0Tv8y1hP742x360BOiVMv5/wOVxfKe6+4+n+ImISDWY2aaEs466uvu7SccTh7rQ9CQiEqezgGmlmiQgxkRhZveY2Wdm9laW6WZmN1sojTHbzLrGFYuISBzM7H3gPMKJGiUrziOKUYRrE7I5nNBe2Z5wxeZtMcYiIlLj3L2du7d195lJxxKn2BKFu08kdCpm049QMsM9nGK2hdV83SAREdlISRYpa8n6F+0siZ77OH1GMxtGOOqgSZMme3Xo0KEgAYqI1FaLF8P338PKlbBmDbjP+NzdW1RnWUkmig0usiLLxU/ufgdwB0BZWZlPnz49zrhERGq18pNZzeC22+Czz2DECEu/Yj9vSZ71tIT1r+5sRcVVtiIiUg0ffgj9+sH9UT2Hs86CK3Je2lq5JBPFGODk6OynfYCvPVydKiIiVeQOd94JnTrBc8/B8uU1t+zYmp7M7AHC1Y7bWLib0xWEome4++2EUgN9CFfffk8oSywiIlW0YAGccQa8+CIceGBIGDtVp0JWFrElCncfVMl0J9wkR0RENsKbb8KMGXDHHXD66aFvoibp1owiIrXQW2/B66/DySfD0UfDwoWw9daVv646VMJDRKQWWbUKRoyArl3hsstgxYrwfFxJApQoRERqjSlTQoK48koYMABmzoTGjeN/XzU9iYjUAh9+CPvvDz/9KTz1FBxxROHeW0cUIiJF7J13wt+WLeGhh2DOnMImCVCiEBEpSl99BcOGQYcOMHFieO6YY2Dzqt7tvQao6UlEpMiMGROuqP7kE7j4Yth778pfEyclChGRInL66XD33bDbbvDkk1BWlnREShQiIolLLeJXVgZt28Ill0CjRsnGVU6JQkQkQYsXw5lnwsCBMHhwGC426swWEUnAunWhBHjnzjBhQrhvRLHSEYWISIG9+27oi5g4EXr3DjWadtgh6aiyU6IQESmwuXNh9my45x4YMqTmi/jVNCUKEZECeOMNmDULTjkl3Fho4ULYcsuko8qP+ihERGK0ciX84Q/hbKY//KGiiF9tSRKgRCEiEpvXXoM994RrroETTyxcEb+apqYnEZEYfPgh9OwJ224L48bB4YcnHVH16YhCRKQGvf12+NuyJTz8cCjiV5uTBChRiIjUiGXLYOhQ6NQJXn45PHf00dCsWbJx1QQ1PYmIbKR//QvOPhuWLoVLL02+iF9NU6IQEdkIQ4fCvfdCly7w73+HO9CVGiUKEZEqSi3it88+0L49XHQRNGyYbFxxUaIQEamCDz6AX/0qnO568snh5kKlTp3ZIiJ5WLcObrkFdt0VJk2C1auTjqhwdEQhIlKJefNCEb9Jk+CQQ2DkSGjXLumoCkeJQkSkEvPmheshRo0KzU3FXsSvpilRiIhkMHNmKOJ36qnQt28o4rfFFklHlQz1UYiIpFixAn7/+3AtxIgRFUX86mqSACUKEZEfvfJKuB7iT38KTUyzZtXOIn41TU1PIiKEIn4HHhhqNI0fHzqtJdARhYjUaXPnhr8tW8Jjj8GbbypJpFOiEJE66csvw21IO3cO964GOOooaNo00bCKkpqeRKTOeewx+PWv4Ysv4LLLoFu3pCMqbkoUIlKnDBkC990Xivc980zovJbclChEpOSlFvHr0QM6doQLL4QG+gbMS6x9FGZ2mJnNM7P5ZjY8w/Q2Zvaimc00s9lm1ifOeESk7nnvvdA5/Y9/hPFhw+CSS5QkqiK2RGFm9YFbgMOBTsAgM+uUNtvlwMPuvicwELg1rnhEpG5ZuxZuvjkU8Zs8ueKoQqouziOKbsB8d1/o7quAB4F+afM4sHk03Bz4KMZ4RKSOePtt2H9/OO886Nkz1GkaMiTpqGqvOA++WgKLU8aXAN3T5hkBPGtmvwGaAL0zLcjMhgHDANq0aVPjgYpIaZk/PxTyGz0afvnLulfEr6bFeUSR6aNJP/gbBIxy91ZAH2C0mW0Qk7vf4e5l7l7WokWLGEIVkdpuxgy4554wfNRRoW/ipJOUJGpCnIliCdA6ZbwVGzYtnQY8DODurwGNgW1ijElESswPP8Dw4dC9O1x9dUURv803z/06yV+ciWIa0N7MdjCzRoTO6jFp8ywCDgIws46ERLE0xphEpIRMnAh77AHXXRf6IGbOVBG/OMTWR+Hua8zsHGA8UB+4x93nmNlVwHR3HwNcCNxpZucTmqWGuOvcBBGp3IcfwkEHQevW8NxzYVjiYbXte7msrMynT5+edBgikpA334TddgvDTz0VKr42aZJsTLWBmc1w97LqvFZFAUWkVvj8cxg8GHbfvaKI35FHKkkUgq5NFJGi5g6PPALnnAPLlsEVV4SOaykcJQoRKWqnnBKuhygrg+efr2h2ksJRohCRopNaxK9nz9Dc9Nvfqj5TUtRHISJFZeFC6N0bRo0K46edBhddpCSRJCUKESkKa9fCTTeFpqVp06Cevp2KhnK0iCRu7lwYOhSmTIEjjoDbb4dWrZKOSsopUYhI4t57DxYsgPvvh4EDVZ+p2ChRiEgipk2DWbPgjDPCUcTChdCsWdJRSSZqBRSRgvr++9A5vc8+8Kc/VRTxU5IoXkoUIlIwEyaEU11vuCEcSaiIX+2gpicRKYglS+Dgg6FtW3jhhVCjSWoHHVGISKzeeCP8bdUKnnwSZs9WkqhtlChEJBZLl8KJJ0KXLvDSS+G5Pn1gs82SjUuqTk1PIlKj3OHBB+Hcc+Hrr+HKK2HffZOOSjaGEoWI1KjBg+Gf/wwVXu++Gzp3Tjoi2VhKFCKy0datCxfJmYX+h732CkcU9esnHZnUhEr7KMxsUzO71Mxuj8Z3NrPD4w9NRGqD+fPDbUjvvTeMn3YanH++kkQpyacz+x7AgP2i8Y+Aa2OLSERqhTVr4C9/CUX8Zs6ERo2Sjkjikk+iaO/u1wKrAdz9e0LiEJE66q23Qgf1xRfDoYeGon4nnZR0VBKXfPooVplZY8ABzGwHYFWsUYlIUVu0CD74IJzddMIJKuJX6vJJFFcDzwCtzOw+oCdweqxRiUjRmTIlXDw3bFi4HmLhQmjaNOmopBAqbXpy96eB44EzgH8B3dz9ubgDE5Hi8N13cMEFoanp+uth5crwvJJE3ZHPWU/PuvtSd3/S3Z9w98/M7NlCBCciyXrhhVDE769/hTPPhNdfh002SToqKbSsTU9m1ghoDPzUzJpR0YG9OdCmALGJSIKWLAkd1TvsEEpwHHBA0hFJUnL1UfwauAD4CTCHikTxDXB7zHGJSEJmzoQ99wxF/MaOhZ49YdNNk45KkpS16cnd/+rurYFL3L2Nu7eOHp3d/aYCxigiBfDppzBgAHTtWlHE77DDlCQkj7Oe3P0mM+sAdCI0RZU/f3+cgYlIYbiH2kznnQfLl8M110CPHklHJcWk0kRhZpcDhwAdgPHAocAkQIlCpASceGK4HmLffUMRv44dk45Iik0+11EMALoAr7v7YDPbDhgZb1giEqfUIn6HHBKSxK9/rfpMklk+JTx+cPe1wJro7KdPgB3jDUtE4vLOO6HC6z33hPFTT1WlV8ktn0Qx08y2IBQHnA5MBV6PNSoRqXFr1oQL5vbYI9yOVJ3Ukq+cTU9mZsAId/8KuMXMxgObu7sShUgtMns2DB0KM2bAMcfALbfAdtslHZXUFjkThbu7mT0F7BWNzy9IVCJSo5YsgcWL4ZFHoH9/FfGTqsmn6WmqmXWtzsLN7DAzm2dm881seJZ5TjCzuWY2x8x0JpVIDXn1Vbg9ujS2vIjfcccpSUjV5ZMo9iMki3lm9rqZzTSzSpuezKw+cAtwOOEajEFm1iltnvbApcDP3b0z8Nsqr4GIrGf58nBNxH77wQ03VBTxa9Ik2bik9srn9Nijq7nsbsB8d18IYGYPAv2AuSnznAHc4u7LANz9s2q+l4gAzz4byoAvWhROd732WhXxk42Xz5XZC6q57JbA4pTxJUD3tHl2ATCzV4D6hI7zZ9IXZGbDgGEAbdqoHqFIJosXwxFHwE47wcSJ4YhCpCbk0/RUXZlaQj1tvAHQHugFDALuik7FXf9F7ne4e5m7l7Vo0aLGAxWpzWbMCH9bt4Zx42DWLCUJqVlxJoolQOuU8VbARxnmedLdV7v7e8A8QuIQkUp88gkcfzyUlVUU8Tv4YGjcOPfrRKoqr0RhZq3M7MBoeBMzy6dbbBrQ3sx2iO5tMRAYkzbPE0D5crchNEUtzDd4kbrIHe67Dzp1CmXAr71WRfwkXvnc4W4o4Qv+ruiptsCTlb3O3dcA5xAKCb4NPOzuc8zsKjPrG802HvjCzOYCLwIXu/sXVV8Nkbpj4EAYMiQkilmz4NJLoWHDpKOSUmbu6d0GaTOYzSKcwTTF3feMnpvt7rsXIL4NlJWV+fTp05N4a5HEpBbxu+8++PZbOPtsqBdn47GUFDOb4e5l1XltPrvZCndflfJm9cncUS0iMfjvf8NtSO++O4yfcgqcc46ShBROPrvaK2b2O6Bx1E/xEPBUvGGJyOrVof9hjz1g7lxo2jTpiKSuyidR/A74FvgvcB7wPHBZnEGJ1HWzZkG3bnDZZdC3b0gUAwcmHZXUVflcmd0HuMvdb4s7GBEJPvkkPB57DI49NulopK7L54jiBGC+md1rZodGfRQiUsMmTYJbbw3Dhx0GCxYoSUhxqDRRuPtgwvUNY4GhwEIzuz3uwETqim+/DZ3T++8PN91UUcRvs82SjUukXF7nTbj7SsK1E6MIF9KdEGNMInXG+PGw667hSOK88+D111XET4pPpX0UZtabcFV1b+AV4B/AiTHHJVLyFi+GI4+EnXcOzU66ulqKVT6d2WcCDwK/cfcfYo5HpKS5w7Rp4Yym1q3h6adDAT/VZ5Jilk8fxXHu/qiShMjG+fjjcBvS7t0rivj17q0kIcUv6xGFmb3k7j3NbBnrlwc3wu20t4o9OpES4A6jRsEFF8CKFXDddfDznycdlUj+cjU9HRj93aYQgYiUqhNOgEcfDWc13XUX7LJL0hGJVE3Wpid3XxcN3u3ua1MfwN2FCU+kdlq7NhTyAzjqqHBW04QJShJSO+Vzeux6VWKjC+72jicckdrv7bfD0UN5Eb+TT4azzlIRP6m9su66ZnZJ1D+xu5l9GT2WAUuBcQWLUKSWWL0arrkGunSBefOgefOkIxKpGbn6KK4HbgD+BAwvfzJqehKRFDNnhpsJzZ4NAwbAzTfDT36SdFQiNSNXotjZ3d81s9FA5/InzcKtKNx9dsyxidQan34Kn38OTzwB/folHY1IzcqVKIYDpwG3ZJjmwAGxRCRSS0ycCG++Cb/+dSjiN38+bLpp0lGJ1LysicLdT4v+7l+4cESK3zffwPDhcNtt4Sym008P9ZmUJKRUVXoehpkda2bNouHhZvawme0Rf2gixWfcOOjcGUaODBfQqYif1AX5nLA3wt2/NbMewFGEW6GOjDcskeKzeHHof2jeHF59FW64AZo0SToqkfjlkyjKz3I6ErjV3R8D9BtK6gR3mDw5DLduDc8+G44iundPNi6RQsonUXxsZrcQSo2PM7NGeb5OpFb76CM4+mjYd9+KIn4HHgiNGiUbl0ih5Xsr1JeAPu6+jFD7aXjul4jUXu6hJlOnTuEI4i9/URE/qdsqvR+Fuy83s7lALzPrBbzs7k/HHplIQo47Dh5/HHr2DAlj552TjkgkWfmc9XQO8DDQJno8bGZnxx2YSCGlFvE7+mi4/XZ44QUlCRHI7w53w4Bu7r4cwMyuBV4Fbo0zMJFCeeutcC3EaafBGWfA4MFJRyRSXPLpozBgdcr46ug5kVpt1Sq48kro2hUWLIAtt0w6IpHilM8RxWhgspk9RkgQRwP3xRqVSMxmzAhF/N56C048EW66CVq0SDoqkeKUT2f29Wb2IlBeyuNMd58Wb1gi8friC/jqKxg7Fo48MuloRIpbPkcUACujx7ror0it8+KLoYjfuefCIYfAu+9C48ZJRyVS/PI56+ky4AFgO6AVcL+ZXRp3YCI15euv4Ve/gl/8IhTyWxn91FGSEMlPPp3ZJwF7u/vl7n4Z0A04Od6wRGrG2LHhwrm77oKLLgp9EyriJ1I1+TQ9fZA2XwNgYTzhiNScxYuhf3/o0CHcUGhv3eldpFrySRTfA3PMbDzhhkWHAJPM7EYAd78gxvhEqsQdXnsNevSoKOLXo4fqM4lsjHyanv4NjABeAyYDVwEvAHOiR1ZmdpiZzTOz+WaWtT6UmR1nZm5mZXlHLpJmyRLo2zfUZSov4terl5KEyMbK5/TYu6uzYDOrT7iN6sHAEmCamY1x97lp8zUDzgWmVOd9RNatgzvvhIsvhjVr4MYbYb/9ko5KpHTEWS68GzDf3Re6+yrgQSDTbeevBq4HVsQYi5Sw/v3hzDNDH8Rbb8H550P9+klHJVI64kwULYHFKeNLoud+ZGZ7Aq3d/alcCzKzYWY23cymL126tOYjlVpnzZqKIn79+4cjiueegx13TDYukVKUd6Iws6qeVJipHpSnLK8e8FfgwsoW5O53uHuZu5e1UJ2FOm/27HAzoTvvDOMnnRSK+pkqkInEIp8L7rqZ2ZvAu9H4Hmb2tzyWvQRonTLeCvgoZbwZsCswwczeB/YBxqhDW7JZuRKuuAL22gs++EC1mUQKJZ8jipsJ98v+AsDd3wAOzON104D2ZrZDdPvUgcCY8onu/rW7b+Pu7dy9HeGMqr7uPr2K6yB1wLRpocrrVVfBoEHw9ttw7LFJRyVSN+RzHUU9d//A1j+uX1vZi9x9TXTTo/FAfeAed59jZlcB0919TO4liFRYtgyWL4dx4+Dww5OORqRuySdRLDazboBHp7z+Bngnn4W7+zhgXNpz/5Nl3l75LFPqjhdeCEX8zjsvFPF75x2V3xBJQj5NT2cBFxBug/opoS/hrDiDkrrtq6/CneYOOghGjqwo4qckIZKMfC64+4zQvyASuyefhLM+xxUeAAAQw0lEQVTOgk8/hd/9DkaMUIIQSVqlicLM7iTltNZy7j4sloikzlq0CI4/Hjp2hDFjoEznv4kUhXz6KJ5LGW4MHMP6F9KJVJs7TJoE++8PbdqEi+b22Uf1mUSKST5NTw+ljpvZaOA/sUUkdcaiRaH0xtNPw4QJ0LMnHHBA0lGJSLrqlPDYAWhb04FI3bFuHdx6K3TuDBMnws03q4ifSDHLp49iGRV9FPWAL4GsJcNFKnPssaHT+uCD4Y47oF27pCMSkVxyJgoLV9ntAXwYPbXO3Tfo2BapzJo1UK9eeAwYAP36wZAhqs8kUhvkbHqKksK/3H1t9FCSkCp74w3o3j0cPUAowXHqqUoSIrVFPn0UU82sa+yRSMlZsQIuvzyc5rpkCWy7bdIRiUh1ZG16MrMG7r4G2A84w8wWAN8Ryoe7uyt5SFZTp8Ipp8B//xv+3ngjbLVV0lGJSHXk6qOYCnQFji5QLFJCvvkGfvgBnnkGDj006WhEZGPkShQG4O4LChSL1HLPPgtz5oRbkfbuDfPmqfyGSCnIlShamNkF2Sa6+40xxCO10LJlcMEFMGpUuDbi7LNDglCSECkNuTqz6wNNCXeiy/QQ4fHHoVMnGD0aLr0Upk9XghApNbmOKD5296sKFonUOosWwcCBsOuu4YZCe+6ZdEQiEodcRxQ6y1024A4vvRSG27QJNxeaMkVJQqSU5UoUBxUsCqkVPvgg3Ia0V6+KZLHfftCwYaJhiUjMsiYKd/+ykIFI8Vq3Dv7+99BRPWkS/O1voSy4iNQN+dyPQuq4o4+GsWPD9RAjR0Jb1Q4WqVOUKCSj1auhfv1QxG/QIDjuOBg8WPWZROqi6tyPQkrc669Dt25w++1hfNAgOPlkJQmRukqJQn70ww/hWohu3eCTT6B166QjEpFioKYnAWDy5FC87513YOhQ+MtfYMstk45KRIqBEoUA8N13oV/iP/8JdZpERMopUdRhzzwTivhdeCEcdFAoCd6oUdJRiUixUR9FHfTFF6GZ6fDD4b77YNWq8LyShIhkokRRh7jDo4+GIn733x/uPjdtmhKEiOSmpqc6ZNEiOPFE2H33cO+IPfZIOiIRqQ10RFHi3EPhPghXVE+YEM5wUpIQkXwpUZSw996DQw4JHdXlRfx69IAGOo4UkSpQoihBa9fC//5vuE/ElClw220q4ici1affliWoXz/497+hT59QhkNXWIvIxlCiKBGpRfwGDw71mU48UfWZRGTjxdr0ZGaHmdk8M5tvZsMzTL/AzOaa2Wwze97MVMC6GqZPh7Ky0MQEMGAA/PKXShIiUjNiSxRmVh+4BTgc6AQMMrNOabPNBMrcfXfgUeD6uOIpRT/8AJdcAt27w9Kluk+EiMQjziOKbsB8d1/o7quAB4F+qTO4+4vu/n00OhloFWM8JeW118IprtdfH4r4zZ0LRx6ZdFQiUori7KNoCSxOGV8CdM8x/2nA05kmmNkwYBhAmzZtaiq+Wu2HH8ItSp97Lpz+KiISlzgTRaYWcs84o9lJQBnQM9N0d78DuAOgrKws4zLqgnHjQhG/iy+GX/wC3n4bGjZMOioRKXVxNj0tAVJPzGwFfJQ+k5n1Bi4D+rr7yhjjqbU+/xxOOgmOOAL++c+KIn5KEiJSCHEmimlAezPbwcwaAQOBMakzmNmewEhCkvgsxlhqJXd48EHo2BEefhiuuAKmTlURPxEprNiantx9jZmdA4wH6gP3uPscM7sKmO7uY4A/A02BRyycy7nI3fvGFVNts2hRKAe+xx5w992w225JRyQidZG5164m/7KyMp8+fXrSYcTGHZ5/vuIuc5Mnw957h4vpRESqy8xmuHtZdV6rWk9FZMGCcAbTwQdXFPHbZx8lCRFJlhJFEVi7Fm68MTQtzZgBI0eqiJ+IFA/VeioCRx0FTz8dLpi77TZopcsORaSIKFEkZNWqcF+IevVgyJBQyG/gQNVnEpHio6anBEydCnvtBbfeGsZPOCFUe1WSEJFipERRQN9/DxdeCPvuC8uWwU47JR2RiEjl1PRUIJMmhWsiFi6EX/0KrrsOmjdPOioRkcopURRI+Y2FXnwRevVKOhoRkfwpUcRo7NhQuO93v4MDDwylwBtoi4tILaM+ihgsXRpuQ9q3LzzwQEURPyUJEamNlChqkDvcf38o4vfoo3DVVTBlior4iUjtpt+4NWjRIjj1VNhzz1DEr3PnpCMSEdl4OqLYSOvWwfjxYbhtW3j5ZXjlFSUJESkdShQb4d13w53mDjsMJk4Mz3XrpiJ+IlJalCiqYc0a+POfYffdYdas0MykIn4iUqrUR1ENRx4Zmpv69QtlOLbfPumIRETio0SRp5Urwz2q69WD00+HoUPh+ONVn0lESp+anvIweTJ07Qq33BLGjzsuFPJTkhCRukCJIofvvoPzz4cePeDbb6F9+6QjEhEpPDU9ZfHyy6GI33vvwdlnw5/+BJtvnnRUIiKFp0SRxZo1oU/ipZfggAOSjkZEJDlKFCmeeCIU8bv00lDEb84c1WcSEVEfBfDpp6Fz+phjQo0mFfETEalQpxOFO4weDZ06wZNPwh//GM5wUhE/EZEKdfo386JF4ZqIsrJwdXWHDklHJCJSfOrcEcW6dfD002G4bdtQwG/iRCUJEZFs6lSieOedcBvSPn3C2UwQjiZUxE9EJLs6kSjWrIHrrgtF/N58E+69V6e8iojkq070URxxBDz7LBx7bCjDse22SUckIlJ7lGyiWLEiXDBXvz4MGxYe/fsnHZWISO1Tkk1Pr7wCXbpUFPHr319JQkSkukoqUSxfDueeG24itGIFdOyYdEQiIrVfyTQ9vfRSKOK3aBGccw5cey00bZp0VCIitV/JJAqAzTYLVV9//vOkIxERKR21OlE8/jj897/w+99Dz57h1FddEyEiUrNi7aMws8PMbJ6ZzTez4Rmmb2JmD0XTp5hZu3yW+8kn4S5z/fvDv/5VUcRPSUJEpObFlijMrD5wC3A40AkYZGad0mY7DVjm7jsDfwWuq2y5X3wROqmfeircTOjVV1XET0QkTnEeUXQD5rv7QndfBTwI9Eubpx9wXzT8KHCQWe47UX/wAey6K7zxBgwfHq6VEBGR+MTZR9ESWJwyvgTonm0ed19jZl8DWwOfp85kZsOAYdHoykmT7C0V8QNgG9K2VR2mbVFB26KCtkWFn1X3hXEmikxHBl6NeXD3O4A7AMxsuruXbXx4tZ+2RQVtiwraFhW0LSqY2fTqvjbOpqclQOuU8VbAR9nmMbMGQHPgyxhjEhGRKoozUUwD2pvZDmbWCBgIjEmbZwxwSjR8HPCCu29wRCEiIsmJrekp6nM4BxgP1Afucfc5ZnYVMN3dxwB3A6PNbD7hSGJgHou+I66YayFtiwraFhW0LSpoW1So9rYw/YAXEZFcSqoooIiI1DwlChERyaloE0Vc5T9qozy2xQVmNtfMZpvZ82bWNok4C6GybZEy33Fm5mZWsqdG5rMtzOyEaN+YY2b3FzrGQsnjf6SNmb1oZjOj/5M+ScQZNzO7x8w+M7O3skw3M7s52k6zzaxrXgt296J7EDq/FwA7Ao2AN4BOafOcDdweDQ8EHko67gS3xYHAZtHwWXV5W0TzNQMmApOBsqTjTnC/aA/MBLaMxn+SdNwJbos7gLOi4U7A+0nHHdO2OADoCryVZXof4GnCNWz7AFPyWW6xHlHEUv6jlqp0W7j7i+7+fTQ6mXDNSinKZ78AuBq4HlhRyOAKLJ9tcQZwi7svA3D3zwocY6Hksy0c2Dwabs6G13SVBHefSO5r0foB//BgMrCFmW1X2XKLNVFkKv/RMts87r4GKC//UWry2RapTiP8YihFlW4LM9sTaO3uTxUysATks1/sAuxiZq+Y2WQzO6xg0RVWPttiBHCSmS0BxgG/KUxoRaeq3ydA8d6PosbKf5SAvNfTzE4CyoCesUaUnJzbwszqEaoQDylUQAnKZ79oQGh+6kU4ynzZzHZ1969ijq3Q8tkWg4BR7n6Dme1LuH5rV3dfF394RaVa35vFekSh8h8V8tkWmFlv4DKgr7uvLFBshVbZtmgG7ApMMLP3CW2wY0q0Qzvf/5En3X21u78HzCMkjlKTz7Y4DXgYwN1fAxoTCgbWNXl9n6Qr1kSh8h8VKt0WUXPLSEKSKNV2aKhkW7j71+6+jbu3c/d2hP6avu5e7WJoRSyf/5EnCCc6YGbbEJqiFhY0ysLIZ1ssAg4CMLOOhESxtKBRFocxwMnR2U/7AF+7+8eVvagom548vvIftU6e2+LPQFPgkag/f5G7900s6JjkuS3qhDy3xXjgEDObC6wFLnb3L5KLOh55bosLgTvN7HxCU8uQUvxhaWYPEJoat4n6Y64AGgK4++2E/pk+wHzge+DUvJZbgttKRERqULE2PYmISJFQohARkZyUKEREJCclChERyUmJQkREclKikKJlZmvNbFbKo12Oedtlq5hZaGZWZmY3R8O9zKxHyrQzzezkAsbSpVQrpUrhFOV1FCKRH9y9S9JBVFV0gV/5RX69gOXAq9G022v6/cysQVTvLJMuhLIu42r6faXu0BGF1CrRkcPLZvZ69OiRYZ7OZjY1OgqZbWbto+dPSnl+pJnVz/Da983sumi+qWa2c/R8Wwv3+ii/50eb6PnjzewtM3vDzCZGz/Uys6eiI6AzgfOj99zfzEaY2UVm1tHMpqat1+xoeC8ze8nMZpjZ+EzVPc1slJndaGYvAteZWTcze9XC/RZeNbOfRVcpXwUMiN5/gJk1sXDPgmnRvJmq74qsL+n66Xroke1BuJp4VvT4V/TcZkDjaLg94cpbgHZENfiBvwG/jIYbAZsCHYGxQMPo+VuBkzO85/vAZdHwycBT0fBY4JRoeCjwRDT8JtAyGt4i+tsr5XUjgItSlv/jeLReO0bDlwCXE66ifRVoET0/gHClcXqco4CngPrR+OZAg2i4N/BYNDwE+HvK664FTiqPF3gHaJL0Z61HcT/U9CTFLFPTU0Pg72bWhZBIdsnwuteAy8ysFfC4u79rZgcBewHTojInmwLZ6mI9kPL3r9HwvsCx0fBowv0uAF4BRpnZw8DjVVk5QpG6E4D/R0gIA4CfEQob/ieKsz6QrRbPI+6+NhpuDtwXHT05UdmGDA4B+prZRdF4Y6AN8HYVY5c6RIlCapvzgU+BPQhNpxvcnMjd7zezKcARwHgzO51QXvk+d780j/fwLMMbzOPuZ5pZ9+i9ZkUJLF8PEepzPR4W5e+a2W7AHHffN4/Xf5cyfDXworsfEzV5TcjyGgP6u/u8KsQpdZz6KKS2aQ587OE+AoMJv7jXY2Y7Agvd/WZCtczdgeeB48zsJ9E8W1n2e4sPSPn7WjT8KhWFJ38JTIqWs5O7T3H3/wE+Z/0SzgDfEsqfb8DdFxCOiv5ASBoQSoG3sHDPBMysoZl1zhJnqubAh9HwkBzvPx74jUWHKxYqD4vkpEQhtc2twClmNpnQ7PRdhnkGAG+Z2SygA+HWj3MJfQDPRp3G/wGy3QJyk+iI5DzCEQzAucCp0WsHR9MA/mxmb0an5k4k3K851VjgmPLO7Azv9RBwEhX3SlhFKJt/nZm9QejH2KDDPoPrgT+Z2SusnzxfBDqVd2YTjjwaArOjmK/OY9lSx6l6rEgKCzc8KnP3z5OORaRY6IhCRERy0hGFiIjkpCMKERHJSYlCRERyUqIQEZGclChERCQnJQoREcnp/wNtqV2CXGGpkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Area Under the Curve is:  0.008264462809917356\n",
      "67 13.0 0.19402985074626866\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# coding:utf8\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def load_sim_data():\n",
    "    \"\"\"\n",
    "    测试数据，\n",
    "    :return: data_arr   feature对应的数据集\n",
    "            label_arr  feature对应的分类标签\n",
    "    \"\"\"\n",
    "    data_mat = np.matrix([[1.0, 2.1],\n",
    "                          [2.0, 1.1],\n",
    "                          [1.3, 1.0],\n",
    "                          [1.0, 1.0],\n",
    "                          [2.0, 1.0]])\n",
    "    class_labels = [1.0, 1.0, -1.0, -1.0, 1.0]\n",
    "    return data_mat, class_labels\n",
    "\n",
    "\n",
    "def load_data_set(file_name):\n",
    "    \"\"\"\n",
    "    加载马的疝气病的数据\n",
    "    :param file_name: 文件名\n",
    "    :return: 必须要是np.array或者np.matrix不然后面没有，shape\n",
    "    \"\"\"\n",
    "    num_feat = len(open(file_name).readline().split('\\t'))\n",
    "    data_arr = []\n",
    "    label_arr = []\n",
    "    fr = open(file_name)\n",
    "    for line in fr.readlines():\n",
    "        line_arr = []\n",
    "        cur_line = line.strip().split('\\t')\n",
    "        for i in range(num_feat - 1):\n",
    "            line_arr.append(float(cur_line[i]))\n",
    "        data_arr.append(line_arr)\n",
    "        label_arr.append(float(cur_line[-1]))\n",
    "    return np.matrix(data_arr), label_arr\n",
    "\n",
    "\n",
    "def stump_classify(data_mat, dimen, thresh_val, thresh_ineq):\n",
    "    \"\"\"\n",
    "    (将数据集，按照feature列的value进行 二分法切分比较来赋值分类)\n",
    "    :param data_mat: Matrix数据集\n",
    "    :param dimen: 特征的哪一个列\n",
    "    :param thresh_val: 特征列要比较的值\n",
    "    :param thresh_ineq: \n",
    "    :return: np.array\n",
    "    \"\"\"\n",
    "    ret_array = np.ones((np.shape(data_mat)[0], 1))\n",
    "    # data_mat[:, dimen] 表示数据集中第dimen列的所有值\n",
    "    # thresh_ineq == 'lt'表示修改左边的值，gt表示修改右边的值\n",
    "    # （这里其实我建议理解为转换左右边，就是一棵树的左右孩子，可能有点问题。。。待考证）\n",
    "    if thresh_ineq == 'lt':\n",
    "        ret_array[data_mat[:, dimen] <= thresh_val] = -1.0\n",
    "    else:\n",
    "        ret_array[data_mat[:, dimen] > thresh_val] = -1.0\n",
    "    return ret_array\n",
    "\n",
    "\n",
    "def build_stump(data_arr, class_labels, D):\n",
    "    \"\"\"\n",
    "    得到决策树的模型 (这个比较重要，需要看懂）\n",
    "    :param data_arr: 特征标签集合\n",
    "    :param class_labels: 分类标签集合\n",
    "    :param D: 最初的特征权重值\n",
    "    :return: bestStump    最优的分类器模型\n",
    "            min_error     错误率\n",
    "            best_class_est  训练后的结果集\n",
    "    \"\"\"\n",
    "    data_mat = np.mat(data_arr)\n",
    "    label_mat = np.mat(class_labels).T\n",
    "\n",
    "    m, n = np.shape(data_mat)\n",
    "    num_steps = 10.0\n",
    "    best_stump = {}\n",
    "    best_class_est = np.mat(np.zeros((m, 1)))\n",
    "    # 无穷大\n",
    "    min_err = np.inf\n",
    "    for i in range(n):\n",
    "        range_min = data_mat[:, i].min()\n",
    "        range_max = data_mat[:, i].max()\n",
    "        step_size = (range_max - range_min) / num_steps\n",
    "        for j in range(-1, int(num_steps) + 1):\n",
    "            for inequal in ['lt', 'gt']:\n",
    "                thresh_val = (range_min + float(j) * step_size)\n",
    "                predicted_vals = stump_classify(data_mat, i, thresh_val, inequal)\n",
    "                err_arr = np.mat(np.ones((m, 1)))\n",
    "                err_arr[predicted_vals == label_mat] = 0\n",
    "                # 这里是矩阵乘法\n",
    "                weighted_err = D.T * err_arr\n",
    "                '''\n",
    "                dim            表示 feature列\n",
    "                thresh_val      表示树的分界值\n",
    "                inequal        表示计算树左右颠倒的错误率的情况\n",
    "                weighted_error  表示整体结果的错误率\n",
    "                best_class_est    预测的最优结果 （与class_labels对应）\n",
    "                '''\n",
    "                # print('split: dim {}, thresh {}, thresh inequal: {}, the weighted err is {}'.format(\n",
    "                #     i, thresh_val, inequal, weighted_err\n",
    "                # ))\n",
    "                if weighted_err < min_err:\n",
    "                    min_err = weighted_err\n",
    "                    best_class_est = predicted_vals.copy()\n",
    "                    best_stump['dim'] = i\n",
    "                    best_stump['thresh'] = thresh_val\n",
    "                    best_stump['ineq'] = inequal\n",
    "    # best_stump 表示分类器的结果，在第几个列上，用大于／小于比较，阈值是多少 (单个弱分类器)\n",
    "    return best_stump, min_err, best_class_est\n",
    "\n",
    "\n",
    "def ada_boost_train_ds(data_arr, class_labels, num_it=40):\n",
    "    \"\"\"\n",
    "    adaBoost训练过程放大\n",
    "    :param data_arr: 特征标签集合\n",
    "    :param class_labels: 分类标签集合\n",
    "    :param num_it: 迭代次数\n",
    "    :return: weak_class_arr  弱分类器的集合\n",
    "            agg_class_est   预测的分类结果值\n",
    "    \"\"\"\n",
    "    weak_class_arr = []\n",
    "    m = np.shape(data_arr)[0]\n",
    "    # 初始化 D，设置每个特征的权重值，平均分为m份\n",
    "    D = np.mat(np.ones((m, 1)) / m)\n",
    "    agg_class_est = np.mat(np.zeros((m, 1)))\n",
    "    for i in range(num_it):\n",
    "        # 得到决策树的模型\n",
    "        best_stump, error, class_est = build_stump(data_arr, class_labels, D)\n",
    "        # print('D: {}'.format(D.T))\n",
    "        # alpha 目的主要是计算每一个分类器实例的权重(加和就是分类结果)\n",
    "        # 计算每个分类器的 alpha 权重值\n",
    "        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))\n",
    "        best_stump['alpha'] = alpha\n",
    "        # store Stump Params in Array\n",
    "        weak_class_arr.append(best_stump)\n",
    "        # print('class_est: {}'.format(class_est.T))\n",
    "        # 分类正确：乘积为1，不会影响结果，-1主要是下面求e的-alpha次方\n",
    "        # 分类错误：乘积为 -1，结果会受影响，所以也乘以 -1\n",
    "        expon = np.multiply(-1 * alpha * np.mat(class_labels).T, class_est)\n",
    "        # 判断正确的，就乘以-1，否则就乘以1， 为什么？ 书上的公式。\n",
    "        # print('(-1取反)预测值 expon=', expon.T)\n",
    "        # 计算e的expon次方，然后计算得到一个综合的概率的值\n",
    "        # 结果发现： 判断错误的样本，D对于的样本权重值会变大。\n",
    "        # multiply是对应项相乘\n",
    "        D = np.multiply(D, np.exp(expon))\n",
    "        D = D / D.sum()\n",
    "        # 预测的分类结果值，在上一轮结果的基础上，进行加和操作\n",
    "        # print('叠加前的分类结果class_est: {}'.format(class_est.T))\n",
    "        agg_class_est += alpha * class_est\n",
    "        # print('叠加后的分类结果agg_class_est: {}'.format(agg_class_est.T))\n",
    "        # sign 判断正为1， 0为0， 负为-1，通过最终加和的权重值，判断符号。\n",
    "        # 结果为：错误的样本标签集合，因为是 !=,那么结果就是0 正, 1 负\n",
    "        agg_errors = np.multiply(np.sign(agg_class_est) != np.mat(class_labels).T,\n",
    "                                 np.ones((m, 1)))\n",
    "        error_rate = agg_errors.sum() / m\n",
    "        # print('total error: {}\\n'.format(error_rate))\n",
    "        if error_rate == 0.0:\n",
    "            break\n",
    "    return weak_class_arr, agg_class_est\n",
    "\n",
    "\n",
    "def ada_classify(data_to_class, classifier_arr):\n",
    "    \"\"\"\n",
    "    通过刚刚上面那个函数得到的弱分类器的集合进行预测\n",
    "    :param data_to_class: 数据集\n",
    "    :param classifier_arr: 分类器列表\n",
    "    :return: 正负一，也就是表示分类的结果\n",
    "    \"\"\"\n",
    "    data_mat = np.mat(data_to_class)\n",
    "    m = np.shape(data_mat)[0]\n",
    "    agg_class_est = np.mat(np.zeros((m, 1)))\n",
    "    for i in range(len(classifier_arr)):\n",
    "        class_est = stump_classify(\n",
    "            data_mat, classifier_arr[i]['dim'],\n",
    "            classifier_arr[i]['thresh'],\n",
    "            classifier_arr[i]['ineq']\n",
    "        )\n",
    "        agg_class_est += classifier_arr[i]['alpha'] * class_est\n",
    "        #print(agg_class_est)\n",
    "    return np.sign(agg_class_est)\n",
    "\n",
    "\n",
    "def plot_roc(pred_strengths, class_labels):\n",
    "    \"\"\"\n",
    "    (打印ROC曲线，并计算AUC的面积大小)\n",
    "    :param pred_strengths: 最终预测结果的权重值\n",
    "    :param class_labels: 原始数据的分类结果集\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    # variable to calculate AUC\n",
    "    y_sum = 0.0\n",
    "    # 对正样本的进行求和\n",
    "    num_pos_class = np.sum(np.array(class_labels) == 1.0)\n",
    "    # 正样本的概率\n",
    "    y_step = 1 / float(num_pos_class)\n",
    "    # 负样本的概率\n",
    "    x_step = 1 / float(len(class_labels) - num_pos_class)\n",
    "    # np.argsort函数返回的是数组值从小到大的索引值\n",
    "    # get sorted index, it's reverse\n",
    "    sorted_indicies = pred_strengths.argsort()\n",
    "    # 测试结果是否是从小到大排列\n",
    "    # 可以选择打印看一下\n",
    "    # 开始创建模版对象\n",
    "    fig = plt.figure()\n",
    "    fig.clf()\n",
    "    ax = plt.subplot(111)\n",
    "    # cursor光标值\n",
    "    cur = (1.0, 1.0)\n",
    "    # loop through all the values, drawing a line segment at each point\n",
    "    for index in sorted_indicies.tolist()[0]:\n",
    "        if class_labels[index] == 1.0:\n",
    "            del_x = 0\n",
    "            del_y = y_step\n",
    "        else:\n",
    "            del_x = x_step\n",
    "            del_y = 0\n",
    "            y_sum += cur[1]\n",
    "        # draw line from cur to (cur[0]-delX, cur[1]-delY)\n",
    "        # 画点连线 (x1, x2, y1, y2)\n",
    "        # print cur[0], cur[0]-delX, cur[1], cur[1]-delY\n",
    "        ax.plot([cur[0], cur[0] - del_x], [cur[1], cur[1] - del_y], c='b')\n",
    "        cur = (cur[0] - del_x, cur[1] - del_y)\n",
    "    # 画对角的虚线线\n",
    "    ax.plot([0, 1], [0, 1], 'b--')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve for AdaBoost horse colic detection system')\n",
    "    # 设置画图的范围区间 (x1, x2, y1, y2)\n",
    "    ax.axis([0, 1, 0, 1])\n",
    "    plt.show()\n",
    "    '''\n",
    "    参考说明：http://blog.csdn.net/wenyusuran/article/details/39056013\n",
    "    为了计算 AUC ，我们需要对多个小矩形的面积进行累加。\n",
    "    这些小矩形的宽度是x_step，因此可以先对所有矩形的高度进行累加，最后再乘以x_step得到其总面积。\n",
    "    所有高度的和(y_sum)随着x轴的每次移动而渐次增加。\n",
    "    '''\n",
    "    print(\"the Area Under the Curve is: \", y_sum * x_step)\n",
    "\n",
    "\n",
    "def test():\n",
    "    # D = np.mat(np.ones((5, 1)) / 5)\n",
    "    # data_mat, class_labels = load_sim_data()\n",
    "    # print(data_mat.shape)\n",
    "    # result = build_stump(data_mat, class_labels, D)\n",
    "    # print(result)\n",
    "    # classifier_array, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 9)\n",
    "    # print(classifier_array, agg_class_est)\n",
    "    data_mat, class_labels = load_data_set('./dataset/horseColicTraining2.txt')\n",
    "    print(data_mat.shape, len(class_labels))\n",
    "    weak_class_arr, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 40)\n",
    "    print(weak_class_arr, '\\n-----\\n', agg_class_est.T)\n",
    "    plot_roc(agg_class_est, class_labels)\n",
    "    data_arr_test, label_arr_test = load_data_set(\"./dataset/horseColicTest2.txt\")\n",
    "    m = np.shape(data_arr_test)[0]\n",
    "    predicting10 = ada_classify(data_arr_test, weak_class_arr)\n",
    "    err_arr = np.mat(np.ones((m, 1)))\n",
    "    # 测试：计算总样本数，错误样本数，错误率\n",
    "    print(m,\n",
    "          err_arr[predicting10 != np.mat(label_arr_test).T].sum(),\n",
    "          err_arr[predicting10 != np.mat(label_arr_test).T].sum() / m\n",
    "          )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**要点补充**\n",
    "\n",
    "> 非均衡现象：\n",
    "\n",
    "在分类器训练时，正例数目和反例数目不相等（相差很大）。或者发生在正负例分类错误的成本不同的时候。\n",
    "\n",
    "* 判断马是否能继续生存(不可误杀)\n",
    "* 过滤垃圾邮件(不可漏判)\n",
    "* 不能放过传染病的人\n",
    "* 不能随便认为别人犯罪\n",
    "\n",
    "我们有多种方法来处理这个问题： 具体可[参考此链接](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)\n",
    "\n",
    "再结合书中的方法，可以归为八大类：\n",
    "\n",
    "**1. 能否收集到更多的数据？**\n",
    "\n",
    "这个措施往往被人们所忽略，被认为很蠢。但是更大的数据集更能体现样本的分布，多样性。\n",
    "\n",
    "**2. 尝试使用其他的评价指标**\n",
    "\n",
    "Accuracy 或者error rate 不能用于非均衡的数据集。这会误导人。这时候可以尝试其他的评价指标。\n",
    "\n",
    "Confusion Matrix 混淆矩阵：使用一个表格对分类器所预测的类别与其真实的类别的样本统计，分别为：TP、FN、FP与TN。\n",
    "\n",
    "Precision：精确度\n",
    "\n",
    "Recall： 召回率\n",
    "\n",
    "F1 Score (or F-Score)： 精确度和召回率的加权平均\n",
    "\n",
    "或者使用\n",
    "\n",
    "Kappa (Cohen's kappa)\n",
    "\n",
    "ROC Curves\n",
    "\n",
    "> ROC 评估方法\n",
    "\n",
    "ROC 曲线: 最佳的分类器应该尽可能地处于左上角\n",
    "\n",
    "* 对不同的 ROC 曲线进行比较的一个指标是曲线下的面积(Area Unser the Curve, AUC).\n",
    "\n",
    "* AUC 给出的是分类器的平均性能值，当然它并不能完全代替对整条曲线的观察。\n",
    "\n",
    "* 一个完美分类器的 AUC 为1，而随机猜测的 AUC 则为0.5。\n",
    "\n",
    "**3. 尝试对样本重抽样**\n",
    "\n",
    "欠抽样(undersampling)或者过抽样(oversampling)\n",
    "\n",
    "- 欠抽样: 意味着删除样例\n",
    "- 过抽样: 意味着复制样例(重复使用)\n",
    " \n",
    "对大类进行欠抽样\n",
    "\n",
    "对小类进行过抽样\n",
    "\n",
    "或者结合上述两种方法进行抽样\n",
    "\n",
    "一些经验法则：\n",
    "\n",
    "* 考虑样本（超过1万、十万甚至更多）进行欠采样，即删除部分样本；\n",
    "* 考虑样本（不足1为甚至更少）进行过采样，即添加部分样本的副本；\n",
    "* 考虑尝试随机采样与非随机采样两种采样方法；\n",
    "* 考虑对各类别尝试不同的采样比例，不一定是1:1\n",
    "* 考虑同时使用过采样与欠采样  \n",
    "\n",
    "**4. 尝试产生人工生成的样本**\n",
    "\n",
    "一种简单的方法就是随机抽样小类样本的属性（特征）来组成新的样本即属性值随机采样。你可以根据经验进行抽样，可以使用其他方式比如朴素贝叶斯方法假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之间的非线性关系。\n",
    "\n",
    "当然也有系统性的算法。最常用的SMOTE(Synthetic Minority Over-Sampling Technique)。 顾名思义，这是一种over sampling（过抽样）的方式。它是产生人为的样本而不是制造样本副本。这个算法是选取2个或者2个以上相似的样本（根据距离度量 distance measure），然后每次选择其中一个样本，并随机选择一定数量的邻居样本对选择的那个样本的一个属性增加噪声(每次只处理一个属性)。这样就构造了更多的新生数据。具体可以参见原始论文。\n",
    "\n",
    "python实现可以查阅UnbalancedDataset\n",
    "\n",
    "**5. 尝试不同的算法**\n",
    "\n",
    "强烈建议不要在每个问题上使用你最喜欢的算法。虽然这个算法带来较好的效果，但是它也会蒙蔽你观察数据内蕴含的其他的信息。至少你得在同一个问题上试试各种算法。具体可以参阅[Why you should be Spot-Checking Algorithms on your Machine Learning Problems](https://machinelearningmastery.com/why-you-should-be-spot-checking-algorithms-on-your-machine-learning-problems/)\n",
    "\n",
    "比如说，决策树经常在非均衡数据集上表现良好。创建分类树时候使用基于类变量的划分规则强制使类别表达出来。如果有疑惑，可以尝试一些流行的决策树，比如, C4.5, C5.0, CART 和 Random Forrest。\n",
    "\n",
    "**6. 尝试使用惩罚的模型**\n",
    "\n",
    "你可以使用同种算法但是以不同的角度对待这个问题。\n",
    "\n",
    "惩罚的模型就是对于不同的分类错误给予不同的代价（惩罚）。比如对于错分的小类给予更高的代价。这种方式会使模型偏差，更加关注小类。\n",
    "\n",
    "通常来说这种代价/惩罚或者比重在学习中算法是特定的。比如使用代价函数来实现：  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**代价函数**\n",
    "\n",
    "基于代价函数的分类器决策控制：TP\\*(-5)+FN\\*1+FP\\*50+TN\\*0\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1538019596900_wHrEiGvHiY.jpg)\n",
    "\n",
    "这种方式叫做 cost sensitive learning，Weka 中相应的框架可以实现叫[CostSensitiveClassifier](http://weka.sourceforge.net/doc.dev/weka/classifiers/meta/CostSensitiveClassifier.html)\n",
    "\n",
    "如果当你只能使用特定算法而且无法重抽样，或者模型效果不行，这时候使用惩罚（penalization）是可行的方法。这提供另外一种方式来“平衡”类别。但是设定惩罚函数/代价函数是比较复杂的。最好还是尝试不同的代价函数组合来得到最优效果。\n",
    "\n",
    "**7.尝试使用不同的角度**\n",
    "\n",
    "其实有很多研究关于非均衡数据。他们有自己的算法，度量，术语。\n",
    "\n",
    "从它们的角度看看你的问题，思考你的问题，说不定会有新的想法。\n",
    "\n",
    "两个领域您可以考虑： anomaly detection(异常值检测) 和 change detection（变化趋势检测）。\n",
    "\n",
    "Anomaly dectection 就是检测稀有事件。 比如通过机器震动来识别机器谷中或者根据一系列系统的调用来检测恶意操作。与常规操作相比，这些事件是罕见的。\n",
    "\n",
    "把小类想成异常类这种转变可能会帮助你想到新办法来分类数据样本。\n",
    "\n",
    "change detection 变化趋势检测类似于异常值检测。但是他不是寻找异常值而是寻找变化或区别。比如通过使用模式或者银行交易记录来观察用户行为转变。\n",
    "\n",
    "这些两种转变可能会给你新的方式去思考你的问题和新的技术去尝试。    \n",
    "\n",
    "**8.尝试去创新**\n",
    "\n",
    "仔细思考你的问题然后想想看如何将这问题细分为几个更切实际的小问题。\n",
    "\n",
    "比如：\n",
    "\n",
    "将你的大类分解成多个较小的类；\n",
    "\n",
    "使用One Class分类器（看待成异常点检测）；\n",
    "\n",
    "对数据集进行抽样成多个数据集，使用集成方式，训练多个分类器，然后联合这些分类器进行分类；\n",
    "\n",
    "这只是一个例子。更多的可以参阅[In classification, how do you handle an unbalanced training set?](https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set) 和[Classification when 80% of my training set is of one class](https://www.reddit.com/r/MachineLearning/comments/12evgi/classification_when_80_of_my_training_set_is_of/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
