{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五章 Logistic算法\n",
    "\n",
    "## Logistic回归概述\n",
    "\n",
    "Logistic 回归虽然名字叫回归，但是它是用来做分类的。其主要思想是: 根据现有数据对分类边界线建立回归公式，以此进行分类。\n",
    "\n",
    "## 须知概念\n",
    "\n",
    "### Sigmoid 函数\n",
    "\n",
    "**回归概念**\n",
    "\n",
    "假设现在有一些数据点，我们用一条直线对这些点进行拟合（这条直线称为最佳拟合直线），这个拟合的过程就叫做回归。进而可以得到对这些点的拟合直线方程，那么我们根据这个回归方程，怎么进行分类呢？请看下面。\n",
    "\n",
    "**二值型输出分类函数** \n",
    "\n",
    "我们想要的函数应该是: 能接受所有的输入然后预测出类别。例如，在两个类的情况下，上述函数输出 0 或 1.或许你之前接触过具有这种性质的函数，该函数称为 **海维塞得阶跃函数(Heaviside step function)**，或者直接称为**单位阶跃函数**。然而，海维塞得阶跃函数的问题在于: 该函数在跳跃点上从 0 瞬间跳跃到 1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质（可以输出 0 或者 1 的性质），且数学上更易处理，这就是 Sigmoid 函数。 Sigmoid 函数具体的计算公式如下:\n",
    "\n",
    "$$ \\sigma(z) = \\frac {1} {1 + e^{-z} }$$\n",
    "\n",
    "下图给出了 Sigmoid 函数在不同坐标尺度下的两条曲线图。当 x 为 0 时，Sigmoid 函数值为 0.5 。随着 x 的增大，对应的 Sigmoid 值将逼近于 1 ; 而随着 x 的减小， Sigmoid 值将逼近于 0 。如果横坐标刻度足够大， Sigmoid 函数看起来很像一个阶跃函数。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535969640336_YmIAGIAyBV.jpg)\n",
    "\n",
    "因此，为了实现 Logistic 回归分类器，我们可以在每个特征上都乘以一个回归系数（如下公式所示），然后把所有结果值相加，将这个总和代入 Sigmoid 函数中，进而得到一个范围在 0~1 之间的数值。任何大于 0.5 的数据被分入 1 类，小于 0.5 即被归入 0 类。所以， Logistic 回归也可以被看成是一种概率估计。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于最优化方法的回归系数确定\n",
    "\n",
    "Sigmoid 函数的输入记为 z ，由下面公式得到:\n",
    "\n",
    "$$z_0 = w_0x_0 + w_1x_1 + w_2x_2 + ... + w_nx_n $$\n",
    "\n",
    "如果采用向量的写法，上述公式可以写成 $$z = w^Tx $$，它表示将这两个数值向量对应元素相乘然后全部加起来即得到 z 值。其中的向量 x 是分类器的输入数据，向量 w 也就是我们要找到的最佳参数（系数），从而使得分类器尽可能地精确。为了寻找该最佳参数，需要用到最优化理论的一些知识。我们这里使用的是——梯度上升法。\n",
    "\n",
    "### 梯度上升法\n",
    "\n",
    "**梯度的介绍** \n",
    "\n",
    "需要一点点向量方面的数学基础\n",
    "\n",
    "```\n",
    "向量 = 值 + 方向  \n",
    "梯度 = 向量\n",
    "梯度 = 梯度值 + 梯度方向\n",
    "```\n",
    "\n",
    "**梯度上升法的思想** \n",
    "\n",
    "要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为 ▽ ，则函数 f(x, y) 的梯度由下式表示:\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535973409185_tM4eFBBCho.jpg)\n",
    "\n",
    "这个梯度意味着要沿 x 的方向移动 f(x, y)对x求偏导$$\\frac{\\partial f(x,y)}{\\partial x}$$ ，沿 y 的方向移动 f(x, y)对y求偏导$$ \\frac{\\partial f(x,y)}{\\partial y} $$。其中，函数$$f(x, y)$$必须要在待计算的点上有定义并且可微。下图是一个具体的例子。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535973585318_O1OeaE8juW.jpg)\n",
    "\n",
    "上图展示的，梯度上升算法到达每个点后都会重新估计移动的方向。从 P0 开始，计算完该点的梯度，函数就根据梯度移动到下一点 P1。在 P1 点，梯度再次被重新计算，并沿着新的梯度方向移动到 P2 。如此循环迭代，直到满足停止条件。迭代过程中，梯度算子总是保证我们能选取到最佳的移动方向。\n",
    "\n",
    "上图中的梯度上升算法沿梯度方向移动了一步。可以看到，梯度算子总是指向函数值增长最快的方向。这里所说的是移动方向，而未提到移动量的大小。该量值称为步长，记作 α 。用向量来表示的话，梯度上升算法的迭代公式如下:\n",
    "\n",
    "$$ w := w + \\alpha ▽_Wf(w)$$\n",
    "\n",
    "该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或者算法达到某个可以允许的误差范围。\n",
    "\n",
    "介绍一下几个相关的概念：\n",
    "\n",
    "```\n",
    "例如：y = w1x1 + w2x2 + ... + wnxn\n",
    "梯度：参考上图的例子，二维图像，x方向代表第一个系数，也就是 w1，y方向代表第二个系数也就是 w2，这样的向量就是梯度。\n",
    "α：上面的梯度算法的迭代公式中的阿尔法，这个代表的是移动步长。移动步长会影响最终结果的拟合程度，最好的方法就是随着迭代次数更改移动步长。\n",
    "步长通俗的理解，100米，如果我一步走10米，我需要走10步；如果一步走20米，我只需要走5步。这里的一步走多少米就是步长的意思。\n",
    "▽f(w)：代表沿着梯度变化的方向。\n",
    "```\n",
    "\n",
    "Note: 我们常听到的是梯度下降算法，它与这里的梯度上升算法是一样的，只是公式中的加法需要变成减法。因此，对应的公式可以写成\n",
    "\n",
    "$$ w := w - \\alpha ▽_Wf(w)$$\n",
    "\n",
    "梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**局部最优现象** \n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535973813643_WfTnRPD60a.jpg)\n",
    "\n",
    "上图表示参数 θ 与误差函数 J(θ) 的关系图，红色的部分是表示 J(θ) 有着比较高的取值，我们需要的是，能够让 J(θ) 的值尽量的低。也就是深蓝色的部分。θ0，θ1 表示 θ 向量的两个维度。\n",
    "\n",
    "可能梯度下降的最终点并非是全局最小点，可能是一个局部最小点，如我们上图中的右边的梯度下降曲线，描述的是最终到达一个局部最小点，这是我们重新选择了一个初始点得到的。\n",
    "\n",
    "看来我们这个算法将会在很大的程度上被初始点的选择影响而陷入局部最小点。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic 回归 原理\n",
    "\n",
    "### Logistic 回归 工作原理\n",
    "\n",
    "```\n",
    "每个回归系数初始化为 1\n",
    "重复 R 次:\n",
    "    计算整个数据集的梯度\n",
    "    使用 步长 x 梯度 更新回归系数的向量\n",
    "返回回归系数\n",
    "```\n",
    "### Logistic 回归 开发流程\n",
    "\n",
    "* 收集数据: 采用任意方法收集数据\n",
    "* 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。\n",
    "* 分析数据: 采用任意方法对数据进行分析。\n",
    "* 训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。\n",
    "* 测试算法: 一旦训练步骤完成，分类将会很快。\n",
    "* 使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。\n",
    "\n",
    "### Logistic 回归 算法特点\n",
    "\n",
    "* 优点: 计算代价不高，易于理解和实现。\n",
    "* 缺点: 容易欠拟合，分类精度可能不高。  \n",
    "适用数据类型: 数值型和标称型数据。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1535973957394_TkqodoCwJz.jpg)\n",
    "\n",
    "[清晰版链接](http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530807.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic 回归 项目案例\n",
    "\n",
    "### 项目案例1: 使用 Logistic 回归在简单数据集上的分类\n",
    "\n",
    "**项目概述** \n",
    "\n",
    "在一个简单的数据集上，采用梯度上升法找到 Logistic 回归分类器在此数据集上的最佳回归系数\n",
    "\n",
    "**开发流程**\n",
    "\n",
    "* 收集数据: 可以使用任何方法\n",
    "* 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳\n",
    "* 分析数据: 画出决策边界\n",
    "* 训练算法: 使用梯度上升找到最佳参数\n",
    "* 测试算法: 使用 Logistic 回归进行分类\n",
    "* 使用算法: 对简单数据集中数据进行分类\n",
    "\n",
    "> 收集数据: 可以使用任何方法\n",
    "\n",
    "我们采用存储在 TestSet.txt 文本文件中的数据，存储格式如下:\n",
    "\n",
    "```\n",
    "-0.017612   14.053064   0\n",
    "-1.395634   4.662541    1\n",
    "-0.752157   6.538620    0\n",
    "-1.322371   7.152853    0\n",
    "0.423363    11.054677   0\n",
    "```\n",
    "\n",
    "绘制在图中，如下图所示:\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1536042791389_tWN0foYZD1.jpg)\n",
    "\n",
    "> 准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳\n",
    "\n",
    "> 分析数据: 画出决策边界\n",
    "\n",
    "画出数据集和 Logistic 回归最佳拟合直线的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBestFit(dataArr, labelMat, weights):\n",
    "    '''\n",
    "        Desc:\n",
    "            将我们得到的数据可视化展示出来\n",
    "        Args:\n",
    "            dataArr:样本数据的特征\n",
    "            labelMat:样本数据的类别标签，即目标变量\n",
    "            weights:回归系数\n",
    "        Returns:\n",
    "            None\n",
    "    '''\n",
    "\n",
    "    n = shape(dataArr)[0]\n",
    "    xcord1 = []; ycord1 = []\n",
    "    xcord2 = []; ycord2 = []\n",
    "    for i in range(n):\n",
    "        if int(labelMat[i])== 1:\n",
    "            xcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
    "        else:\n",
    "            xcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
    "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
    "    x = arange(-3.0, 3.0, 0.1)\n",
    "    \"\"\"\n",
    "    y的由来，卧槽，是不是没看懂？\n",
    "    首先理论上是这个样子的。\n",
    "    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "    w0*x0+w1*x1+w2*x2=f(x)\n",
    "    x0最开始就设置为1叻， x2就是我们画图的y值，而f(x)被我们磨合误差给算到w0,w1,w2身上去了\n",
    "    所以： w0+w1*x+w2*y=0 => y = (-w0-w1*x)/w2   \n",
    "    \"\"\"\n",
    "    y = (-weights[0]-weights[1]*x)/weights[2]\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel('X'); plt.ylabel('Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练算法: 使用梯度上升找到最佳参数\n",
    "\n",
    "Logistic 回归梯度上升优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常的处理方案\n",
    "# 两个参数：第一个参数==> dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。\n",
    "# 第二个参数==> classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    # 转化为矩阵[[1,1,2],[1,1,2]....]\n",
    "    dataMatrix = mat(dataMatIn)             # 转换为 NumPy 矩阵\n",
    "    # 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]\n",
    "    # transpose() 行列转置函数\n",
    "    # 将行向量转化为列向量   =>  矩阵的转置\n",
    "    labelMat = mat(classLabels).transpose() # 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量\n",
    "    # m->数据量，样本数 n->特征数\n",
    "    m,n = shape(dataMatrix)\n",
    "    # print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100\n",
    "    # alpha代表向目标移动的步长\n",
    "    alpha = 0.001\n",
    "    # 迭代次数\n",
    "    maxCycles = 500\n",
    "    # 生成一个长度和特征数相同的矩阵，此处n为3 -> [[1],[1],[1]]\n",
    "    # weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1\n",
    "    weights = ones((n,1))\n",
    "    for k in range(maxCycles):              #heavy on matrix operations\n",
    "        # m*3 的矩阵 * 3*1 的单位矩阵 ＝ m*1的矩阵\n",
    "        # 那么乘上单位矩阵的意义，就代表：通过公式得到的理论值\n",
    "        # 参考地址： 矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145\n",
    "        # print 'dataMatrix====', dataMatrix \n",
    "        # print 'weights====', weights\n",
    "        # n*3   *  3*1  = n*1\n",
    "        h = sigmoid(dataMatrix*weights)     # 矩阵乘法\n",
    "        # print 'hhhhhhh====', h\n",
    "        # labelMat是实际值\n",
    "        error = (labelMat - h)              # 向量相减\n",
    "        # 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量\n",
    "        weights = weights + alpha * dataMatrix.transpose() * error # 矩阵乘法，最后得到回归系数\n",
    "    return array(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家看到这儿可能会有一些疑惑，就是，我们在迭代中更新我们的回归系数，后边的部分是怎么计算出来的？为什么会是 **alpha * dataMatrix.transpose() * error**?因为这就是我们所求的梯度，也就是对 f(w) 对 w 求一阶导数。具体推导如下:\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1536043128966_SjTwjsfCLA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法: 使用 Logistic 回归进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testLR():\n",
    "    # 1.收集并准备数据\n",
    "    dataMat, labelMat = loadDataSet(\"./dataset/TestSet.txt\")\n",
    "\n",
    "    # print dataMat, '---\\n', labelMat\n",
    "    # 2.训练模型，  f(x)=a1*x1+b2*x2+..+nn*xn中 (a1,b2, .., nn).T的矩阵值\n",
    "    # 因为数组没有是复制n份， array的乘法就是乘法\n",
    "    dataArr = array(dataMat)\n",
    "    # print dataArr\n",
    "    weights = gradAscent(dataArr, labelMat)\n",
    "    # weights = stocGradAscent0(dataArr, labelMat)\n",
    "    # weights = stocGradAscent1(dataArr, labelMat)\n",
    "    # print '*'*30, weights\n",
    "\n",
    "    # 数据可视化\n",
    "    plotBestFit(dataArr, labelMat, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用算法: 对简单数据集中数据进行分类\n",
    "\n",
    "**注意** \n",
    "\n",
    "梯度上升算法在每次更新回归系数时都需要遍历整个数据集，该方法在处理 100 个左右的数据集时尚可，但如果有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为 随机梯度上升算法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与 “在线学习” 相对应，一次处理所有数据被称作是 “批处理”。\n",
    "\n",
    "随机梯度上升算法可以写成如下的伪代码:\n",
    "\n",
    "```\n",
    "所有回归系数初始化为 1\n",
    "对数据集中每个样本\n",
    "    计算该样本的梯度\n",
    "    使用 alpha x gradient 更新回归系数值\n",
    "返回回归系数值\n",
    "```\n",
    "以下是随机梯度上升算法的实现代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机梯度上升\n",
    "# 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高\n",
    "# 随机梯度上升一次只用一个样本点来更新回归系数\n",
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "    m,n = shape(dataMatrix)\n",
    "    alpha = 0.01\n",
    "    # n*1的矩阵\n",
    "    # 函数ones创建一个全1的数组\n",
    "    weights = ones(n)   # 初始化长度为n的数组，元素全部为 1\n",
    "    for i in range(m):\n",
    "        # sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵\n",
    "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
    "        # print 'dataMatrix[i]===', dataMatrix[i]\n",
    "        # 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数\n",
    "        error = classLabels[i] - h\n",
    "        # 0.01*(1*1)*(1*n)\n",
    "        print (weights, \"*\"*10 , dataMatrix[i], \"*\"*10 , error)\n",
    "        weights = weights + alpha * error * dataMatrix[i]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，随机梯度上升算法与梯度上升算法在代码上很相似，但也有一些区别: 第一，后者的变量 h 和误差 error 都是向量，而前者则全是数值；第二，前者没有矩阵的转换过程，所有变量的数据类型都是 NumPy 数组。\n",
    "\n",
    "判断优化算法优劣的可靠方法是看它是否收敛，也就是说参数是否达到了稳定值，是否还会不断地变化？下图展示了随机梯度上升算法在 200 次迭代过程中回归系数的变化情况。其中的系数2，也就是 X2 只经过了 50 次迭代就达到了稳定值，但系数 1 和 0 则需要更多次的迭代。如下图所示:\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1536043270064_qQTJtg2N8Y.jpg)\n",
    "\n",
    "针对这个问题，我们改进了之前的随机梯度上升算法，如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机梯度上升算法（随机化）\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    m,n = shape(dataMatrix)\n",
    "    weights = ones(n)   # 创建与列数相同的矩阵的系数矩阵，所有的元素都是1\n",
    "    # 随机梯度, 循环150,观察是否收敛\n",
    "    for j in range(numIter):\n",
    "        # [0, 1, 2 .. m-1]\n",
    "        dataIndex = range(m)\n",
    "        for i in range(m):\n",
    "            # i和j的不断增大，导致alpha的值不断减少，但是不为0\n",
    "            alpha = 4/(1.0+j+i)+0.0001    # alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001\n",
    "            # 随机产生一个 0～len()之间的一个值\n",
    "            # random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。\n",
    "            randIndex = int(random.uniform(0,len(dataIndex)))\n",
    "            # sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn\n",
    "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            # print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            del(dataIndex[randIndex])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的改进版随机梯度上升算法，我们修改了两处代码。\n",
    "\n",
    "第一处改进为 alpha 的值。alpha 在每次迭代的时候都会调整，这回缓解上面波动图的数据波动或者高频波动。另外，虽然 alpha 会随着迭代次数不断减少，但永远不会减小到 0，因为我们在计算公式中添加了一个常数项。\n",
    "\n",
    "第二处修改为 randIndex 更新，这里通过随机选取样本拉来更新回归系数。这种方法将减少周期性的波动。这种方法每次随机从列表中选出一个值，然后从列表中删掉该值（再进行下一次迭代）。\n",
    "\n",
    "程序运行之后能看到类似于下图的结果图。\n",
    "\n",
    "![](http://aliyuntianchipublic.cn-hangzhou.oss-pub.aliyun-inc.com/public/files/image/null/1536043323597_fhOviGg7Sx.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#项目1完整代码\n",
    "\n",
    "#!/usr/bin/python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ------使用 Logistic 回归在简单数据集上的分类-----------\n",
    "\n",
    "def load_data_set():\n",
    "    \"\"\"\n",
    "    加载数据集\n",
    "    :return:返回两个数组，普通数组 \n",
    "        data_arr -- 原始数据的特征\n",
    "        label_arr -- 原始数据的标签，也就是每条样本对应的类别\n",
    "    \"\"\"\n",
    "    data_arr = []\n",
    "    label_arr = []\n",
    "    f = open('./dataset/TestSet.txt', 'r')\n",
    "    for line in f.readlines():\n",
    "        line_arr = line.strip().split()\n",
    "        # 为了方便计算，我们将 X0 的值设为 1.0 ，也就是在每一行的开头添加一个 1.0 作为 X0\n",
    "        data_arr.append([1.0, np.float(line_arr[0]), np.float(line_arr[1])])\n",
    "        label_arr.append(int(line_arr[2]))\n",
    "    return data_arr, label_arr\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    # 这里其实非常有必要解释一下，会出现的错误 RuntimeWarning: overflow encountered in exp\n",
    "    # 这个错误在学习阶段虽然可以忽略，但是我们至少应该知道为什么\n",
    "    # 这里是因为我们输入的有的 x 实在是太小了，比如 -6000之类的，那么计算一个数字 np.exp(6000)这个结果太大了，没法表示，所以就溢出了\n",
    "    # 如果是计算 np.exp（-6000），这样虽然也会溢出，但是这是下溢，就是表示成零\n",
    "    # 去网上搜了很多方法，比如 使用bigfloat这个库（我竟然没有安装成功，就不尝试了，反正应该是有用的\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def grad_ascent(data_arr, class_labels):\n",
    "    \"\"\"\n",
    "    梯度上升法，其实就是因为使用了极大似然估计，这个大家有必要去看推导，只看代码感觉不太够\n",
    "    :param data_arr: 传入的就是一个普通的数组，当然你传入一个二维的ndarray也行\n",
    "    :param class_labels: class_labels 是类别标签，它是一个 1*100 的行向量。\n",
    "                    为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给label_mat\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # 注意一下，我把原来 data_mat_in 改成data_arr,因为传进来的是一个数组，用这个比较不容易搞混\n",
    "    # turn the data_arr to numpy matrix\n",
    "    data_mat = np.mat(data_arr)\n",
    "    # 变成矩阵之后进行转置\n",
    "    label_mat = np.mat(class_labels).transpose()\n",
    "    # m->数据量，样本数 n->特征数\n",
    "    m, n = np.shape(data_mat)\n",
    "    # 学习率，learning rate\n",
    "    alpha = 0.001\n",
    "    # 最大迭代次数，假装迭代这么多次就能收敛2333\n",
    "    max_cycles = 500\n",
    "    # 生成一个长度和特征数相同的矩阵，此处n为3 -> [[1],[1],[1]]\n",
    "    # weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1\n",
    "    weights = np.ones((n, 1))\n",
    "    for k in range(max_cycles):\n",
    "        # 这里是点乘  m x 3 dot 3 x 1\n",
    "        h = sigmoid(data_mat * weights)\n",
    "        error = label_mat - h\n",
    "        # 这里比较建议看一下推导，为什么这么做可以，这里已经是求导之后的\n",
    "        weights = weights + alpha * data_mat.transpose() * error\n",
    "    return weights\n",
    "\n",
    "\n",
    "def plot_best_fit(weights):\n",
    "    \"\"\"\n",
    "    可视化\n",
    "    :param weights: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    data_mat, label_mat = load_data_set()\n",
    "    data_arr = np.array(data_mat)\n",
    "    n = np.shape(data_mat)[0]\n",
    "    x_cord1 = []\n",
    "    y_cord1 = []\n",
    "    x_cord2 = []\n",
    "    y_cord2 = []\n",
    "    for i in range(n):\n",
    "        if int(label_mat[i]) == 1:\n",
    "            x_cord1.append(data_arr[i, 1])\n",
    "            y_cord1.append(data_arr[i, 2])\n",
    "        else:\n",
    "            x_cord2.append(data_arr[i, 1])\n",
    "            y_cord2.append(data_arr[i, 2])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(x_cord1, y_cord1, s=30, color='k', marker='^')\n",
    "    ax.scatter(x_cord2, y_cord2, s=30, color='red', marker='s')\n",
    "    x = np.arange(-3.0, 3.0, 0.1)\n",
    "    y = (-weights[0] - weights[1] * x) / weights[2]\n",
    "    \"\"\"\n",
    "    y的由来，卧槽，是不是没看懂？\n",
    "    首先理论上是这个样子的。\n",
    "    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "    w0*x0+w1*x1+w2*x2=f(x)\n",
    "    x0最开始就设置为1叻， x2就是我们画图的y值，而f(x)被我们磨合误差给算到w0,w1,w2身上去了\n",
    "    所以： w0+w1*x+w2*y=0 => y = (-w0-w1*x)/w2   \n",
    "    \"\"\"\n",
    "    ax.plot(x, y)\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('y1')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def stoc_grad_ascent0(data_mat, class_labels):\n",
    "    \"\"\"\n",
    "    随机梯度上升，只使用一个样本点来更新回归系数\n",
    "    :param data_mat: 输入数据的数据特征（除去最后一列）,ndarray\n",
    "    :param class_labels: 输入数据的类别标签（最后一列数据）\n",
    "    :return: 得到的最佳回归系数\n",
    "    \"\"\"\n",
    "    m, n = np.shape(data_mat)\n",
    "    alpha = 0.01\n",
    "    weights = np.ones(n)\n",
    "    for i in range(m):\n",
    "        # sum(data_mat[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,\n",
    "        # 此处求出的 h 是一个具体的数值，而不是一个矩阵\n",
    "        h = sigmoid(sum(data_mat[i] * weights))\n",
    "        error = class_labels[i] - h\n",
    "        # 还是和上面一样，这个先去看推导，再写程序\n",
    "        weights = weights + alpha * error * data_mat[i]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def stoc_grad_ascent1(data_mat, class_labels, num_iter=150):\n",
    "    \"\"\"\n",
    "    改进版的随机梯度上升，使用随机的一个样本来更新回归系数\n",
    "    :param data_mat: 输入数据的数据特征（除去最后一列）,ndarray\n",
    "    :param class_labels: 输入数据的类别标签（最后一列数据\n",
    "    :param num_iter: 迭代次数\n",
    "    :return: 得到的最佳回归系数\n",
    "    \"\"\"\n",
    "    m, n = np.shape(data_mat)\n",
    "    weights = np.ones(n)\n",
    "    for j in range(num_iter):\n",
    "        # 这里必须要用list，不然后面的del没法使用\n",
    "        data_index = list(range(m))\n",
    "        for i in range(m):\n",
    "            # i和j的不断增大，导致alpha的值不断减少，但是不为0\n",
    "            alpha = 4 / (1.0 + j + i) + 0.01\n",
    "            # 随机产生一个 0～len()之间的一个值\n",
    "            # random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。\n",
    "            rand_index = int(np.random.uniform(0, len(data_index)))\n",
    "            h = sigmoid(np.sum(data_mat[data_index[rand_index]] * weights))\n",
    "            error = class_labels[data_index[rand_index]] - h\n",
    "            weights = weights + alpha * error * data_mat[data_index[rand_index]]\n",
    "            del(data_index[rand_index])\n",
    "    return weights\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    这个函数只要就是对上面的几个算法的测试，这样就不用每次都在power shell 里面操作，不然麻烦死了\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    data_arr, class_labels = load_data_set()\n",
    "    # 注意，这里的grad_ascent返回的是一个 matrix, 所以要使用getA方法变成ndarray类型\n",
    "    # weights = grad_ascent(data_arr, class_labels).getA()\n",
    "    # weights = stoc_grad_ascent0(np.array(data_arr), class_labels)\n",
    "    weights = stoc_grad_ascent1(np.array(data_arr), class_labels)\n",
    "    plot_best_fit(weights)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目案例2: 从疝气病症预测病马的死亡率\n",
    "\n",
    "**项目概述**\n",
    "\n",
    "使用 Logistic 回归来预测患有疝病的马的存活问题。疝病是描述马胃肠痛的术语。然而，这种病不一定源自马的胃肠问题，其他问题也可能引发马疝病。这个数据集中包含了医院检测马疝病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。\n",
    "\n",
    "**开发流程**\n",
    "\n",
    "```\n",
    "收集数据: 给定数据文件\n",
    "准备数据: 用 Python 解析文本文件并填充缺失值\n",
    "分析数据: 可视化并观察数据\n",
    "训练算法: 使用优化算法，找到最佳的系数\n",
    "测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，\n",
    "         通过改变迭代的次数和步长的参数来得到更好的回归系数\n",
    "使用算法: 实现一个简单的命令行程序来手机马的症状并输出预测结果并非难事，\n",
    "         这可以作为留给大家的一道习题\n",
    "```\n",
    "\n",
    "> 收集数据: 给定数据文件\n",
    "\n",
    "病马的训练数据已经给出来了，如下形式存储在文本文件中:\n",
    "```\n",
    "1.000000    1.000000    39.200000   88.000000   20.000000   0.000000    0.000000    4.000000    1.000000    3.000000    4.000000    2.000000    0.000000    0.000000    0.000000    4.000000    2.000000    50.000000   85.000000   2.000000    2.000000    0.000000\n",
    "2.000000    1.000000    38.300000   40.000000   24.000000   1.000000    1.000000    3.000000    1.000000    3.000000    3.000000    1.000000    0.000000    0.000000    0.000000    1.000000    1.000000    33.000000   6.700000    0.000000    0.000000    1.000000\n",
    "```\n",
    "\n",
    "> 准备数据: 用 Python 解析文本文件并填充缺失值\n",
    "\n",
    "处理数据中的缺失值\n",
    "\n",
    "假设有100个样本和20个特征，这些数据都是机器收集回来的。若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？ 它们是否还可以用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采用一些方法来解决这个问题。\n",
    "\n",
    "下面给出了一些可选的做法： 使用可用特征的均值来填补缺失值； 使用特殊值来填补缺失值，如 -1； 忽略有缺失值的样本； 使用有相似样本的均值添补缺失值； * 使用另外的机器学习算法预测缺失值。\n",
    "\n",
    "现在，我们对下一节要用的数据集进行预处理，使其可以顺利地使用分类算法。在预处理需要做两件事: * 所有的缺失值必须用一个实数值来替换，因为我们使用的 NumPy 数据类型不允许包含缺失值。我们这里选择实数 0 来替换所有缺失值，恰好能适用于 Logistic 回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值。回归系数的更新公式如下:\n",
    "\n",
    "```\n",
    "weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "\n",
    "如果 dataMatrix 的某个特征对应值为 0，那么该特征的系数将不做更新，即:\n",
    "\n",
    "weights = weights\n",
    "\n",
    "另外，由于 Sigmoid(0) = 0.5 ，即它对结果的预测不具有任何倾向性，因此我们上述做法也不会对误差造成任何影响。基于上述原因，将缺失值用 0 代替既可以保留现有数据，也不需要对优化算法进行修改。此外，该数据集中的特征取值一般不为 0，因此在某种意义上说它也满足 “特殊值” 这个要求。\n",
    "```\n",
    "\n",
    "如果在测试数据集中发现了一条数据的类别标签已经缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换。采用 Logistic 回归进行分类时这种做法是合理的，而如果采用类似 kNN 的方法就可能不太可行。\n",
    "\n",
    "原始的数据集经过预处理后，保存成两个文件: horseColicTest.txt 和 horseColicTraining.txt 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 分析数据: 可视化并观察数据\n",
    "\n",
    "将数据使用 MatPlotlib 打印出来，观察数据是否是我们想要的格式\n",
    "\n",
    "> 训练算法: 使用优化算法，找到最佳的系数\n",
    "\n",
    "下面给出 原始的梯度上升算法，随机梯度上升算法，改进版随机梯度上升算法 的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正常的处理方案\n",
    "# 两个参数：第一个参数==> dataMatIn 是一个2维NumPy数组，每列分别代表每个不同的特征，每行则代表每个训练样本。\n",
    "# 第二个参数==> classLabels 是类别标签，它是一个 1*100 的行向量。为了便于矩阵计算，需要将该行向量转换为列向量，做法是将原向量转置，再将它赋值给labelMat。\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    # 转化为矩阵[[1,1,2],[1,1,2]....]\n",
    "    dataMatrix = mat(dataMatIn)             # 转换为 NumPy 矩阵\n",
    "    # 转化为矩阵[[0,1,0,1,0,1.....]]，并转制[[0],[1],[0].....]\n",
    "    # transpose() 行列转置函数\n",
    "    # 将行向量转化为列向量   =>  矩阵的转置\n",
    "    labelMat = mat(classLabels).transpose() # 首先将数组转换为 NumPy 矩阵，然后再将行向量转置为列向量\n",
    "    # m->数据量，样本数 n->特征数\n",
    "    m,n = shape(dataMatrix)\n",
    "    # print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100\n",
    "    # alpha代表向目标移动的步长\n",
    "    alpha = 0.001\n",
    "    # 迭代次数\n",
    "    maxCycles = 500\n",
    "    # 生成一个长度和特征数相同的矩阵，此处n为3 -> [[1],[1],[1]]\n",
    "    # weights 代表回归系数， 此处的 ones((n,1)) 创建一个长度和特征数相同的矩阵，其中的数全部都是 1\n",
    "    weights = ones((n,1))\n",
    "    for k in range(maxCycles):              #heavy on matrix operations\n",
    "        # m*3 的矩阵 * 3*1 的单位矩阵 ＝ m*1的矩阵\n",
    "        # 那么乘上单位矩阵的意义，就代表：通过公式得到的理论值\n",
    "        # 参考地址： 矩阵乘法的本质是什么？ https://www.zhihu.com/question/21351965/answer/31050145\n",
    "        # print 'dataMatrix====', dataMatrix \n",
    "        # print 'weights====', weights\n",
    "        # n*3   *  3*1  = n*1\n",
    "        h = sigmoid(dataMatrix*weights)     # 矩阵乘法\n",
    "        # print 'hhhhhhh====', h\n",
    "        # labelMat是实际值\n",
    "        error = (labelMat - h)              # 向量相减\n",
    "        # 0.001* (3*m)*(m*1) 表示在每一个列上的一个误差情况，最后得出 x1,x2,xn的系数的偏移量\n",
    "        weights = weights + alpha * dataMatrix.transpose() * error # 矩阵乘法，最后得到回归系数\n",
    "    return array(weights)\n",
    "\n",
    "\n",
    "# 随机梯度上升\n",
    "# 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高\n",
    "# 随机梯度上升一次只用一个样本点来更新回归系数\n",
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "    m,n = shape(dataMatrix)\n",
    "    alpha = 0.01\n",
    "    # n*1的矩阵\n",
    "    # 函数ones创建一个全1的数组\n",
    "    weights = ones(n)   # 初始化长度为n的数组，元素全部为 1\n",
    "    for i in range(m):\n",
    "        # sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵\n",
    "        h = sigmoid(sum(dataMatrix[i]*weights))\n",
    "        # print 'dataMatrix[i]===', dataMatrix[i]\n",
    "        # 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数\n",
    "        error = classLabels[i] - h\n",
    "        # 0.01*(1*1)*(1*n)\n",
    "        print (weights, \"*\"*10 , dataMatrix[i], \"*\"*10 , error)\n",
    "        weights = weights + alpha * error * dataMatrix[i]\n",
    "    return weights\n",
    "\n",
    "\n",
    "# 随机梯度上升算法（随机化）\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter=150):\n",
    "    m,n = shape(dataMatrix)\n",
    "    weights = ones(n)   # 创建与列数相同的矩阵的系数矩阵，所有的元素都是1\n",
    "    # 随机梯度, 循环150,观察是否收敛\n",
    "    for j in range(numIter):\n",
    "        # [0, 1, 2 .. m-1]\n",
    "        dataIndex = range(m)\n",
    "        for i in range(m):\n",
    "            # i和j的不断增大，导致alpha的值不断减少，但是不为0\n",
    "            alpha = 4/(1.0+j+i)+0.0001    # alpha 会随着迭代不断减小，但永远不会减小到0，因为后边还有一个常数项0.0001\n",
    "            # 随机产生一个 0～len()之间的一个值\n",
    "            # random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。\n",
    "            randIndex = int(random.uniform(0,len(dataIndex)))\n",
    "            # sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn\n",
    "            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n",
    "            error = classLabels[randIndex] - h\n",
    "            # print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]\n",
    "            weights = weights + alpha * error * dataMatrix[randIndex]\n",
    "            del(dataIndex[randIndex])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 测试算法: 为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长的参数来得到更好的回归系数\n",
    "\n",
    "Logistic 回归分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类函数，根据回归系数和特征向量来计算 Sigmoid的值\n",
    "def classifyVector(inX, weights):\n",
    "    '''\n",
    "    Desc: \n",
    "        最终的分类函数，根据回归系数和特征向量来计算 Sigmoid 的值，大于0.5函数返回1，否则返回0\n",
    "    Args:\n",
    "        inX -- 特征向量，features\n",
    "        weights -- 根据梯度下降/随机梯度下降 计算得到的回归系数\n",
    "    Returns:\n",
    "        如果 prob 计算大于 0.5 函数返回 1\n",
    "        否则返回 0\n",
    "    '''\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5: return 1.0\n",
    "    else: return 0.0\n",
    "\n",
    "# 打开测试集和训练集,并对数据进行格式化处理\n",
    "def colicTest():\n",
    "    '''\n",
    "    Desc:\n",
    "        打开测试集和训练集，并对数据进行格式化处理\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        errorRate -- 分类错误率\n",
    "    '''\n",
    "    frTrain = open('./dataset/horseColicTraining.txt')\n",
    "    frTest = open('./dataset/horseColicTest.txt')\n",
    "    trainingSet = []\n",
    "    trainingLabels = []\n",
    "    # 解析训练数据集中的数据特征和Labels\n",
    "    # trainingSet 中存储训练数据集的特征，trainingLabels 存储训练数据集的样本对应的分类标签\n",
    "    for line in frTrain.readlines():\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        trainingSet.append(lineArr)\n",
    "        trainingLabels.append(float(currLine[21]))\n",
    "    # 使用 改进后的 随机梯度下降算法 求得在此数据集上的最佳回归系数 trainWeights\n",
    "    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)\n",
    "    errorCount = 0\n",
    "    numTestVec = 0.0\n",
    "    # 读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率\n",
    "    for line in frTest.readlines():\n",
    "        numTestVec += 1.0\n",
    "        currLine = line.strip().split('\\t')\n",
    "        lineArr = []\n",
    "        for i in range(21):\n",
    "            lineArr.append(float(currLine[i]))\n",
    "        if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[21]):\n",
    "            errorCount += 1\n",
    "    errorRate = (float(errorCount) / numTestVec)\n",
    "    print (\"the error rate of this test is: %f\" % errorRate)\n",
    "    return errorRate\n",
    "\n",
    "\n",
    "# 调用 colicTest() 10次并求结果的平均值\n",
    "def multiTest():\n",
    "    numTests = 10\n",
    "    errorSum = 0.0\n",
    "    for k in range(numTests):\n",
    "        errorSum += colicTest()\n",
    "    print (\"after %d iterations the average error rate is: %f\" % (numTests, errorSum/float(numTests))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用算法: 实现一个简单的命令行程序来收集马的症状并输出预测结果并非难事，这可以作为留给大家的一道习题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is 0.26865671641791045\n",
      "the error rate is 0.4925373134328358\n",
      "the error rate is 0.29850746268656714\n",
      "the error rate is 0.3283582089552239\n",
      "the error rate is 0.5074626865671642\n",
      "the error rate is 0.29850746268656714\n",
      "the error rate is 0.3283582089552239\n",
      "the error rate is 0.31343283582089554\n",
      "the error rate is 0.47761194029850745\n",
      "the error rate is 0.31343283582089554\n",
      "after 10 iteration the average error rate is 0.3626865671641791\n"
     ]
    }
   ],
   "source": [
    "# 项目二完整代码\n",
    "#!/usr/bin/python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # 这里其实非常有必要解释一下，会出现的错误 RuntimeWarning: overflow encountered in exp\n",
    "    # 这个错误在学习阶段虽然可以忽略，但是我们至少应该知道为什么\n",
    "    # 这里是因为我们输入的有的 x 实在是太小了，比如 -6000之类的，那么计算一个数字 np.exp(6000)这个结果太大了，没法表示，所以就溢出了\n",
    "    # 如果是计算 np.exp（-6000），这样虽然也会溢出，但是这是下溢，就是表示成零\n",
    "    # 去网上搜了很多方法，比如 使用bigfloat这个库（我竟然没有安装成功，就不尝试了，反正应该是有用的\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def stoc_grad_ascent1(data_mat, class_labels, num_iter=150):\n",
    "    \"\"\"\n",
    "    改进版的随机梯度上升，使用随机的一个样本来更新回归系数\n",
    "    :param data_mat: 输入数据的数据特征（除去最后一列）,ndarray\n",
    "    :param class_labels: 输入数据的类别标签（最后一列数据\n",
    "    :param num_iter: 迭代次数\n",
    "    :return: 得到的最佳回归系数\n",
    "    \"\"\"\n",
    "    m, n = np.shape(data_mat)\n",
    "    weights = np.ones(n)\n",
    "    for j in range(num_iter):\n",
    "        # 这里必须要用list，不然后面的del没法使用\n",
    "        data_index = list(range(m))\n",
    "        for i in range(m):\n",
    "            # i和j的不断增大，导致alpha的值不断减少，但是不为0\n",
    "            alpha = 4 / (1.0 + j + i) + 0.01\n",
    "            # 随机产生一个 0～len()之间的一个值\n",
    "            # random.uniform(x, y) 方法将随机生成下一个实数，它在[x,y]范围内,x是这个范围内的最小值，y是这个范围内的最大值。\n",
    "            rand_index = int(np.random.uniform(0, len(data_index)))\n",
    "            h = sigmoid(np.sum(data_mat[data_index[rand_index]] * weights))\n",
    "            error = class_labels[data_index[rand_index]] - h\n",
    "            weights = weights + alpha * error * data_mat[data_index[rand_index]]\n",
    "            del(data_index[rand_index])\n",
    "    return weights\n",
    "\n",
    "def classify_vector(in_x, weights):\n",
    "    \"\"\"\n",
    "    最终的分类函数，根据回归系数和特征向量来计算 Sigmoid 的值，大于0.5函数返回1，否则返回0\n",
    "    :param in_x: 特征向量，features\n",
    "    :param weights: 根据梯度下降/随机梯度下降 计算得到的回归系数\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # print(np.sum(in_x * weights))\n",
    "    prob = sigmoid(np.sum(in_x * weights))\n",
    "    if prob > 0.5:\n",
    "        return 1.0\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def colic_test():\n",
    "    \"\"\"\n",
    "    打开测试集和训练集，并对数据进行格式化处理,其实最主要的的部分，比如缺失值的补充（真的需要学会的），人家已经做了\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    f_train = open('./dataset/HorseColicTraining.txt', 'r')\n",
    "    f_test = open('./dataset/HorseColicTest.txt', 'r')\n",
    "    training_set = []\n",
    "    training_labels = []\n",
    "    # 解析训练数据集中的数据特征和Labels\n",
    "    # trainingSet 中存储训练数据集的特征，trainingLabels 存储训练数据集的样本对应的分类标签\n",
    "    for line in f_train.readlines():\n",
    "        curr_line = line.strip().split('\\t')\n",
    "        if len(curr_line) == 1:\n",
    "            continue    # 这里如果就一个空的元素，则跳过本次循环\n",
    "        line_arr = [float(curr_line[i]) for i in range(21)]\n",
    "        training_set.append(line_arr)\n",
    "        training_labels.append(float(curr_line[21]))\n",
    "    # 使用 改进后的 随机梯度下降算法 求得在此数据集上的最佳回归系数 trainWeights\n",
    "    train_weights = stoc_grad_ascent1(np.array(training_set), training_labels, 500)\n",
    "    error_count = 0\n",
    "    num_test_vec = 0.0\n",
    "    # 读取 测试数据集 进行测试，计算分类错误的样本条数和最终的错误率\n",
    "    for line in f_test.readlines():\n",
    "        num_test_vec += 1\n",
    "        curr_line = line.strip().split('\\t')\n",
    "        if len(curr_line) == 1: \n",
    "            continue    # 这里如果就一个空的元素，则跳过本次循环\n",
    "        line_arr = [float(curr_line[i]) for i in range(21)]\n",
    "        if int(classify_vector(np.array(line_arr), train_weights)) != int(curr_line[21]):\n",
    "            error_count += 1\n",
    "    error_rate = error_count / num_test_vec\n",
    "    print('the error rate is {}'.format(error_rate))\n",
    "    return error_rate\n",
    "\n",
    "\n",
    "def multi_test():\n",
    "    \"\"\"\n",
    "    调用 colicTest() 10次并求结果的平均值\n",
    "    :return: nothing \n",
    "    \"\"\"\n",
    "    num_tests = 10\n",
    "    error_sum = 0\n",
    "    for k in range(num_tests):\n",
    "        error_sum += colic_test()\n",
    "    print('after {} iteration the average error rate is {}'.format(num_tests, error_sum / num_tests))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #colic_test()\n",
    "    multi_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课后练习\n",
    "\n",
    "使用逻辑回归算法对个人收入水平的数据进行分类\n",
    "\n",
    "个人收入水平调查分析\n",
    "\n",
    "数据集链接： https://tianchi.aliyun.com/datalab/dataSet.html?spm=5176.100073.0.0.14156fc1uW82Um&dataId=3657\n",
    "\n",
    "该数据集是某地区的个人收入调查分析，包含32561条记录，其中目标变量是收入水平（分别是<=50k以及>50k），其他自变量包括年龄，受教育时间，性别，资产净增，资产损失，一周工作时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
